input,actual_output,expected_output,retrieval_context,context,source_file
What is Thresholding?,"Thresholding is an image processing technique used to classify pixels in an image into either foreground or background based on their intensity values. The result is typically a binary image where pixels are set to one of two values (e.g., 0 for black or 255 for white), depending on whether their original intensity was above or below a chosen threshold value.

Here's a breakdown of what thresholding involves:
*   **Purpose**: Many image processing operations make decisions about whether a pixel belongs to an object of interest or the background. Thresholding facilitates this by simplifying the image into two distinct regions.
*   **Process**:
    *   A threshold value, `T`, is chosen.
    *   Each pixel in the grey-level image is examined.
    *   If a pixel's brightness (intensity) is less than `T`, it's classified as part of the object (e.g., set to 0, representing a dark object).
    *   Otherwise, it's classified as part of the background (e.g., set to 255, representing a light background).
*   **Output**: The output is a binary image, meaning pixels can only have two values (e.g., 0 or 1, or 0 or 255).
*   **Applications**: Binary images produced by thresholding are commonly used for tasks like edge detection and image segmentation.
*   **Variations**:
    *   **Adaptive Thresholding**: This method adjusts the threshold value across different regions of the image, rather than using a single global threshold.
    *   **Otsu Thresholding**: An automatic method that calculates the optimal threshold value by maximizing the variance between the two classes (foreground and background) it creates.
    *   **Thresholding with Hysteresis**: Used in edge detection (e.g., Canny edge detector), this technique uses two thresholds (high and low) to keep weak edges if they connect to strong edges, helping to fill gaps.

Thresholding can be applied to grey-level images and can also be extended to color images by defining sets of color values that correspond to objects.

For more details, you can refer to the lecture notes on Thresholding & Binary Images.
Reference: `output_context_reference`: ['./documents/Lec-4-Thresholding and Binary Images.pdf', './documents/Lab 4.pdf']

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

",Thresholding is a method of image segmentation that creates binary images. It works by setting a threshold value (T) and classifying pixels as either object or background based on whether their brightness is above or below this threshold.,"University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.",
What is the difference between Otsu and Rosin thresholding methods?,"Otsu and Rosin are both thresholding methods used in image processing, but they are designed for different types of image histograms.

*   **Otsu Thresholding:** This method is typically used for images where the histogram is **bimodal**, meaning it has two distinct peaks. It works by finding the threshold value that minimizes the intra-class variance (the variance within the foreground and background pixel groups) or, equivalently, maximizes the inter-class variance (the variance between the foreground and background pixel groups). It aims to separate the two modes effectively.

*   **Rosin Thresholding (Unimodal Thresholding):** In contrast, Rosin's method is designed for histograms that are **unimodal**, meaning they have only one prominent peak. This often occurs in images where one intensity level is dominant, such as text on a plain background where the text is mainly white and there's a small amount of black. The method works by:
    1.  Finding the peak of the histogram.
    2.  Drawing a line from this peak to the top of the furthest bin.
    3.  Identifying the bin whose top is furthest from this line; the value of this bin is then chosen as the threshold.

In essence, Otsu is best for separating two distinct groups of pixels, while Rosin is more suitable for images where one dominant group needs to be separated from a smaller, less frequent group.

You can find more details about these methods in the lecture materials. For instance, the distinction between Otsu and Rosin is highlighted in the ""Thresholding & Binary Images"" slides.


**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

","Otsu's method is used for bimodal histograms, assuming two peaks that can be separated by a single threshold. Rosin's method is for unimodal histograms, where there's only one peak, and it finds the threshold by drawing a line from the peak to the furthest bin.","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f‚Äô =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¬¢ .¬∞ , ,* ,‚Ä¢ Finds the peak‚Ä¢ Draws a line from there to the top of thefurthest bin‚Ä¢ Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin‚Äôs unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f‚Äô =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods‚Ä¢ Imaging conditions and object properties can vary within a single image as well as across sets of images'e '‚Ä¢ Histograms can be too complex for any method‚Äôs|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f‚Äô =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¬¢ .¬∞ , ,* ,‚Ä¢ Finds the peak‚Ä¢ Draws a line from there to the top of thefurthest bin‚Ä¢ Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin‚Äôs unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f‚Äô =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods‚Ä¢ Imaging conditions and object properties can vary within a single image as well as across sets of images'e '‚Ä¢ Histograms can be too complex for any method‚Äôs|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a structuring element in morphological operations?,"In morphological operations, a **structuring element** is a fundamental component, similar to a filter mask used in spatial filtering. It's essentially a small binary mask that is used to probe or interact with the image being processed.

Here are some key characteristics of structuring elements:
*   **Shape and Size:** Structuring elements can vary in shape (e.g., elliptic, rectangular, cross-shaped, disk-shaped) and size. Larger structuring elements tend to produce more pronounced effects on the image.
*   **Origin:** The origin of the structuring element does not necessarily have to be in its center.
*   **Purpose:** They define the neighborhood for the morphological operation, dictating how pixels in the input image are combined to produce the output image.
*   **Creation:** You can create structuring elements using functions like `getStructuringElement` (as mentioned in the lab sheet for COMP2032) or even using NumPy's `ones` function for rectangular ones.

Think of it this way: when you perform a morphological operation like dilation or erosion, the structuring element is ""swept"" across the image. At each position, the relationship between the structuring element and the local image pixels determines the output pixel value.

You can find more details about structuring elements in the COMP2032 Morphology lecture slides and Lab Sheet 5.
<details><summary>References</summary><ul><li>./documents/Lab 5.pdf</li><li>./documents/Lec-5-Morphology.pdf</li></ul></details>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

",A structuring element is a small matrix or kernel used to probe an image in morphological operations like erosion and dilation. It defines the neighbourhood of a pixel and is used to modify the pixel's value based on the values of its neighbours.,"if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt",
Explain the process of Histogram Backprojection.,"Histogram Backprojection is a technique used to find regions in an image where colors from a target model histogram appear together. It emphasizes colors that occur as expected in the target model, rather than those that are much more prevalent and might belong to other objects.

Here's a breakdown of the process and its characteristics:

*   **Goal:** To locate objects or specific color distributions within an image based on a predefined color model (histogram).
*   **Mechanism:** It identifies areas in an image where the colors present match those in a model histogram.
*   **Color Reliance:** The method gives more weight to colors that appear in the image with a frequency similar to their expected frequency in the model. Conversely, colors that are much more common in the image (suggesting they are part of other objects) are given less weight.
*   **Robustness:**
    *   It works well even when other objects with similar colors are present in the image.
    *   It is robust to partial occlusion of the target object. Experiments have shown that even when a significant portion of an object is occluded, Histogram Backprojection can still effectively locate it.
*   **Efficiency:** Histogram Backprojection is described as ""extremely efficient,"" making it suitable for real-time applications like object tracking. For instance, with 32x32 convolutions, it can be executed four times per second.
*   **Steps (as implied by figures):**
    1.  **Ratio Histogram Backprojection:** This step seems to involve creating a ratio of the image histogram to the model histogram, highlighting areas where the model's colors are present.
    2.  **Convolution Step:** A convolution operation is then performed, likely to smooth the results or further refine the localization of the target object.

In essence, Histogram Backprojection is a powerful and efficient method for object localization and tracking, particularly effective in scenarios with color-based models and partial occlusion.

<details><summary>Reference</summary>
*   ['./documents/Swain1.pdf']</details>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

","Histogram backprojection is a technique for locating an object in an image based on a model histogram of its appearance (e.g., color). Each pixel is assigned a probability based on how well it matches the model histogram, producing a probability map where high values indicate likely object locations.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main components of a CNN?,"The main components of a Convolutional Neural Network (CNN) are:

*   **Convolutional Layer:** This layer is where the network applies filters to the input image to extract features. Hyperparameters such as the number of filters, stride, and zero-padding are required for this layer.
*   **Pooling Layer:** This layer is used to reduce the spatial dimensions (width and height) of the feature maps, which helps in reducing computational complexity and controlling overfitting.
*   **Fully Connected Layer:** These layers are typically found at the end of the CNN and are responsible for classification based on the features extracted by the convolutional and pooling layers.
*   **Activation Function:** These functions determine the output of the neural network. Common activation functions include ReLU, Sigmoid, Softmax, and Leaky ReLU.

CNNs are designed for computer vision tasks and use neural networks to split images into smaller patches, allowing them to capture both local and global relationships within images.

For more detailed information, you can refer to the following sources:
- **Lec-9-CNN.pdf**
- **Practicals with Solutions - CNN.pdf**

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","A Convolutional Neural Network (CNN) is composed of several layers, including the input layer, convolutional layer, pooling layer, activation function, fully connected layer, and output layer.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
What is the purpose of the pooling layer in a CNN?,"The pooling layer in a Convolutional Neural Network (CNN) serves to reduce the size of the feature map outputs generated by the convolutional layers. This process helps in downsampling the spatial dimensions of the input, which in turn can reduce the computational complexity of the network and control overfitting.

For example, after a convolutional layer processes an image, a max-pooling layer is commonly created to reduce the size of the feature map. This is illustrated in the `ToyModel` example provided, where a `pooling` layer is applied after each `conv` layer and `relu` activation function.

You can find more details about this in the provided context from ""Laboratory Sheet 9 - CNN.pdf"" (Page 7).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Laboratory Sheet 9 - CNN.pdf"" target=""_blank"">Laboratory Sheet 9   CNN</a>

","The pooling layer occurs after a convolutional layer and its purpose is to reduce the dimensionality of the resulting feature map. Common pooling operations include max pooling, average pooling, and global average pooling.","out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f¬∞ University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f¬∞ University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What is the difference between erosion and dilation in morphological image processing?,"Erosion and dilation are fundamental operations in morphological image processing, and they are essentially opposites of each other.

Here's a breakdown of their differences:

**Dilation:**
*   **Purpose:** Dilation expands the boundaries of foreground objects in an image. It's often used to fill in small holes or gaps within objects, connect broken segments, or make objects more prominent.
*   **Mechanism:** Imagine ""growing"" the foreground pixels. When a structuring element (a small shape or kernel) is superimposed on the image, if any part of the structuring element overlaps with a foreground pixel, the output pixel at the structuring element's origin is set to foreground.
*   **Effect:** It can be thought of as performing dilation on the background. It adds pixels to the boundaries of objects.
*   **Example Application:** Filling in gaps between broken characters in an image like `text_broken.tif` to make them whole. (output_context_reference: ['./documents/Lab 5.pdf'])

**Erosion:**
*   **Purpose:** Erosion shrinks the boundaries of foreground objects. It's used to remove small noise components, separate touching objects, or thin out objects.
*   **Mechanism:** This is the opposite of dilation. When a structuring element is superimposed on the image, the foreground pixel at the structuring element's origin is kept only if the entire structuring element fits within the foreground object. If any part of the structuring element overlaps with the background, the foreground pixel is set to background.
*   **Effect:** It can be thought of as performing dilation on the background. It removes pixels from the boundaries of objects.
*   **Example Application:** Separating touching objects (like cells or coins) or removing small isolated noise pixels. (output_context_reference: ['./documents/Lec-5-Morphology.pdf'])

**Key Relationship:**
*   Erosion is considered the dual of dilation. This means that eroding the foreground pixels is equivalent to dilating the background pixels with the same structuring element. (output_context_reference: ['./documents/Lec-5-Morphology.pdf'])

In essence, dilation expands, while erosion shrinks. They are often used in combination (e.g., in operations like opening and closing) to achieve more complex image processing tasks.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

","Erosion removes pixels on object boundaries, shrinking bright regions and enlarging dark ones. Dilation adds pixels to the boundaries of objects, enlarging bright regions and shrinking dark ones.","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.‚óè Read in the Squares.tif image from Moodle.‚óè Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.‚óè Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)‚óè Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¬¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.‚óè Read in the Squares.tif image from Moodle.‚óè Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.‚óè Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)‚óè Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¬¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and",
What is the Hough Transform used for?,"The Hough Transform is a technique primarily used in image processing for detecting shapes that can be represented by mathematical equations, such as lines and circles.

Here's a breakdown of its purpose and how it works:
*   **Shape Detection:** It's designed to identify geometric primitives like lines and circles within an image.
*   **Parameter Space Conversion:** The core idea involves converting image space into a parameter space.
*   **""Voting"" Procedure:** A ""voting"" procedure is conducted on the points that are part of the edges in the image. This means that each edge point ""votes"" for all possible shapes (defined by parameters) that could pass through it.
*   **Applications:** It's particularly useful when edge detection provides a set of points that are likely to lie on these shapes, and the goal is to determine how many such shapes exist and what their specific parameters are. For example, it can be used to find lines in an image where edge detection has already highlighted potential line segments.

In essence, while traditional template matching would try to fit a predefined shape (like a line) to different positions and orientations in an image, the Hough Transform takes each data item (an edge point) and asks what lines (or other shapes) could possibly pass through it. This makes it a more efficient approach for detecting such shapes.

For more details, you can refer to the lecture materials on Hough Transform, specifically ""Lec-6B-Hough Transform.pdf"" and ""Lab 6.pdf"".

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6B-Hough Transform.pdf"" target=""_blank"">Lec 6B Hough Transform</a>

","The Hough Transform is a feature extraction technique used to find instances of objects with a certain shape, such as lines or circles, within an image. It works by transforming points from the image space to a parameter space, where peaks correspond to the parameters of the detected shapes.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.‚óè Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a ‚Äúvoting‚Äù procedure on the points which are part of the edges in the image.‚óè Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.‚óè Apply the Hough Line Transform on the edge-detected image using HoughLines.‚óè To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (ùúå,ùúÉ) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don‚Äôt know the line parameters ‚Äì we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line‚Äôs   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.‚óè Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a ‚Äúvoting‚Äù procedure on the points which are part of the edges in the image.‚óè Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.‚óè Apply the Hough Line Transform on the edge-detected image using HoughLines.‚óè To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (ùúå,ùúÉ) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don‚Äôt know the line parameters ‚Äì we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line‚Äôs   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines",
What is the Livewire segmentation algorithm?,"The Livewire segmentation algorithm, also known as ""Intelligent Scissors,"" is a method used for segmenting images. Its core concept is to avoid traversing object boundaries while segmenting images, leveraging available information to guide the segmentation process. Livewire is famously used to segment out Regions of Interest (ROI) for further processing.

The implementation of Livewire typically involves six significant steps:
1.  Image Processing
2.  Converting Image to Graph
3.  Shortest Path Algorithm
4.  Interactive Visualisation
5.  Displaying the Image
6.  Event Handling

While Livewire is not available in OpenCV, unofficial implementations are used, often requiring libraries such as Matplotlib, Skimage, Math, Heapq, and Itertools. The algorithm is based on graph theory.

Before Livewire, Active Contour was a famously used method to extract an ROI based on its contour. Livewire improves upon this by adapting and ""snapping"" to an object boundary as a free point (controlled by cursor movement) moves, making the segmentation process more interactive and precise.

For more details, you can refer to the paper ""Intelligent Scissors for Image Composition"" by E. N. Mortensen and W. A. Barrett (ACM Computer Graphics (SIGGRAPH '95), 1995).


**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

","Livewire, also known as ""Intelligent Scissors,"" is an interactive image segmentation algorithm. It allows a user to trace an object's boundary, and the algorithm finds the minimum-cost path that follows object boundaries. It's based on graph theory and uses Dijkstra's algorithm to find the shortest path between user-selected points.","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the GrabCut algorithm?,"The GrabCut algorithm is an interactive foreground extraction technique used in image editing. It specializes in separating the foreground of an image from its background with minimal user interaction.

Here's a breakdown of what it is and how it works:

*   **Purpose:** GrabCut's primary goal is to efficiently segment an image, specifically to extract the main object (foreground) from its surrounding background.
*   **User Interaction:** Initially, the user provides a rough rectangle around the object of interest. This initial input helps the algorithm distinguish between foreground and background. Further interactions, such as using brushes to mark definite foreground or background areas, can refine the segmentation (output_context_reference: `./documents/Lec-8-Interactive Segmentation.pdf`).
*   **Underlying Principle:** GrabCut is a practical implementation that utilizes graph cuts for image segmentation (output_context_reference: `./documents/Lec-8-Interactive Segmentation.pdf`). While ""graph cut"" is a broader term for optimization techniques in image processing, GrabCut applies this specifically to foreground extraction.
*   **Iterative Optimization:** It employs an iterative version of the graph-cut optimization, which enhances its ability to accurately segment images (output_context_reference: `./documents/grab_cut.pdf`). This iterative process allows the algorithm to refine its understanding of the foreground and background over several steps.
*   **Combining Information:** GrabCut successfully combines both texture (color) information and edge (contrast) information, which are crucial for effective image segmentation (output_context_reference: `./documents/grab_cut.pdf`).
*   **Alpha Matting:** A key feature of GrabCut is its ability to estimate the alpha-matte around an object's boundary. This helps in dealing with blur and mixed pixels at object edges, resulting in smoother and more accurate extractions (output_context_reference: `./documents/Lec-8-Interactive Segmentation.pdf`, `./documents/grab_cut.pdf`).

In essence, GrabCut aims to provide high-quality foreground alpha mattes for moderately difficult images with a relatively modest amount of user effort (output_context_reference: `./documents/grab_cut.pdf`).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

","GrabCut is an interactive foreground extraction algorithm that uses iterated graph cuts. The user draws a rectangle around an object, and the algorithm automatically extracts the object from the background.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single ‚Äúout- side‚Äù lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images‚Äô, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single ‚Äúout- side‚Äù lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images‚Äô, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original",
What is histogram equalization?,"Histogram equalization is a technique in image processing used to improve the contrast of an image. It works by redistributing the pixel intensity values to achieve a more uniform distribution across the image's dynamic range. This process aims to make the image's histogram flatter, which typically enhances visual contrast [2].

**The Algorithm for Histogram Equalization:**
1.  **Compute the Cumulative Distribution Function (CDF):** The CDF of the input image's pixel intensities is calculated.
2.  **Map Pixel Intensities:** For each pixel in the input image, its corresponding output pixel intensity is determined by using the CDF as a look-up table.
3.  **Scale Output:** Since CDF values typically range from 0 to 1, the equalized image's pixel values are scaled to fit the range supported by the output image format (e.g., 0-255 for an 8-bit image) [3].

The goal is for the histogram of the output image to be approximately uniform [3].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Practicals - Histogram (Whole Image Understanding).pdf"" target=""_blank"">Practicals   Histogram (Whole Image Understanding)</a>

",Histogram equalization is a technique used to improve the contrast of an image by redistributing the pixel intensities to achieve a more uniform distribution. This is done by computing the cumulative density function (CDF) and using it as a look-up table to map input pixel values to output pixel values.,"the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee ‚ÄîACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 ‚Äì 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let‚Äôs BREAK this down‚Ä¶So, in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their","the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee ‚ÄîACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 ‚Äì 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let‚Äôs BREAK this down‚Ä¶So, in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their",
What is the difference between sampling and quantization in digital image formation?,"In digital image formation, **sampling** and **quantization** are two fundamental processes involved in converting a continuous analog image into a discrete digital image.

Here's the difference:

*   **Sampling:** This refers to the **digitization of the spatial coordinates**. In simpler terms, it's about how many pixels make up the image. When you sample an image, you are taking discrete measurements of the light intensity at specific locations across the image plane. The rate at which these samples are taken determines the **spatial resolution** of the image. If you under-sample (sample at a rate too coarse or below the Nyquist rate), you can introduce artifacts like aliasing [./documents/Lec-1B-Digital Images and Point Processes.pdf].

*   **Quantization:** This refers to the **digitization of the light intensity function**. After sampling, you have a set of discrete spatial points, each with an associated light intensity value. Quantization is the process of converting these continuous intensity values into a finite set of discrete values, often represented by a specific number of bits (e.g., 8 bits for 256 grey levels). This process determines the **grey level, colour, or radiometric resolution** of the image [./documents/Lec-1B-Digital Images and Point Processes.pdf].

In essence:
*   **Sampling** answers ""How many pixels are there?"" (spatial resolution).
*   **Quantization** answers ""How many distinct brightness/colour levels can each pixel have?"" (intensity/colour resolution).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","Sampling is the digitization of spatial coordinates, determining the spatial resolution of the image (number of pixels). Quantization is the digitization of the light intensity function, determining the number of grey levels or colors that can be represented at each pixel.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE‚Ä¶Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE‚Ä¶Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:¬´< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‚Äò@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression‚Ä¢ Map vector values (R,G,B) onto scalar values‚Ä¢ Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f‚ÄôBuilding a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f‚ÄôBuilding a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE‚Ä¶Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE‚Ä¶Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:¬´< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‚Äò@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression‚Ä¢ Map vector values (R,G,B) onto scalar values‚Ä¢ Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f‚ÄôBuilding a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f‚ÄôBuilding a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2",
What is aliasing in the context of image sampling?,"Aliasing, in the context of image sampling, occurs when two distinct signals (or images) become indistinguishable after being sampled. This can happen if the sampling rate of a new image is less than the Nyquist rate of the original image, leading to artefacts.

Essentially, the human vision system might reconstruct an image that differs from the true, unsampled image because of this undersampling. To prevent aliasing, high-frequency signals can be smoothed out before sampling.

**Reference:**
*   `./documents/Lec-1B-Digital Images and Point Processes.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",Aliasing occurs when an image is under-sampled (sampled at a rate below the Nyquist rate). This results in artifacts where two different signals (images) become indistinguishable when sampled.,"fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing ‚Ä¢ Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original ‚Ä¢ Smooth out high frequency signals before sampling so its impossible to ‚Äúsee‚Äù the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? ‚Ä¢ When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels‚Ä¢ Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can‚Äôt increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition ‚Äúseamlessly‚Äù in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass Ô¨Åltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter Ô¨Åtting of a Butterworth low-pass Ô¨Ålter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2","fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing ‚Ä¢ Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original ‚Ä¢ Smooth out high frequency signals before sampling so its impossible to ‚Äúsee‚Äù the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? ‚Ä¢ When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels‚Ä¢ Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can‚Äôt increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition ‚Äúseamlessly‚Äù in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass Ô¨Åltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter Ô¨Åtting of a Butterworth low-pass Ô¨Ålter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2",
How can noise be reduced in an image?,"Noise in an image can be reduced using several methods, broadly categorised into linear and non-linear filtering techniques.

One common approach is **averaging multiple images**. If you have several images of the same scene, taking the mean value of each corresponding pixel across these images can significantly reduce noise. This works because noise is often randomly added to each pixel value with a mean of zero, so averaging a large set of estimates for the same pixel causes these random noise values to cancel out [cite: `./documents/Lec-3A-Linear Filters.pdf`] (p. 9).

When only a single image is available, **spatial filtering** is employed. This involves averaging pixel values over a local region. Ideally, this region should only include pixels that are expected to have the same true value [cite: `./documents/Lec-3A-Linear Filters.pdf`] (p. 10).

Many filters, particularly linear ones, operate using **convolution**. This process involves multiplying each image value by a corresponding entry in a filter (or kernel) and then summing the results. This effectively calculates a weighted sum of neighbouring pixels for each output pixel [cite: `./documents/Lec-3A-Linear Filters.pdf`] (p. 11), [cite: `./documents/Lab 3.pdf`] (p. 1).

Specific linear filtering techniques include:
*   **Mean Filtering**: This involves applying a ""blur"" filter with a certain kernel size (e.g., 3x3, 5x5, 7x7) to average the pixel values within that kernel [cite: `./documents/Lab 3.pdf`] (p. 1).
*   **Gaussian Filtering**: This uses a Gaussian blur, where the weights in the filter decrease with distance from the centre, giving more importance to closer pixels. The level of blurring is controlled by the sigma ($\sigma$) parameter of the Gaussian distribution. As you change the sigma value (e.g., 0.5, 1.0, 1.5), you need to adjust the mask size accordingly [cite: `./documents/Lec-3A-Linear Filters.pdf`] (p. 8), [cite: `./documents/Lab 3.pdf`] (p. 1).

It's important to note that image noise often arises from imperfect sensors or lossy image compression methods. It is frequently modelled as additive, meaning the recorded value is the true value plus a random noise value [cite: `./documents/Lec-3A-Linear Filters.pdf`] (p. 7). Gaussian noise, where sensor measurements are slightly off the true value but tend to be near it, is a common model for this type of noise [cite: `./documents/Lec-3A-Linear Filters.pdf`] (p. 7).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3A-Linear Filters.pdf"" target=""_blank"">Lec 3A Linear Filters</a>

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

","Noise can be reduced using various filtering techniques. For additive noise like Gaussian noise, mean or Gaussian filtering can be effective. For ""salt and pepper"" noise, a median filter is often used.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I‚Äô) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‚Äò‚Äò1‚Äô* means the object has been correctly located, a ‚Äú2‚Äù indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise‚Ä¢ Noise is randomly added to each value‚Ä¢ Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern ‚Äì multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)√óP(x-1,y-1)  + F(0,-1)√óP(x,y-1)  + F(+1,-1)√óP(x+1,y-1)  + F(-1,0)√óP(x-1,y)  + ‚Ä¶  + F(+1,+1)√óP(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise‚Ä¢‚Ä¢Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI‚Ä¢ On average they give the right value‚Ä¢ They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (Œº) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2œÄœÉ  e  (x  --  2œÉ  Œº)  2 Œº28Gaussian NoiseThe level of noise is related to the Gaussian parameter, œÉœÉ = 1œÉ = 10œÉ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I‚Äô) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‚Äò‚Äò1‚Äô* means the object has been correctly located, a ‚Äú2‚Äù indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise‚Ä¢ Noise is randomly added to each value‚Ä¢ Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern ‚Äì multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)√óP(x-1,y-1)  + F(0,-1)√óP(x,y-1)  + F(+1,-1)√óP(x+1,y-1)  + F(-1,0)√óP(x-1,y)  + ‚Ä¶  + F(+1,+1)√óP(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise‚Ä¢‚Ä¢Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI‚Ä¢ On average they give the right value‚Ä¢ They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (Œº) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2œÄœÉ  e  (x  --  2œÉ  Œº)  2 Œº28Gaussian NoiseThe level of noise is related to the Gaussian parameter, œÉœÉ = 1œÉ = 10œÉ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.",
What is the difference between linear and non-linear filters?,"Linear and non-linear filters differ primarily in their operational mechanisms and their effects on image characteristics.

**Linear Filters:**
*   **Mechanism:** Linear filtering is performed using convolution. A linear filter (or kernel/mask) slides over each pixel in the image, and the output for each pixel is calculated as a weighted sum of its neighboring pixels. This involves multiplication and addition operations.
*   **Examples:** Mean filtering and Gaussian filtering are common types of linear filters.
*   **Effect:** Linear filters tend to smooth sharp image changes.

**Non-Linear Filters:**
*   **Mechanism:** Non-linear filtering does not use convolution. Instead, a non-linear mathematical operation is performed on each pixel independently, based on the values of its neighboring pixels.
*   **Examples:** Median filtering, anisotropic diffusion, and bilateral filtering are examples of non-linear filters.
*   **Effect:** Unlike linear filters, non-linear filters tend to preserve or even enhance sharp image changes, which can be beneficial for tasks like edge preservation while reducing noise.

In summary, the key distinction lies in the use of convolution and weighted sums for linear filters versus independent non-linear operations for non-linear filters, leading to different effects on image features like edges and noise.

For more details, you can refer to the lab sheet and lecture notes on linear and non-linear filtering [2, 1].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Linear filters, like mean and Gaussian filters, compute a linear function of a set of pixel values. Non-linear filters, such as the median filter, do not follow this linear relationship and tend to preserve or even enhance sharp image changes.","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.‚óè Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.‚óè Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 ‚Äì 0.1), K (0 ‚Äì 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.‚óè Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 ‚Äì 100) and sigma (50 ‚Äì 250). You may keep both sigma values the same for simplicity‚Äôs sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  ‚Ä¢ Median filtering  ‚Ä¢ Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‚Äòesau ad oe ‚Ç¨ THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i‚Ä¢ A false saturation gives a white spot in the image (salt)‚Ä¢ A failed response gives a black spot in the image (pepper)‚Ä¢ Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.‚óè Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.‚óè Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 ‚Äì 0.1), K (0 ‚Äì 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.‚óè Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 ‚Äì 100) and sigma (50 ‚Äì 250). You may keep both sigma values the same for simplicity‚Äôs sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  ‚Ä¢ Median filtering  ‚Ä¢ Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‚Äòesau ad oe ‚Ç¨ THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i‚Ä¢ A false saturation gives a white spot in the image (salt)‚Ä¢ A failed response gives a black spot in the image (pepper)‚Ä¢ Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the",
Explain the concept of superpixels.,"Superpixels are becoming increasingly popular in computer vision applications. They are generated by algorithms that cluster pixels in a combined five-dimensional color and image plane space to efficiently create compact, nearly uniform superpixels.

The concept of superpixels involves grouping pixels that share similar characteristics (like color and proximity) into larger, perceptually meaningful regions. This process helps reduce the complexity of an image by working with these larger segments instead of individual pixels, which can be beneficial for subsequent computer vision tasks.

Some algorithms, like the SLIC (Simple Linear Iterative Clustering) superpixels algorithm, are designed to produce a desired number of regular, compact superpixels with low computational overhead. This algorithm is known for its simplicity, requiring only the number of desired superpixels as an input parameter, and its efficiency, scaling linearly in computational cost and memory usage.

Superpixels have been shown to be effective in various applications, including object category recognition and medical image segmentation, often providing better quality and higher computational efficiency compared to other state-of-the-art methods.

To summarize, superpixels are essentially perceptually uniform regions formed by grouping similar pixels, simplifying image processing tasks and improving efficiency.

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","Superpixels are groups of pixels that are similar in color and proximity. They provide a convenient primitive from which to compute local image features, capturing redundancy in the image and reducing the complexity of subsequent image processing tasks.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oÔ¨Äer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eÔ¨Écacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eÔ¨Éciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiÔ¨Åcation model for segmentation. ICCV (2003) 10‚Äì172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oÔ¨Äer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by Ô¨Ånding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oÔ¨Äer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eÔ¨Écacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eÔ¨Éciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiÔ¨Åcation model for segmentation. ICCV (2003) 10‚Äì172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oÔ¨Äer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by Ô¨Ånding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient",
What is the SLIC algorithm for generating superpixels?,"The SLIC (Simple Linear Iterative Clustering) algorithm is a method for generating superpixels, which are clusters of pixels that share similar characteristics. It's designed to be efficient and produce compact, nearly uniform superpixels.

Here's a breakdown of what makes SLIC stand out:

*   **Clustering in 5D Space:** SLIC clusters pixels in a combined five-dimensional space, which includes both color (e.g., L\*a\*b\* color space) and image plane coordinates (x, y). This allows it to group pixels that are not only similar in color but also spatially close.
*   **Simplicity and Efficiency:** The algorithm is known for its simplicity, making it very easy to use. It's also computationally efficient, meaning it can generate superpixels at a lower cost compared to some other methods.
*   **Single Parameter Control:** A key advantage of SLIC is that a single parameter specifies the desired number of superpixels, offering straightforward control over the output.
*   **Performance:** Experiments have shown that SLIC can achieve segmentation quality equal to or greater than many state-of-the-art methods, as measured by metrics like boundary recall and under-segmentation error.

While some segmentation schemes are very general, SLIC is specifically tailored for superpixel clustering using a particular distance measure, making it more streamlined for this purpose.

For more detailed information, you can refer to the original paper on SLIC Superpixels.


**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","SLIC (Simple Linear Iterative Clustering) is an algorithm that clusters pixels in a combined five-dimensional color and image plane space to generate compact, nearly uniform superpixels. It is known for its efficiency and simplicity, with a single parameter specifying the number of superpixels.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciÔ¨Åc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oÔ¨Äer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481√ó321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciÔ¨Åc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oÔ¨Äer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481√ó321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required",
What is the purpose of the activation function in a CNN?,"The activation function in a Convolutional Neural Network (CNN) plays a crucial role in introducing non-linearity into the network.

Here's why it's important:

*   **Introducing Non-linearity:** Without activation functions, a CNN would only be able to learn linear transformations of its input. Real-world data is often complex and non-linear. Activation functions allow the network to learn and represent these complex, non-linear relationships in the data.
*   **Enabling Complex Pattern Recognition:** By adding non-linearity, activation functions enable the network to learn more intricate patterns and features from the input images, which is essential for tasks like image classification, object detection, and segmentation.

Activation functions are one of the key components of a CNN, alongside the input layer, convolutional layer, pooling layer, and fully connected layer.

Some commonly used activation functions, particularly in the output layer, include Sigmoid and Softmax. ReLU (Rectified Linear Unit) and Leaky ReLU are also popular choices, often used in hidden layers.

For further reading, you can refer to the following sources:
*   Alzubaidi, L. et al. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74. (output_context_reference: ['./documents/Practicals with Solutions - CNN.pdf'])
*   Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. (output_context_reference: ['./documents/Lec-9-CNN.pdf'])

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","The activation function, also known as the transfer function, determines the output of a neural network. It introduces non-linearity into the model, allowing it to learn more complex patterns. Common activation functions include ReLU, Sigmoid, and Softmax.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between overfitting and underfitting in the context of CNNs?,"In the context of Convolutional Neural Networks (CNNs), underfitting and overfitting are common problems that relate to how well a model learns from the training data and generalizes to new, unseen data.

Here's the difference:

*   **Underfitting:** This occurs when the model is **not learning enough** from the training data. It's too simple to capture the underlying patterns in the data, leading to poor performance on both the training data and new data.
*   **Overfitting:** This occurs when the model is **learning too much** from the training data. It memorizes the training data, including noise and specific details, rather than learning generalizable patterns. As a result, it performs very well on the training data but poorly on new, unseen data.

To put it simply:
*   **Underfitting:** Model is too simple.
*   **Overfitting:** Model is too complex and has memorized the training data.

This information is derived from the retrieved context, specifically from a section discussing common problems in CNNs.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on new, unseen data. Underfitting happens when the model is not complex enough to capture the underlying patterns in the data and performs poorly on both training and test data.","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What are some common CNN architectures?,"Some common CNN architectures include R-CNN, Fast R-CNN, and Faster R-CNN. These are all types of Convolutional Neural Networks (CNNs).

**References**
* Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","Some well-known CNN architectures include LeNet, AlexNet, VGG, ResNet, GoogLeNet, and MobileNet. There are also region-based CNNs (R-CNN, Fast R-CNN, Faster R-CNN) used for object detection.","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:",
What is the watershed algorithm for segmentation?,"The Watershed algorithm is a segmentation method that falls under the category of gradient ascent algorithms. It works by treating an image as a topographic map, where intensity values represent altitudes.

Here's how it generally works:

*   **Gradient Ascent:** The algorithm performs gradient ascent from local minima in the image plane.
*   **Catchment Basins:** As it ascends, it identifies ""catchment basins,"" which are regions where water would collect if the image were a physical landscape.
*   **Watersheds:** The ""watersheds"" are the lines that separate these catchment basins. These lines effectively form the boundaries between different segmented regions.

A fast version of the algorithm, proposed by Vincent and Soille, is based on the queuing of pixels. The Watershed algorithm is a region-based segmentation approach that emphasizes finding boundaries between regions.

For example, if you have an image fragment with intensity values, the Watershed algorithm would segment it by labeling regions based on these intensity values, much like how a landscape would be divided by mountain ridges (watersheds) separating valleys (catchment basins).

You can find more information about this in the ""Practical Exercises with Solutions Segmentation"" document.

**References:**
*   `./documents/Practicals with Solutions - Segmentation.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - Segmentation.pdf"" target=""_blank"">Practicals with Solutions   Segmentation</a>

<a href=""http://localhost:5003/documents/Practicals - Segmentation.pdf"" target=""_blank"">Practicals   Segmentation</a>

","The watershed algorithm is a region-based segmentation method. It treats the gradient magnitude image as a topographic surface, flooding basins from local minima until watershed lines (boundaries) are formed.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eÔ¨Écient level-set based geometric Ô¨Çow. The geometric Ô¨Çow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the Ô¨Åve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are ‚Äì views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I¬∞v0 ‚ÄúACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -¬∞boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eÔ¨Écient level-set based geometric Ô¨Çow. The geometric Ô¨Çow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the Ô¨Åve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are ‚Äì views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I¬∞v0 ‚ÄúACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -¬∞boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!",
What is the purpose of a structuring element in morphological operations?,"In morphological operations, a structuring element serves a similar purpose to a mask in convolution processes. It is a critical component for defining the shape and extent of the operation.

Here's a breakdown of its purpose:
*   **Defining the Operation's Shape and Size:** Structuring elements vary in shape and size, much like filter masks used in spatial filtering. They can be rectangular, elliptic, cross-shaped, or other forms. The choice of shape and size dictates how the morphological operation (e.g., dilation, erosion) will interact with the image's features.
*   **Determining Expansion or Contraction:** For operations like dilation, the structuring element defines the amount and direction of expansion of foreground regions. In essence, it specifies how much the boundaries of an object will grow.
*   **Similar to Filter Masks:** While similar to filter masks, structuring elements are used for identifying rather than weighting pixels.
*   **Impact on Results:** Larger structuring elements generally produce more extreme effects. However, similar effects can often be achieved by repeatedly applying operations with a smaller, but similarly shaped, structuring element.

In essence, the structuring element is the ""tool"" that dictates how morphological operations will modify the shape and structure of objects within an image.

You can find more information about structuring elements in the Lab 5 and Morphology lecture materials.
[./documents/Lab 5.pdf]
[./documents/Lec-5-Morphology.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

",A structuring element is a small kernel used in morphological operations like erosion and dilation. It defines the shape of the neighborhood of a pixel that is considered when applying the operation.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony",
What is the difference between opening and closing in morphological image processing?,"Opening and closing are fundamental operations in morphological image processing, both based on set theory and requiring a structuring element. While the provided context doesn't explicitly define the difference between them, it does mention them as key morphological operations along with erosion and dilation.

In general image processing:
*   **Opening** typically involves an erosion followed by a dilation. It's used to remove small objects, smooth contours, and break narrow connections.
*   **Closing** typically involves a dilation followed by an erosion. It's used to fill small holes, smooth contours, and connect nearby objects.

Both operations are useful for various image processing tasks, such as noise reduction or shape analysis. For instance, the context asks, ""How can we use morphological operators to remove the text? A: erode white C: close white B: dilate white D: open white,"" implying that these operations have distinct effects suitable for different problems.

If you'd like to delve deeper into the specific mathematical definitions or practical applications of opening and closing, I can try to find more information or you could contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) for a more detailed explanation.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

","Opening is an erosion followed by a dilation, which can remove small objects and smooth object contours. Closing is a dilation followed by an erosion, which can fill small holes and connect nearby objects.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left",
How can color histograms be used for image retrieval?,"Color histograms are a powerful tool for image retrieval due to their ability to represent the color content of an image in a way that is robust to certain transformations.

Here's how they are used:

*   **Representation for Object Identification:** A color histogram counts the occurrences of each color within an image given a discrete color space. This provides an excellent representation for identifying objects because different objects often have distinct color distributions.
*   **Invariance to Transformations:** Color histograms are invariant to several common image transformations, including translation and rotation about an axis perpendicular to the image plane. They also change slowly with rotation about other axes, occlusion, and changes in distance to the object. This robustness makes them suitable for matching objects even when their appearance varies slightly.
*   **Distinguishing Objects:** Histograms for different objects can vary significantly, allowing for discrimination between them. While there's a vast number of possible histograms (exponential in the number of colors), this large feature space can be effectively utilized for recognition.
*   **Comparison Methods:**
    *   **Histogram Intersection:** This method is particularly well-suited for comparing histograms for recognition tasks because it doesn't require precise separation of the object from its background or occluding objects. Experiments have shown that Histogram Intersection can distinguish models from large databases, is robust to occlusion, and can handle variations in image and histogram resolution. It's also efficient, with only a small number of histograms needed to represent a 3D object. However, its effectiveness under variable lighting conditions relies on an effective color-constancy algorithm.
    *   **Incremental Intersection:** A modification of Histogram Intersection, this method allows for efficient indexing into very large databases.
*   **Object Localization (Histogram Backprojection):** Once a model histogram is established for a known object (the ""target""), Histogram Backprojection can be used to find the location of that object within an image. This algorithm identifies regions in the image where the colors present in the model histogram are found.

In essence, color histograms provide a compact and relatively invariant signature of an image's color content, which can then be compared using techniques like Histogram Intersection to retrieve similar images or locate specific objects.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

","Color histograms can be used to represent the color distribution of an image. By comparing the color histogram of a query image with the histograms of images in a database using a similarity measure like histogram intersection, similar images can be retrieved.","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is",
What is histogram intersection?,"Histogram intersection is a method used to compare two histograms, typically an image histogram and a model histogram. It quantifies how much two representations (histograms) overlap.

Here's a breakdown of what it is and how it works:

*   **Definition:** Given two histograms, J and M, each with 'n' bins, their intersection is calculated by summing the minimum value for each corresponding bin across both histograms.
    Mathematically, it's defined as:
    $$ \sum_{j=1}^{n} \min(J_j, M_j) $$
    where $J_j$ and $M_j$ are the values of the j-th bin in histograms J and M, respectively.

*   **Purpose:**
    *   It measures how much of a query (model) may be present in a target image (and vice-versa).
    *   It's particularly suited for object recognition because it does not require precise segmentation of the object from its background or occluding objects.

*   **Normalization:** To obtain a fractional match value between 0 and 1, the intersection result is normalized by the total number of pixels in the model histogram. The normalized match value is:
    $$ \frac{\sum_{j=1}^{n} \min(J_j, M_j)}{\text{Number of pixels in Model Histogram}} $$

*   **Robustness:** Histogram Intersection is known to be robust to various challenges in image recognition, including:
    *   Distractions in the background.
    *   Varying viewpoints of the object.
    *   Occlusion (partial hiding of the object).
    *   Varying image resolutions.
    *   It is relatively insensitive to histogram resolution.

*   **Key Behavior:** The normalized histogram intersection match value is not significantly reduced by distracting pixels in the background. It only increases if a background pixel has the same color as one in the model and the number of pixels of that color in the object is less than in the model.

*   **Historical Context:** The concept was notably introduced in a paper by Ballard and Swain in 1991, where they demonstrated its effectiveness in recognizing objects using color histograms.

In essence, it tells you how much commonality exists between the color distribution of a known model and a given image, without needing perfect isolation of the object.

**References:**
*   [./documents/Swain1.pdf]
*   [./documents/Lec-2-Histogram (Whole Image Understanding).pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

",Histogram intersection is a method for comparing two histograms. It calculates the sum of the minimum values for each corresponding bin in the two histograms. This measure is robust to occlusion and changes in image resolution.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‚Äò(and ‚Äò-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  √•  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R ‚Äì G - BY = 2 * B ‚Äì R ‚Äì G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly‚Äî distractions in the background of the object,‚Äî viewing the object from a variety of viewpoints,‚Äî occlusion,‚Äî varying image resolution,‚Äî varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‚Äò(and ‚Äò-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  √•  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R ‚Äì G - BY = 2 * B ‚Äì R ‚Äì G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly‚Äî distractions in the background of the object,‚Äî viewing the object from a variety of viewpoints,‚Äî occlusion,‚Äî varying image resolution,‚Äî varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to",
What are the different types of redundancy in image compression?,"In image compression, there are three main types of redundancy:

1.  **Coding redundancy:** This refers to the redundancy introduced by the way pixel values are represented. If an inefficient coding scheme is used, more bits than necessary might be used to represent the grey levels of an image. The grey level histogram of an image gives the probability of occurrence of each grey level, and the average number of bits required per pixel can be calculated based on this. If all pixels take the same number of bits, it might not be optimal if some grey levels occur much more frequently than others.
    <details><summary>Click for Context</summary>
    ['./documents/Lec-10-Image Compression.pdf']
    </details>

2.  **Spatial redundancy:** This type of redundancy arises from the correlation between neighboring pixels in an image. Often, the value of a pixel can be predicted from the values of its surrounding pixels. For example, in a smooth region of an image, adjacent pixels are likely to have very similar intensity values.
    <details><summary>Click for Context</summary>
    ['./documents/Lec-10-Image Compression.pdf']
    </details>

3.  **Psychovisual redundancy:** This refers to information that is visually irrelevant to the human visual system. The human eye is not equally sensitive to all visual information. Therefore, some information in an image can be removed or represented with less precision without a noticeable degradation in perceived quality.
    <details><summary>Click for Context</summary>
    ['./documents/Lec-10-Image Compression.pdf', './documents/Practicals - Image Compression.pdf', './documents/Practicals with Solutions - Image Compression.pdf']
    </details>

Understanding and exploiting these types of redundancy are fundamental to effective image compression techniques like JPEG and GIF.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Image Compression.pdf"" target=""_blank"">Practicals with Solutions   Image Compression</a>

","There are three main types of redundancy in image compression: coding redundancy, spatial redundancy, and psychovisual redundancy.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=‚Äî =0,2,...,L‚Äî1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1√•() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=‚Äî =0,2,...,L‚Äî1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1√•() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1",
What is Huffman coding?,"Huffman coding is a method used in image compression to reduce redundancy, specifically coding redundancy. It's a component within larger compression systems like GIF and JPEG.

In essence, Huffman coding assigns variable-length codes to pixel values based on their frequency of occurrence in an image. More frequent pixel values are assigned shorter codes, while less frequent ones receive longer codes. This results in a compressed representation of the image data.

You can see an example of how to derive a Huffman code for a given set of pixel values and their frequencies in the practical exercises for image compression.

If you'd like to try an example or have any more questions about how it works, just let me know!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Image Compression.pdf"" target=""_blank"">Practicals with Solutions   Image Compression</a>

","Huffman coding is a lossless data compression algorithm. It assigns variable-length codes to input characters, with the lengths of the assigned codes being based on the frequencies of corresponding characters. More frequent characters get shorter codes.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
How does JPEG compression work?,"JPEG compression works by leveraging several techniques to reduce file size, primarily by exploiting psychovisual redundancies in images. Here's a breakdown of the process:

1.  **Quantisation of DCT Blocks**: The core of JPEG compression involves transforming blocks of image data using the Discrete Cosine Transform (DCT). After the DCT, the resulting coefficients are ""quantised."" This means that some of the less visually significant information (often high-frequency details) is discarded or represented with less precision. Increasing the amount of quantisation reduces the file size but can introduce noticeable artefacts, such as blocks becoming visible in the image.

2.  **Zigzag Coding**: After quantisation, the elements of the quantised DCT block are reordered using zigzag coding. This reordering arranges the coefficients in a way that groups low-frequency components (DC and early AC coefficients) together, followed by higher-frequency AC coefficients.

3.  **Run-Length Encoding of Zeros**: The zigzag ordering often produces long sequences of zeros, especially after quantisation. These sequences of zeros, as well as other AC values, are then efficiently represented using run-length encoding. This is a form of lossless compression where repeated values are stored as a count and the value itself, rather than storing each individual zero.

4.  **Overall Compression System**: JPEG is a complete compression scheme that incorporates these components. It's designed to manage the trade-off between file size and image quality, allowing users to choose different levels of compression (e.g., low, medium, or high JPEG compression, which corresponds to different quantisation levels). Higher compression leads to smaller file sizes but more noticeable artefacts.

In essence, JPEG compression works by transforming image data, selectively discarding information that is less perceptible to the human eye (psychovisual redundancy), and then efficiently encoding the remaining data.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

","JPEG compression is a lossy compression method that involves dividing an image into blocks, applying a Discrete Cosine Transform (DCT) to each block, quantizing the DCT coefficients, and then using Huffman coding to compress the quantized coefficients.","Compression‚Äî26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 ‚Äî3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.‚Äî.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -‚Äî . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf‚Äù te . = om ¬´File size: } 49K es 100 dpi high JPEG compression ~ ‚Äî ye ‚Äò ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compressionü§î48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well‚Ä¶the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE‚Ä¶CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing ‚Äì theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f‚Äô(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = √¶ √ß √ß √® 1 MN M - 1 N - 1 √•√• x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 √∂ √∑ √∑ √∏ 2/1 SNR = M - 1 N - 1 √•√• ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - √•√• ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" ùëá ùëü = ( ùëù"" ùë§ ùëëùë§ !ùëëùë† ùëëùëü = ùëëùëá(ùëü) ùëëùëü = ùëë ùëëùëü "" ( ! ùëù"" ùë§ ùëëùë§ = ùëù""(ùëü)ùëù# ùë† = ùëù"" ùëü ùëëùëü ùëëùë† = ùëù"" ùëü 1 ùëù"" ùëü = 1 0 ‚â§ ùë† ‚â§ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram","Compression‚Äî26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 ‚Äî3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.‚Äî.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -‚Äî . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf‚Äù te . = om ¬´File size: } 49K es 100 dpi high JPEG compression ~ ‚Äî ye ‚Äò ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compressionü§î48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well‚Ä¶the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE‚Ä¶CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing ‚Äì theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f‚Äô(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = √¶ √ß √ß √® 1 MN M - 1 N - 1 √•√• x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 √∂ √∑ √∑ √∏ 2/1 SNR = M - 1 N - 1 √•√• ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - √•√• ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" ùëá ùëü = ( ùëù"" ùë§ ùëëùë§ !ùëëùë† ùëëùëü = ùëëùëá(ùëü) ùëëùëü = ùëë ùëëùëü "" ( ! ùëù"" ùë§ ùëëùë§ = ùëù""(ùëü)ùëù# ùë† = ùëù"" ùëü ùëëùëü ùëëùë† = ùëù"" ùëü 1 ùëù"" ùëü = 1 0 ‚â§ ùë† ‚â§ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram",
What is the difference between RGB and HSV color spaces?,"The main difference between RGB (Red, Green, Blue) and HSV (Hue, Saturation, Value) color spaces lies in how they represent color.

*   **RGB** is an **additive** color space where colors are created by mixing different intensities of red, green, and blue light. For example, mixing all three at full intensity creates white.
*   **HSV** is based on color rather than light and separates color information from intensity. It describes color using three components:
    *   **Hue:** This represents the general color (e.g., red, blue, green) and can be thought of as an angle around a color wheel.
    *   **Saturation:** This indicates how strongly colored or pure the color is. A low saturation means the color is closer to a shade of gray, while high saturation means it's vivid.
    *   **Value:** This indicates the brightness or darkness of the color.

A key advantage of HSV is that it **separates color from intensity (value)**, making it less sensitive to changes in illumination compared to RGB. This can be particularly useful in image processing applications where lighting conditions might vary.

For example, when working with plants, you might only be interested in the 'greenness' of an image, which can be easily extracted in HSV or by using specific combinations of RGB channels like `G - (R+B)/2`.



**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","RGB (Red, Green, Blue) is an additive color model based on light, commonly used in displays. HSV (Hue, Saturation, Value) is a color model based on human perception of color. Hue represents the color, saturation represents the intensity of the color, and value represents the brightness. HSV separates color information from intensity, making it less sensitive to illumination changes.","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie‚Ä¢ RGB is OK, but there are other general &application-specific colour spaces‚Ä¶‚Ä¢If you want to work with plants, you might use|just G, or ‚Äògreenness‚Äô:mGG ‚Äì (R+B)/2HSV is based on colour rather   than light  ‚Ä¢ Hue - what general colour is it  ‚Ä¢ Saturation ‚Äì how strongly coloured is it  ‚Ä¢ Value ‚Äì how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject‚Äôs surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie‚Ä¢ RGB is OK, but there are other general &application-specific colour spaces‚Ä¶‚Ä¢If you want to work with plants, you might use|just G, or ‚Äògreenness‚Äô:mGG ‚Äì (R+B)/2HSV is based on colour rather   than light  ‚Ä¢ Hue - what general colour is it  ‚Ä¢ Saturation ‚Äì how strongly coloured is it  ‚Ä¢ Value ‚Äì how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject‚Äôs surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is gamma correction?,"Gamma correction is a technique used in image processing to adjust the intensity of an image. It's particularly important because the way display hardware (like a screen) renders intensity is often not linear with the input voltage it receives.

Here's a breakdown of what gamma correction is and why it's used:

*   **The Problem:** When you send a voltage proportional to a pixel's intensity to a screen, the screen displays an intensity that is related to that voltage, but not always in a simple linear fashion. For instance, a common relationship is that the displayed luminance (L) is proportional to the input voltage (V) raised to a power, such as V^2.5. This non-linear relationship means that what you intend to display might not be what is actually seen on the screen.
*   **The Solution (Gamma Correction):** To ensure that the displayed image matches the intended intensity, you need to transform the image data *before* it's sent to the display. This transformation is called gamma correction. The goal is to create a new image where, when its pixel values are converted to voltages and then displayed by the screen's non-linear response, the final displayed intensity is what you originally wanted.
*   **The Formula:** A common formula for gamma correction is:
    $O = I^\gamma \times 255$
    where:
    *   $O$ is the new, corrected output intensity value.
    *   $I$ is the original input intensity value (often normalized to a range like 0-1, or handled directly with a scaling factor like 255 as seen in the provided context for output).
    *   $\gamma$ (gamma) is the gamma value, which specifies the shape of the curve that maps the input intensities to the output intensities.
*   **Impact of Gamma Value:**
    *   If $\gamma < 1$, the mapping is weighted towards brighter output values, making the image appear brighter.
    *   If $\gamma > 1$, the mapping is weighted towards darker output values, making the image appear darker.
    *   If $\gamma = 1$, the mapping is linear, and no correction is applied.
*   **Practical Application:** In essence, gamma correction ""pre-distorts"" the image data so that when the display device applies its own non-linear transformation, the net result is a linear and accurate representation of the original image's intended intensities.

You can find more details on this in the ""Digital Images and Point Processes"" lecture slides. (Refer to: output_context_reference['./documents/Lec-1B-Digital Images and Point Processes.pdf'], output_context_reference['./documents/Lab 1B.pdf'])

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",Gamma correction is a non-linear operation used to adjust the brightness and contrast of an image. It is often used to compensate for the non-linear response of display devices.,"a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ is the gamma value.ùõæùëÇ = ùêº 255 √ó 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if‚Äô University of Nottingham ‚Äî~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values ùêº to create ùëÇ (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:‚óè Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.‚óè Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.‚Ä¢ When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform‚Ä¢ You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think¬∞.RAMP=25L ‚âà V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction‚Ä¢ We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each","a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ is the gamma value.ùõæùëÇ = ùêº 255 √ó 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if‚Äô University of Nottingham ‚Äî~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values ùêº to create ùëÇ (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:‚óè Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.‚óè Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.‚Ä¢ When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform‚Ä¢ You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think¬∞.RAMP=25L ‚âà V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction‚Ä¢ We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each",
What is anisotropic diffusion?,"Anisotropic diffusion is a process used in image processing for smoothing images. Unlike traditional diffusion methods (like mean or Gaussian filters) that smooth equally in all directions, anisotropic diffusion is directional.

Here's a breakdown of the concept:

*   **Anisotropic** means ""not the same in all directions.""
*   **Basic Idea:** While mean and Gaussian filters make each pixel more similar to all its neighbors, anisotropic diffusion makes a pixel more like those neighbors to which it is *already similar* to. This helps in preserving important features like edges while smoothing homogeneous regions.
*   **How it works:** It uses a ""similarity function,"" `s(p,q)`, which ranges from 0 to 1.
    *   If pixels `p` and `q` are very similar (e.g., have close intensity values), `s(p,q)` will be close to 1.
    *   If pixels `p` and `q` are very different, `s(p,q)` will be close to 0.
    *   This function `s(p,q)` is then used to compute a weighted average of pixel values, where the weight of a neighbor `q` on pixel `p` depends on their similarity. The new value at pixel `p` is based on this weighted average of its neighbors.

In essence, anisotropic diffusion can be seen as a process where smoothing (diffusion) is encouraged within regions of similar intensity but inhibited across boundaries or edges where intensity changes abruptly.

For more details, you can refer to the lecture notes on Non-Linear Filters.
<details><summary>Citation</summary><ul><li>'./documents/Lec-3B-Non-Linear Filters.pdf'</li></ul></details>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Anisotropic diffusion is an edge-preserving smoothing technique. Unlike isotropic filters like Gaussian blur that smooth uniformly in all directions, anisotropic diffusion encourages smoothing within regions of similar intensity while inhibiting it across strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is bilateral filtering?,"Bilateral filtering is a non-linear filtering technique used in image processing. It modifies Gaussian smoothing by incorporating two weighting factors:
1.  **Spatial Weighting:** One Gaussian weights pixels that are near the source pixel (similar to traditional Gaussian blur).
2.  **Range Weighting:** Another Gaussian weights pixels that have similar intensity to the source pixel.

This dual weighting allows bilateral filters to smooth images while preserving edges. If the similarity function (range weighting) were always 1, bilateral filtering would essentially become a mean filter.

You can find more details on this in the lecture materials on Non-Linear Filters.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Bilateral filtering is a non-linear, edge-preserving smoothing filter. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels. The weights depend not only on the spatial distance of pixels but also on the difference in their intensity values.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding‚Ä¶Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x‚Äî)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ¬∞ Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding‚Ä¶Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x‚Äî)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ¬∞ Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3",
What is the Canny edge detector?,"The Canny edge detector is an optimal edge detection algorithm that aims to identify all edges in an image while minimizing false positives and ensuring accurate localization. It was developed by John Canny, who sought to find the optimal edge detector assuming a perfect step edge in Gaussian noise.

The Canny operator is closely approximated by the first derivative of a Gaussian function. It is designed to satisfy three main criteria for optimal edge detection:
1.  **Good Detection:** It should mark all the true edges and only those edges.
2.  **Good Localization:** The detected edge points should be as close as possible to the real edges.
3.  **Minimal Response:** Each edge should be reported only once, avoiding multiple responses for a single edge.

The Canny edge detection process typically involves several steps:
1.  **Noise Reduction:** Gaussian smoothing is applied to the image to reduce noise before calculating gradients.
2.  **Gradient Calculation:** Sobel filters are used in both the x and y directions to calculate the gradient.
3.  **Non-Maximum Suppression:** To obtain thin edges, only the local maximums of the gradient in the gradient direction are saved.
4.  **Hysteresis Thresholding:** This final step selects edges using two threshold values (min and max). Edges above the max threshold are considered strong edges, edges below the min threshold are discarded as non-edges, and edges between the two thresholds are classified based on their connectivity to strong edges.

The Canny edge detector can be considered an extension of Sobel edge detection, incorporating additional steps for improved edge detection performance.

**References:**
*   Lec-6A-Derivative and Edges.pdf
*   Lab 6.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","The Canny edge detector is a multi-stage edge detection algorithm. It involves noise reduction using a Gaussian filter, finding intensity gradients, non-maximum suppression to thin edges, and hysteresis thresholding to connect weak edges to strong ones.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian",
What is unsharp masking?,"Unsharp masking is an image sharpening technique that enhances edges without significantly affecting the rest of the image. It works by creating a ""mask"" that highlights differences between the original image and a smoothed version of it, and then adding this mask back to the original image to make edges more noticeable.

Here's a breakdown of the steps involved:
1.  **Take the original image:** This is the starting point.
2.  **Gaussian smooth it:** A blurred version of the original image is created using a Gaussian filter. This step effectively removes high-frequency details, including edges.
3.  **Subtract the smoothed version from the original:** This subtraction creates the ""unsharp mask."" This mask essentially captures the high-frequency components that were removed by the smoothing, which correspond to the edges.
4.  **Add the mask to the original:** By adding this unsharp mask back to the original image, the edges become more prominent and visually sharper.

While unsharp masking makes edges noticeably sharper, it's important to note that it can also enhance noise, and sometimes, it can sharpen edges too much.

This technique relies on the smoothing effect of a Gaussian function to introduce a difference between the original and processed image, which is then exploited to enhance the edges.

For more details, you can refer to the lecture notes on Derivative and Edges.


**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Unsharp masking is an image sharpening technique. It works by subtracting a blurred version of the image from the original image to create a mask of the image's high-frequency components, which is then added back to the original image to sharpen it.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -‚Äî‚Äî‚Äî__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -‚Äî‚Äî__, I eee C ooo ‚Äî_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask ‚Äî En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make‚Äô ¬∞ the edge appear more obvious -‚Äî‚Ñ¢‚Äî_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  ‚Ä¢  Even if they are noise  ‚Ä¢  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f‚ÄôDerivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I‚Ä¢ Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKA œÉ = 1.5BœÉ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -‚Äî‚Äî‚Äî__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -‚Äî‚Äî__, I eee C ooo ‚Äî_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask ‚Äî En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make‚Äô ¬∞ the edge appear more obvious -‚Äî‚Ñ¢‚Äî_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  ‚Ä¢  Even if they are noise  ‚Ä¢  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f‚ÄôDerivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I‚Ä¢ Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKA œÉ = 1.5BœÉ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is a structuring element in morphology?,"In the context of morphological operations in image processing, a **structuring element** is a fundamental component. It can be understood as:

*   **A patch of the binary image to be processed.** It's essentially a small pattern or shape that is used to probe and interact with the input image. (output_context_reference: './documents/Lec-5-Morphology.pdf')
*   **Similar to a mask in a convolution process**, but instead of weighting pixels, it identifies them. (output_context_reference: './documents/Lec-5-Morphology.pdf')
*   **A set of points** that defines the shape and size of the neighborhood to be considered during morphological operations like dilation and erosion. (output_context_reference: './documents/Lab 5.pdf')
*   **A binary mask** that can vary in shape and size, much like filter masks used in spatial filtering. (output_context_reference: './documents/Lec-5-Morphology.pdf', './documents/Lab 5.pdf')

**Key characteristics and functions of structuring elements:**

*   **Shape and Size:** Structuring elements can be of various shapes (e.g., rectangular, elliptic, cross-shaped) and sizes. (output_context_reference: './documents/Lab 5.pdf')
*   **Origin:** They have an origin (or center point), which is crucial for positioning the element during operations. The origin doesn't necessarily have to be in the center. (output_context_reference: './documents/Lec-5-Morphology.pdf')
*   **Influence on Operations:** The choice of structuring element significantly impacts the outcome of morphological operations. Larger structuring elements generally produce more extreme effects. (output_context_reference: './documents/Lec-5-Morphology.pdf')
*   **Necessity:** Morphological operations like dilation and erosion are meaningless without specifying a structuring element, just as filtering an image requires a specified filter. (output_context_reference: './documents/Lec-5-Morphology.pdf')

In essence, the structuring element defines the ""neighborhood"" or ""template"" that is swept across the image to perform operations, determining how pixels are added or removed from objects.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

","A structuring element is a small matrix of pixels, typically with a defined shape (e.g., square, disk, cross), used as a probe in morphological operations. It defines the neighborhood of a pixel to be processed.","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B‚Ä¢ The boundaries between foreground and background are often smoothed in the process‚Ä¢ The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I‚Ä¢ Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B‚Ä¢ The boundaries between foreground and background are often smoothed in the process‚Ä¢ The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I‚Ä¢ Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô",
How can you use a binary image as a mask?,"Binary images can be effectively used as masks to select specific pixels in another image for an operation. This means that you can define an area of interest in a binary image (where pixels are either 0 for black or 255 for white) and then apply an operation only to the corresponding pixels in a different image.

For example, if you have a binary mask where a certain region is white and the rest is black, you can use this mask to apply an operation (like changing color or intensity) only to the pixels in the original image that correspond to the white region in the mask, while leaving the pixels corresponding to the black region untouched or applying a different operation to them. Python provides support for this kind of masking operation.

[output_context_reference: ./documents/Practicals - Morphology.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

","A binary image can be used as a mask to apply an operation to a specific region of another image. Pixels in the binary mask with a value of 1 (or True) indicate the region where the operation should be applied, while pixels with a value of 0 (or False) are ignored.","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing‚Äô a ume. For each subse‚Ç¨; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing‚Äô a ume. For each subse‚Ç¨; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!",
What is the Nyquist rate?,"The Nyquist rate is the minimum sampling rate required to adequately reconstruct a signal without aliasing. Specifically, it is defined as twice the frequency of the highest frequency component present in the signal that needs to be reconstructed. If a signal is sampled at a rate below the Nyquist rate, it is considered under-sampled, which can lead to artefacts like aliasing.

You can find more details on this in the lecture materials on Digital Images and Point Processes.
<output_context_reference>['./documents/Lec-1B-Digital Images and Point Processes.pdf']</output_context_reference>",The Nyquist rate is the minimum sampling rate required to avoid aliasing. It is twice the frequency of the highest frequency component in the signal (or image) being sampled.,"rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between a point process and a spatial filter?,"The key difference between a point process and a spatial filter lies in how they transform pixel values in an image:

*   **Point Process:**
    *   In a point process, the new value of a target pixel depends *only* on the value of the corresponding source pixel in the original image, and potentially some predefined parameters.
    *   It operates on individual pixels independently.
    *   An example is an intensity transform where `g(x,y) = a.f(x,y) + b`, where `f(x,y)` is the original pixel value and `g(x,y)` is the new pixel value, with `a` being the gain (controlling contrast) and `b` being the bias (controlling brightness).
    *   Colour space transformations are also examples of point processes.
    *   (Source: `Lec-1B-Digital Images and Point Processes.pdf`, slides 27, 29, 30)

*   **Spatial Filter:**
    *   In contrast, a spatial filter determines the new value of a pixel based on the values of its *neighbouring* pixels, not just the pixel itself.
    *   It considers a local region (a ""mask"" or ""kernel"") around each pixel.
    *   Examples include mean filters, Gaussian filters, and median filters.
    *   Mean and Gaussian filters can be seen as diffusion processes, making each pixel more like its neighbours.
    *   (Source: `Lec-3B-Non-Linear Filters.pdf`, slide 10)

In essence, point processes are pixel-wise operations, while spatial filters are neighbourhood operations.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","A point process operates on each pixel independently, where the output pixel value depends only on the corresponding input pixel value. A spatial filter operates on a local neighborhood of pixels, where the output pixel value depends on the values of the input pixel and its neighbors.","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in‚Äô the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‚Äòaction process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in‚Äô the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‚Äòtapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in‚Äô the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‚Äòaction process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in‚Äô the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‚Äòtapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
How can you convert an RGB image to grayscale?,"You can convert an RGB image to grayscale using a couple of methods:

1.  **Simple Averaging:** You can calculate the grayscale value by taking a simple average of the Red, Green, and Blue components of each pixel.
    *   `I = (R + G + B) / 3`
    This method is straightforward but doesn't account for how the human eye perceives different colors.

2.  **Weighted Averaging:** A more perceptually accurate method involves using a weighted average, as our eyes are more sensitive to green light. A common weighting function is:
    *   `I = 0.30 * R + 0.59 * G + 0.11 * B`
    This formula assigns higher weights to the green and red channels, reflecting their importance in human vision.

You can implement these conversions either by explicitly writing a loop to process each pixel or more efficiently using array manipulation facilities provided by libraries like NumPy in Python. Additionally, libraries like OpenCV provide built-in functions, such as `cvtColor`, to perform this conversion directly.

For example, when working with images in Python, it's often useful to convert `uint8` (unsigned 8-bit integer) image data to `int` or `float` before performing calculations to avoid overflow issues, and then convert it back to `uint8` for display or further processing.

(Refer to: retrieved_context_reference_1, retrieved_context_reference_4)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lab 1B.pdf"" target=""_blank"">Lab 1B</a>

","An RGB image can be converted to grayscale by taking a weighted average of the red, green, and blue channels. A common formula is:
Grayscale = 0.30*R + 0.59*G + 0.11*B, which takes into account the human eye's different sensitivities to different colors.","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.‚óè Display its red, green and blue components as three separate grey-level images.‚óè Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)‚óè Perform the above again but using NumPy‚Äôs array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)‚óè Produce and display an image of the atrium showing the ‚Äògreenness‚Äô value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.‚óè Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel ‚Ä¢ Makes processing easier ‚Ä¢ Reduces the amount of information ‚Ä¢ Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*‚Ä¢ Most common starting point‚Ä¢ Retinal cells are sensitive tothree primary colours R, G, B‚Ä¢ Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.‚óè Display its red, green and blue components as three separate grey-level images.‚óè Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)‚óè Perform the above again but using NumPy‚Äôs array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)‚óè Produce and display an image of the atrium showing the ‚Äògreenness‚Äô value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.‚óè Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel ‚Ä¢ Makes processing easier ‚Ä¢ Reduces the amount of information ‚Ä¢ Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*‚Ä¢ Most common starting point‚Ä¢ Retinal cells are sensitive tothree primary colours R, G, B‚Ä¢ Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255",
What is the purpose of histogram backprojection in object tracking?,"Histogram Backprojection is primarily used for **locating and tracking objects efficiently, even in crowded scenes or when objects are moving relative to the camera/robot.**

It is particularly useful because of its computational efficiency, allowing for real-time applications like tracking moving objects. It is also robust to occlusion, meaning it can still effectively locate objects even when parts of them are hidden.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram backprojection produces a probability map of object locations based on a model histogram. This probability map is commonly used with tracking algorithms such as mean-shift to estimate and update the object's position across frames.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‚Äògained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‚Äògained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between first and second derivative methods for edge detection?,"First and second derivative methods are both used for edge detection in image processing, but they differ in how they identify edges and their characteristics.

Here's a breakdown of the differences:

**1. How they detect edges:**

*   **First Derivative Methods:**
    *   They approximate the derivative of a function by calculating the difference between neighboring pixel values.
    *   Edges are identified by **peaks** in the first derivative. A strong response (a high peak) indicates a significant change in intensity, which is characteristic of an edge.
    *   An example of a first derivative method is the Sobel operator. It calculates the gradient in both x and y directions and then combines them to find the magnitude of the gradient. Pixels with high magnitude are classified as edges.
    *   <details><summary>Click to show reference</summary>
        *   ['./documents/Lec-6A-Derivative and Edges.pdf', 'The 1st derivative of a function can be approximated by: ¬∂ ¬∂ f x = xf ( -+ )1 xf )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa5']
        *   ['./documents/Lec-6A-Derivative and Edges.pdf', 'Peaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise']
        *   ['./documents/Lab 6.pdf', 'Sobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.']</details>

*   **Second Derivative Methods:**
    *   They look for **zero crossings** in the second derivative. A zero crossing indicates a point where the rate of change of intensity is highest, which corresponds to the center of an edge.
    *   These methods are often associated with the Laplacian operator.
    *   <details><summary>Click to show reference</summary>
        *   ['./documents/Lec-6A-Derivative and Edges.pdf', 'Peaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise']
        *   ['./documents/Lab 6.pdf', 'The Laplacian is a second-order derivative mask, which is often used to find edges within an image.']</details>

**2. Characteristics and Trade-offs:**

| Feature               | First Derivative Methods                                   | Second Derivative Methods                                |
| :-------------------- | :--------------------------------------------------------- | :------------------------------------------------------- |
| **Edge Response**     | Strong response at edges.                                  | Well-defined, easy to detect zero crossings.             |
| **Noise Sensitivity** | Also respond to noise (can produce false positives).       | Can be very sensitive to noise, often require smoothing. |
| **Localization**      | Peak detection and threshold selection need careful tuning. | Zero crossings provide good localization of edges.       |
| **Edge Continuity**   | Tend to round off corners.                                 | Can form smooth, connected contours.                     |
| **Practical Use**     | Much more common in practical applications.                | Less common in practical applications compared to 1st derivative methods, partly due to the work of John Canny. |
| **Examples**          | Sobel, Prewitt, Roberts.                                   | Laplacian.                                               |
<details><summary>Click to show reference</summary>
*   ['./documents/Lec-6A-Derivative and Edges.pdf', 'Peaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42']</details>

**3. The Canny Operator:**

It's worth noting that the Canny edge detector, considered an optimal edge detector, uses a process that involves both first and second derivative concepts. It first reduces noise using Gaussian smoothing (which can be seen as a form of pre-processing for derivatives), then calculates gradients using Sobel filters (first derivative), and finally uses non-maximum suppression and hysteresis thresholding to refine the edges. The optimal detector Canny sought was closely approximated by the first derivative of a Gaussian.
<details><summary>Click to show reference</summary>
*   ['./documents/Lec-6A-Derivative and Edges.pdf', 'The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely\' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian']
*   ['./documents/Lab 6.pdf', 'Canny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max).']</details>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","First derivative methods (e.g., Sobel, Roberts) look for peaks in the image gradient to find edges. Second derivative methods (e.g., Laplacian of Gaussian) look for zero-crossings in the second derivative of the image to find edges.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in",
What are the main steps in the Canny edge detection algorithm?,"The Canny edge detection algorithm involves several key steps to accurately identify edges in an image. It can be seen as an extension of Sobel edge detection. Here are the main steps:

1.  **Noise Reduction (Gaussian Smoothing):** The first step is to reduce noise in the image. This is typically achieved by applying a Gaussian smoothing filter.
2.  **Gradient Calculation (Sobel Filters):** After noise reduction, the gradient of the image is calculated using Sobel filters in both the x and y directions. This helps to identify areas of significant intensity change.
3.  **Non-Maximum Suppression:** To obtain thin edges, non-maximum suppression is performed. This process saves only the local maximums of the gradient in the gradient direction, effectively thinning out broad edge responses.
4.  **Hysteresis Thresholding:** The final step involves selecting the true edges using hysteresis thresholding. This method uses two threshold values: a minimum (min) and a maximum (max) threshold.
    *   Edges with a gradient magnitude above the maximum threshold are considered strong edges.
    *   Edges with a gradient magnitude below the minimum threshold are considered non-edges.
    *   For edges with gradient magnitudes between the two thresholds, their classification depends on their connectivity to a strong edge or a non-edge. If they are connected to a strong edge, they are considered part of the edge; otherwise, they are discarded.

This multi-stage approach allows the Canny algorithm to produce clean, thin, and continuous edges.

(Source: retrieved context from lecture notes and Lab 6)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","The Canny edge detection algorithm involves: 1) Gaussian smoothing to reduce noise, 2) finding the intensity gradient, 3) non-maximum suppression to thin edges, and 4) hysteresis thresholding to link edges.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the ‚Äúobjects‚Äù in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¬¢ EX 0 if ||X@) ‚Äî XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the ‚Äúobjects‚Äù in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¬¢ EX 0 if ||X@) ‚Äî XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing",
What is the purpose of non-maximum suppression in the Canny edge detector?,"Non-maximum suppression in the Canny edge detector serves the purpose of obtaining **thin edges**. After calculating the gradient using Sobel filters, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction. This process helps to refine the edges, ensuring that they are represented by a single pixel width, rather than a blurred or thick line.

This step is crucial for achieving precise and accurate edge detection.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","Non-maximum suppression is used to thin the edges found by the gradient calculation. For each pixel, it checks if the gradient magnitude is the largest in the direction of the gradient. If not, the pixel is suppressed (set to zero).","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is hysteresis thresholding?,"Hysteresis thresholding is a method used in edge detection, particularly in the Canny operator, to improve the quality of detected edges. It addresses the problem where pixels along an edge might not all survive a simple thresholding process, leading to gaps in the edges.

Here's how it works:
*   **Two Thresholds:** Instead of a single threshold, hysteresis uses two thresholds: a high threshold and a low threshold.
*   **Strong and Weak Edges:**
    *   Pixels with gradient magnitudes above the high threshold are immediately classified as ""strong"" edge pixels.
    *   Pixels with gradient magnitudes below the low threshold are discarded as non-edge pixels.
    *   Pixels with gradient magnitudes between the high and low thresholds are considered ""weak"" edge pixels.
*   **Connecting Edges:** The key idea of hysteresis is to keep these ""weak"" edge pixels *if they connect to ""strong"" edge pixels*. This means that if a weak edge pixel is spatially connected to a strong edge pixel (or a chain of strong and connected weak pixels), it is also classified as an edge pixel.
*   **Assumptions:** This method allows for a band of variation in edge strength but assumes that edges are continuous.

**The effect of hysteresis thresholding is to fill in most of the gaps in detected edges.** It ensures that only genuinely strong edges are preserved, while also including weaker but connected parts of those edges, resulting in more complete and continuous edge lines. While the user still selects the parameters (the high and low thresholds), it makes the process easier and less precise compared to trying to find a single perfect threshold [1].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Hysteresis thresholding is a technique used in the Canny edge detector to connect weak edges to strong edges. It uses two thresholds, a high and a low. Pixels with a gradient magnitude above the high threshold are considered strong edges, and pixels between the two thresholds are considered weak edges. Weak edges are only kept if they are connected to strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of",
What is the difference between the Sobel and Roberts edge detectors?,"The Sobel and Roberts edge detectors are both methods used to identify edges in an image. While both are still in use, there are some key differences:

*   **Sensitivity to Noise:** The Roberts operator is known to be very sensitive to noise. In contrast, larger Sobel operators are more stable in the presence of noise.
*   **Computational Speed:** The Roberts Cross Operator is very quick to compute, involving only 4 pixels, subtractions, and additions.
*   **Current Usage:** Sobel is still commonly used, whereas Roberts is less common nowadays.

To summarize:
*   **Roberts:** Quick to compute, but very sensitive to noise and less common now.
*   **Sobel:** More stable in noise (especially with larger operators) and still widely used.

(Source: retrieved context from lecture notes)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","The Sobel and Roberts edge detectors are both first-derivative edge detectors, but they use different kernels to approximate the gradient. The Sobel operator uses 3x3 kernels, while the Roberts operator uses 2x2 kernels.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute ‚Äì 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¬¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V¬∞L f(x. ¬ª) * G(x. y)] = V¬∞G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <‚Äî‚Äî<‚Äî‚Äî37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute ‚Äì 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¬¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V¬∞L f(x. ¬ª) * G(x. y)] = V¬∞G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <‚Äî‚Äî<‚Äî‚Äî37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a color histogram?,"A color histogram is a representation of the color distribution in an image. It is created by:

1.  **Discretizing the image colors:** This means dividing the continuous range of colors into a finite number of discrete bins. For example, if you're using an RGB color space, you might define bins for different ranges of red, green, and blue values.
2.  **Counting occurrences:** For each discrete color bin, the histogram counts how many times that specific color occurs in the image array.

Essentially, each bin in the histogram represents a specific discrete color, and the value in that bin indicates how many pixels in the image have that color. The image colors that are transformed to a common discrete color are considered to be in the same 3D histogram bin centered at that color.

Color histograms are useful because they are invariant to translation and rotation about the viewing axis, and they change only slowly under changes in viewing angle, scale, and occlusion.

You can find more information about color histograms in the provided context, specifically in Section 2, ""Color Histograms"".


**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",A color histogram is a representation of the distribution of colors in an image. It counts the number of pixels for each color in a discrete color space.,"is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,","is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,",
How can you use a histogram to improve image contrast?,"You can use a histogram to improve image contrast through techniques like histogram equalisation and contrast stretching.

Here's how these methods work:

*   **Histogram Equalisation**: This technique aims to improve image contrast by redistributing the pixel intensity values to achieve a more uniform distribution. It transforms the image such that its output histogram is nearly flat. Histogram equalisation is a general method that doesn't require specific parameters.
*   **Contrast Stretching**: This method enhances contrast by linearly expanding the range of pixel intensity values to a specified range, typically 0 to 255.

Both techniques manipulate the distribution of pixel intensities in the image's histogram to make details more visible and improve the overall visual quality of the image.

For more details, you can refer to the lab materials [2].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

",Histogram equalization can be used to improve image contrast by redistributing the pixel intensities to create a more uniform histogram.,"the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image ‚Ä¢ To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values ‚Ä¢ More general than linear or piecewise contrast stretching - No parameters to specify, ‚Äúapply a transform that makes the output histogram FLAT‚Äù Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability","the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image ‚Ä¢ To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values ‚Ä¢ More general than linear or piecewise contrast stretching - No parameters to specify, ‚Äúapply a transform that makes the output histogram FLAT‚Äù Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability",
What is a binary image?,"A binary image is an image where pixels can only have two values, typically 0 (black) or 1 (white) (or 0 and 255). These images are often the result of a process called thresholding, which classifies pixels into either a foreground or a background based on their intensity values.

Binary images are commonly used in image processing for tasks such as edge detection and segmentation. They simplify the image data, making it easier for certain operations to identify objects or regions of interest.

For example, if you have a grey-level image of a dark object on a light background, thresholding can be applied to convert it into a binary image where the object pixels are one value (e.g., 0 for black) and the background pixels are another (e.g., 255 for white).

Here are some key characteristics and uses of binary images:
*   **Two Values**: Pixels have only two possible intensity values (e.g., 0 and 1, or 0 and 255).
*   **Result of Thresholding**: They are typically created by applying a threshold to a grey-level or color image.
*   **Applications**: Frequently used for edge detection, segmentation, and other image analysis tasks.
*   **Noise Removal**: Like other image types, binary images may also require noise removal and enhancement.

This concept is fundamental in image processing as many operations make a binary decision about pixels‚Äîwhether they belong to an object of interest or the background.

**References:**
*   `./documents/Lec-4-Thresholding and Binary Images.pdf`
*   `./documents/Lab 4.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image is an image where each pixel can have only one of two values, typically 0 or 1 (or black and white).","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform",
How can you create a binary image from a grayscale image?,"You can create a binary image from a grayscale image through a process called **thresholding**.

Thresholding works by classifying pixels in an image as either foreground or background based on their intensity values. A specific ""threshold value"" is chosen. Pixels with intensity values above this threshold are typically set to one value (e.g., 255 for white), and pixels with intensity values below or equal to the threshold are set to another value (e.g., 0 for black). This results in an image containing only two pixel values, hence a binary image [context_link to_page=1 page_content=""Thresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white).""].

For example, if you have a grayscale image where pixel intensities range from 0 to 255, and you choose a threshold of 128:
*   Any pixel with an intensity value greater than 128 would become white (255).
*   Any pixel with an intensity value less than or equal to 128 would become black (0).

This process is often used in image processing for tasks like edge detection and segmentation [context_link to_page=1 page_content=""Binary images are often used in image processing for edge detection and segmentation.""].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image can be created from a grayscale image using thresholding. A threshold value is chosen, and all pixels with an intensity value above the threshold are set to one value (e.g., 1), while all pixels below the threshold are set to the other value (e.g., 0).","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
What are connected components in a binary image?,"In a binary image, connected components refer to sets of connected pixels that share the same pixel value, typically either black or white, and are considered as distinct objects. If black pixels represent objects and white pixels represent the background, then each connected set of black pixels would be assigned a different label.

The process of identifying connected components converts a binary image into a label image, where each unique connected component is assigned a unique label. This allows for the calculation of properties for each individual component.

Identifying connected components can be computationally intensive and often acts as a bottleneck in image processing pipelines. A common approach involves a two-pass sequential algorithm using 4-neighbours:
1.  **First Pass**: The image is scanned from top-left to bottom-right. For each foreground pixel, its top and left neighbours (which have already been processed and potentially assigned labels) are examined.
    *   If both neighbours are background pixels, a new label is assigned to the current pixel.
    *   If one of the neighbours has a label, that label is assigned to the current pixel.
    *   If both neighbours have labels, and they are different, a record is kept that these two labels are equivalent.
2.  **Second Pass**: After the first pass, a list of equivalent labels is used to resolve any initial inconsistencies, ensuring that all pixels belonging to the same connected component share the same final label.

This process essentially groups together all foreground pixels that are connected to each other into distinct ""objects.""

**References:**
*   Lec-5-Morphology.pdf
*   Practicals with Solutions - Morphology.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

","Connected components are sets of connected pixels in a binary image that share the same value (usually the ""on"" value). They represent distinct objects or regions in the image.","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 ‚Äì 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) ‚Äì 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‚Äò ‚Äîs Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 ‚Äì 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) ‚Äì 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‚Äò ‚Äîs Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -",
How can you use connected components analysis?,"Connected components analysis is a technique used in image processing to identify and label connected regions of pixels in a binary image. Here's how it's used and what it achieves:

*   **Converting Binary to Label Image:** It takes a binary image (where pixels are typically either black or white, or 0 and 1) and converts it into a ""label image."" In this label image, each distinct connected set of foreground pixels (e.g., black pixels if they represent objects) is assigned a unique label or different value. This allows you to differentiate between individual objects or regions within the image.
    <details>
        <summary>Click to see citation</summary>
        <ul>
            <li>['./documents/Practicals with Solutions - Morphology.pdf']</li>
            <li>['./documents/Practicals - Morphology.pdf']</li>
            <li>['./documents/Lec-5-Morphology.pdf']</li>
        </ul>
    </details>

*   **Calculating Properties of Components:** Once components are labeled, you can then calculate various properties for each individual component. For example, you could determine the area, perimeter, centroid, or orientation of each connected object.
    <details>
        <summary>Click to see citation</summary>
        <ul>
            <li>['./documents/Practicals with Solutions - Morphology.pdf']</li>
            <li>['./documents/Practicals - Morphology.pdf']</li>
        </ul>
    </details>

*   **Segmentation:** Connected components analysis is often a crucial step in segmentation tasks. For instance, in a task like segmenting a flower from an image, after some initial processing (like thresholding to create a binary image), connected components can help isolate the flower as a distinct object.
    <details>
        <summary>Click to see citation</summary>
        <ul>
            <li>['./documents/Assessment Sheet COMP2032 Coursework 2025.pdf']</li>
        </ul>
    </details>

*   **Bottleneck Consideration:** While powerful, connected component algorithms can sometimes be slow, especially for large images, and can be a bottleneck in an image processing pipeline.
    <details>
        <summary>Click to see citation</summary>
        <ul>
            <li>['./documents/Lec-5-Morphology.pdf']</li>
        </ul>
    </details>

*   **Algorithm Basics (Two-Pass Approach):** A common approach involves two passes over the image:
    1.  **First Pass:** The image is scanned (e.g., top-left to bottom-right). For each foreground pixel, its top and left neighbors are examined.
        *   If both neighbors are background, a new label is assigned to the current pixel.
        *   If one neighbor has a label, that label is assigned to the current pixel.
        *   If both neighbors have labels, and they are different, an equivalence between these labels is recorded, and one of the labels is assigned to the current pixel.
    2.  **Second Pass:** After the first pass, a second pass is used to resolve any label equivalences, ensuring that all pixels belonging to the same connected component have the exact same unique label.
    <details>
        <summary>Click to see citation</summary>
        <ul>
            <li>['./documents/Lec-5-Morphology.pdf']</li>
        </ul>
    </details>

In essence, connected components analysis helps to identify and characterize individual ""objects"" or regions of interest within an image based on pixel connectivity.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

","Connected components analysis is used to identify and label the distinct objects in a binary image. Once labeled, properties of each component, such as its area, perimeter, and shape, can be calculated.","basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.‚óè Use rotate to rotate the image by 90 degrees. Display the result.‚óè Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.‚óè imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated","basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.‚óè Use rotate to rotate the image by 90 degrees. Display the result.‚óè Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.‚óè imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated",
What is the difference between 4-connectivity and 8-connectivity?,"In image processing, connectivity defines how pixels are related to each other based on their spatial proximity and intensity values. The difference between 4-connectivity and 8-connectivity lies in the number of neighboring pixels considered:

*   **4-connectivity**: This method considers only the pixels that share a common edge with the central pixel. These are the pixels directly above, below, to the left, and to the right of the central pixel. It involves checking 4 neighbors.

*   **8-connectivity**: This method is more inclusive, considering all pixels that share either an edge or a corner with the central pixel. This means it includes the 4 neighbors from 4-connectivity plus the 4 diagonal neighbors. It involves checking 8 neighbors.

These concepts are fundamental in image segmentation and other image processing operations, as they influence how regions are defined and how algorithms traverse an image.","In a 2D grid, 4-connectivity considers only the top, bottom, left, and right neighbors of a pixel. 8-connectivity considers these four neighbors as well as the four diagonal neighbors.","< k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than ‚Äúie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an‚Äî-2am‚Äî¬´¬£ +1)-‚Äîk+3an‚Äî- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) ‚Äî assoc(A, A) assoc(A, V) 4 assoc(B, V) ‚Äî assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2‚Äî Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","< k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than ‚Äúie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an‚Äî-2am‚Äî¬´¬£ +1)-‚Äîk+3an‚Äî- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) ‚Äî assoc(A, A) assoc(A, V) 4 assoc(B, V) ‚Äî assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2‚Äî Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
What is a Region of Interest (ROI)?,"A Region of Interest (ROI) is a selected subset of an image that is identified for specific processing or analysis. Instead of processing an entire image, which can be computationally intensive, ROIs allow for focused attention on particular areas that are relevant to a given task.

The concept of ROIs is very powerful because it enables code to be written such that the output of one process can determine where another process is applied. This means you can dynamically define and adapt the areas of an image that are being analyzed or manipulated.

For example, ROIs can be defined by color, allowing for the isolation of specific colored areas within an image for further examination.

[output_context_reference w_source_id=""2"" w_page_number=""31""]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Thresholding and Binary Images.pdf"" target=""_blank"">Practicals with Solutions   Thresholding and Binary Images</a>

",A Region of Interest (ROI) is a selected area of an image that is targeted for a specific purpose or analysis.,"pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‚Äòpendent high-levet requires that, first, the MIVURU representat : ‚Äòaction process be contnes aramelers corresponding ¬£ domain, is Of Images is a paradigm alec sroals. A feahire space ig ‚ÄúdA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon ¬ß domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‚Äò the feature of interest im into a point in thes parameter. After the entiaction process be et rame‚Äòers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each‚Ä¶8 & &‚Ç¨ 8 8 8 8 8B 7250 150o ¬ß@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds,","pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‚Äòpendent high-levet requires that, first, the MIVURU representat : ‚Äòaction process be contnes aramelers corresponding ¬£ domain, is Of Images is a paradigm alec sroals. A feahire space ig ‚ÄúdA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon ¬ß domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‚Äò the feature of interest im into a point in thes parameter. After the entiaction process be et rame‚Äòers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each‚Ä¶8 & &‚Ç¨ 8 8 8 8 8B 7250 150o ¬ß@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds,",
How is a graph used in the Livewire segmentation algorithm?,"In the Livewire segmentation algorithm, the input image is converted into a graph as one of the significant steps. This graph representation is crucial because the algorithm then employs a shortest path algorithm (like Dijkstra's algorithm, which often uses a priority queue implemented with a heap, as mentioned by the need for the `Heapq` library) to find the optimal path, which corresponds to the object boundary. The Livewire algorithm essentially avoids traversing object boundaries by finding the ""shortest"" path through the graph, where path costs are defined in a way that encourages paths along object edges.

This concept is indeed based on graph theory.

Here's a breakdown of how the graph is used in the Livewire algorithm:
1.  **Converting Image to Graph**: The image is transformed into a graph structure. Each pixel or a small region of pixels in the image can be considered a node in the graph.
2.  **Edges and Costs**: Edges connect neighboring nodes, and these edges are assigned ""costs."" These costs are typically derived from image properties like intensity gradients. For instance, edges that lie along strong object boundaries (where there's a significant change in pixel intensity) would have lower costs, while edges that cross uniform regions would have higher costs. This is why the image gradient is included in the cost function.
3.  **Shortest Path Algorithm**: Once the image is represented as a graph with associated edge costs, a shortest path algorithm is applied. As the user interactively moves a ""free point"" (cursor), the algorithm dynamically calculates the shortest path from a starting point (often a user-defined seed point) to the current cursor position. This shortest path ""snaps"" to the object boundary because paths along boundaries have lower accumulated costs.
4.  **Interactive Visualization**: The calculated shortest path is then displayed, allowing the user to see the segmentation adapt and snap to the object boundary in real-time as they move the cursor.

Essentially, the graph provides a mathematical framework to represent the image and allows the application of graph algorithms to find optimal paths that delineate object contours.

If you'd like to delve deeper into the specifics of how the image gradient influences the cost function, we can explore that further!

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

","In the Livewire algorithm, the image is converted into a graph where each pixel is a node. The edge weights between adjacent pixels are determined by the image's edge information, such as intensity differences.","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the role of Dijkstra's algorithm in Livewire?,"Dijkstra's algorithm plays a crucial role in the Livewire segmentation algorithm, which is also known as ""Intelligent Scissors"" [cite:./documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf].

Here's how it fits in:

1.  **Graph Representation:** The Livewire algorithm first converts the image into a graph. In this graph, each pixel in the image represents a node [cite:./documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf].
2.  **Edge Weights:** The connections (edges) between adjacent pixels are assigned weights. These weights are typically determined by the image's edge information, such as the intensity difference or gradient between neighboring pixels [cite:./documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf]. The idea is that paths along strong object boundaries should have lower ""costs"" or weights.
3.  **Shortest Path Algorithm:** Dijkstra's algorithm is then used as the ""Shortest Path Algorithm"" step within Livewire [cite:./documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf]. It efficiently finds the path of minimum cost (or shortest path) between a user-defined ""seed point"" and a ""free point"" (which is typically the cursor position) [cite:./documents/mort-sigg95.pdf].
4.  **Interactive Adaptation:** As the user moves the ""free point"" (cursor), Dijkstra's algorithm continuously recalculates the optimal path from the fixed ""seed point"" to the current ""free point."" This allows the livewire segment to dynamically adapt and ""snap"" to object boundaries, guiding the user in tracing the desired object outline [cite:./documents/mort-sigg95.pdf].

In essence, Dijkstra's algorithm is fundamental to Livewire's ability to find and display the ""intelligent"" boundary segment that follows the strongest edges in the image between the user's input points.

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

",Dijkstra's algorithm is used in the Livewire algorithm to find the shortest path between a user-selected start point and end point on the image graph. This shortest path corresponds to the object boundary.,"f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.‚Ä¢ Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if‚Äô University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:‚Ä¢ The image is converted into a graph where each pixel represents a node.‚Ä¢ The edge weights between adjacent pixels are determined based on the image's edge information.‚Ä¢ Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:‚Ä¢ The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively ‚Äúsnap‚Äù and ‚Äúwrap‚Äù the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. SpeciÔ¨Åcally, note the live-wire segments corresponding to user-speciÔ¨Åed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-Ô¨Çy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.‚Ä¢ Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if‚Äô University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:‚Ä¢ The image is converted into a graph where each pixel represents a node.‚Ä¢ The edge weights between adjacent pixels are determined based on the image's edge information.‚Ä¢ Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:‚Ä¢ The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively ‚Äúsnap‚Äù and ‚Äúwrap‚Äù the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. SpeciÔ¨Åcally, note the live-wire segments corresponding to user-speciÔ¨Åed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-Ô¨Çy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input",
What are the main steps in the GrabCut algorithm?,"The GrabCut algorithm is an interactive foreground extraction technique that uses iterated graph cuts. Here are the main steps involved:

1.  **User Input (Initialisation)**: The process begins with the user providing an initial, loose rectangle around the object of interest. This rectangle broadly indicates the foreground area.
    <details>
    <summary>Click to show citation</summary>
    ['./documents/grab_cut.pdf']
    </details>

2.  **Initial Segmentation**: Based on the user-defined rectangle, GrabCut performs an initial segmentation to separate the foreground and background. Pixels inside the rectangle are initially considered potential foreground, while those outside are considered definite background.

3.  **Model Initialisation**: The algorithm initialises statistical models (Gaussian Mixture Models - GMMs) for both the foreground and background. These models capture the color and texture properties of the respective regions.

4.  **Iterated Graph Cuts**:
    *   **Graph Construction**: A graph is constructed where each pixel is a node. Edges connect adjacent pixels and also connect pixels to two special nodes: a source (representing foreground) and a sink (representing background).
    *   **Edge Weights**: The weights of the edges are determined by both color information (from the GMMs) and boundary information (contrast between adjacent pixels). This allows the algorithm to combine both texture and edge cues.
    *   **Min-Cut/Max-Flow**: The min-cut algorithm (equivalent to max-flow) is applied to this graph to find the optimal cut that separates the foreground from the background. This cut aims to minimise an energy function that penalises dissimilar colors within a segment and large color differences across segment boundaries.
    *   **Model Update**: After each graph cut, the GMMs for the foreground and background are updated based on the new segmentation. This iterative refinement allows the models to better represent the true foreground and background distributions.

5.  **User Refinement (Optional)**: If the initial result is not satisfactory, the user can provide further interactions using brushes (e.g., red for background, white for foreground, yellow for matting). These interactions guide the algorithm to refine the segmentation.
    <details>
    <summary>Click to show citation</summary>
    ['./documents/Lec-8-Interactive Segmentation.pdf']
    </details>

6.  **Alpha Matting (Border Refinement)**: For precise extraction, especially around object boundaries, GrabCut can also estimate an alpha matte. This allows for smooth transitions and avoids ""background bleeding"" by estimating the transparency of pixels at the object's edge.
    <details>
    <summary>Click to show citation</summary>
    ['./documents/grab_cut.pdf']
    </details>

This iterative process, combining user interaction with powerful graph-cut optimisation and model refinement, allows GrabCut to achieve high-quality foreground extraction.

**Cited Materials:**

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

<a href=""http://localhost:5003/documents/Laboratory Sheet 8 - Interactive Segmentation.pdf"" target=""_blank"">Laboratory Sheet 8   Interactive Segmentation</a>

","The GrabCut algorithm involves an iterative process of: 1) assigning GMM components to pixels, 2) learning GMM parameters from the data, 3) estimating the segmentation using a min-cut algorithm, and repeating until convergence.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let‚Äôs try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‚Äòinput‚Äô to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if‚Äô University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let‚Äôs try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‚Äòinput‚Äô to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if‚Äô University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:",
What is the purpose of the Transformer in the Vision Transformer (ViT) model?,"The Vision Transformer (ViT) model uses a Transformer Encoder. The purpose of the Transformer in the ViT model is to process the input, which consists of ""Patch + Position Embedding"" and an ""Extra learnable [class] embedding."" These are then fed into the Transformer Encoder as ""Linear Projection of Flattened Patches"" and ""Embedded Patches.""

While the provided context doesn't go into the specific mechanisms of *how* the Transformer achieves this, in general, Transformers are known for their ability to capture long-range dependencies in data and process sequences effectively. In the context of ViT, they apply this capability to image data by treating patches of an image as a sequence.

The source for this information is from Dosovitskiy, A., et al. (2020), ""An image is worth 16x16 words: Transformers for image recognition at scale.""


**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

",The Transformer in a Vision Transformer (ViT) model is used to capture both local and global relationships within an image by treating the image as a sequence of patches.,"words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNNü§î22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNNü§î22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the main difference between CNNs and ViTs?,"I cannot answer this question based on the provided context. The context includes a question ""Is CNN better than ViT?"" with a True/False answer, and infographics related to Vision Transformers (ViT) and Convolutional Neural Networks (CNNs), but it does not explicitly state the main differences between them.

For more information on this topic, I recommend contacting Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my), or attending their office hours.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - CNN.pdf"" target=""_blank"">Practicals   CNN</a>

","CNNs use convolutional layers to extract features from an image, which are inherently local operations. ViTs, on the other hand, divide the image into patches and use a Transformer architecture to learn relationships between these patches, allowing them to capture more global context.","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by",
What is the purpose of zero-padding in convolutional layers?,"The purpose of zero-padding in convolutional layers is to add zeroes to every side of the input boundaries. This ensures that filters fit the input image correctly during the convolution operation.

\
The information is from the ""Convolutional Layer"" section of the provided context.
\


**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

",Zero-padding is the process of adding zeros around the border of an input image before applying a convolutional filter. This is done to control the spatial size of the output feature map and to ensure that the filter can be applied to pixels at the edge of the image.,"Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep","Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep",
What is stride in a convolutional layer?,"In a convolutional layer, **stride** refers to the number of pixels the filtering window moves after each operation [1].

Essentially, it dictates how much the filter ""steps"" across the input image. For example, a stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it skips a pixel, moving two pixels at a time.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","Stride is the number of pixels by which the filter moves across the image at each step. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time, resulting in a smaller output feature map.
QA about Material Content","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights",
What are the deliverables for the coursework?,"The coursework for the ""Introduction to Image Processing (IIP) / COMP 2032"" module requires group submissions. The deliverables are:

1.  **Python codes:** These should be submitted as a ZIP folder containing all the `.py` files.
2.  **A 6-page conference paper:** This paper should have a maximum of 2000 words and be submitted in PDF format. Please note that submitting additional pages will result in a deduction of marks.

This coursework involves developing a software solution using Python for an image processing problem and collaboratively producing a conference paper that describes and critically evaluates your solution.

(Source: retrieved_context_reference
```
[
  {
    ""source"": ""./documents/Assessment Sheet COMP2032 Coursework 2025.pdf"",
    ""start_page"": 1,
    ""end_page"": 1
  }
]
```)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The coursework requires a group submission of Python code files (as a ZIP folder), a 6-page conference paper (2000 words max, PDF format), and result images (as a ZIP folder). There is also an individual submission of a peer assessment form.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the deadline for the coursework submission?,"The deadline for the coursework submission is **Friday, 25 April 2025, by 11:59 pm**.

[output_context_reference: ['./documents/Assessment Sheet COMP2032 Coursework 2025.pdf']]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The submission date for the coursework is Friday, 25 April 2025, by 11:59 pm.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the penalty for late submission?,"I cannot find any information about the penalty for late submission in the provided context. The available documents discuss aspects like coursework breakdown, in-lab test details, and technical concepts like Histogram Backprojection and Adaptive Thresholding.

For information regarding late submission penalties, you should contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my). You could also try to attend Dr. Chandesa's office hours.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Enquiry about late submission penalty&body="">Send email</a>",Work submitted after the deadline is subject to a penalty of 5 marks for each late working day.,"reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3","reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3",
What is the topic of the coursework?,"The coursework for COMP2032 involves practical experience in writing programs to manipulate digital images. It is assessed through a combination of a programming assignment and a 2000-word conference paper.

The assessment requires:
*   Developing a Python application.
*   Writing a literature review, explanation, and evaluation of results for the conference paper.
*   An in-lab test with individual questions.

This coursework aims to lay the foundation for studying advanced topics in related fields such as Computer Vision and Computer Graphics.

\cite{./documents/Lec-1A-Introduction.pdf}
\cite{./documents/Lec-11-Finale and Revision.pdf}

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

",The coursework involves developing a software solution in Python for the semantic segmentation of flowers. This includes creating an image processing pipeline to separate flowers from the background and then classifying them.,"practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main parts of the coursework?,"The main parts of the coursework for COMP2032 are:

1.  **Programming + 2000-word conference paper:** This involves developing a Python application, writing a literature review, and explaining and evaluating your results in a conference paper format.
2.  **In-lab test:** This is an individual assessment where you will answer all questions.
3.  **Individual submission for peer assessment:** You need to submit a peer assessment form via Moodle. Failure to do so will result in a 10% penalty on your overall group mark.

The coursework is 100% continuous assessment (CW).

Essentially, it combines a practical programming component with a written academic paper and an individual test, plus a peer assessment aspect.

<details><summary>Reference</summary><ul><li>./documents/Assessment Sheet COMP2032 Coursework 2025.pdf</li><li>./documents/Lec-1A-Introduction.pdf</li></ul></details>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The coursework has two main parts: a SEGMENTATION PART where you design and implement an image processing pipeline to segment flowers from images in Dataset 1, and a SEMANTIC PART where you classify the entire flower dataset (Dataset 2) using a provided Transformer-based model.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
What are the requirements for the output images in the segmentation part?,"The output images for the segmentation part of the coursework have specific requirements to ensure proper evaluation. Here's a breakdown:

*   **File Format:** Accepted formats are `.jpg` and `.png`. Ensure that both your segmented images and the ground truth masks use the same format.
*   **Image Size & Resolution:** Images must be 256x256 pixels. If your segmentation output is a different size, it will be automatically resized, which could lead to distortions.
*   **Content:** Your output image must have a black background with only the segmented flower visible.
*   **Folder Structure:** For submission, your segmented outputs should be placed in a `/segmented/` subfolder within your main submission folder. For example: `/segmentation_submission/segmented/image_001.png`.
*   **Naming Convention:** Each segmented image should have a corresponding `.jpg` file saved in the output folder, categorized within three sub-folders (e.g., easy, medium, and hard) with the same name as the input image.

These requirements are crucial for the evaluation model to properly assess your segmentation results using metrics like Cosine Similarity and Accuracy.

(Sources: output_context_reference: ['./documents/Assessment Sheet COMP2032 Coursework 2025.pdf', './documents/Guidelines for Using the Segmentation Evaluation Model.pdf'])

**Cited Materials:**

<a href=""http://localhost:5003/documents/Guidelines for Using the Segmentation Evaluation Model.pdf"" target=""_blank"">Guidelines for Using the Segmentation Evaluation Model</a>

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The output images must have a black background with only the segmented flower. They should be saved as .jpg files in an ""output"" folder, categorized into sub-folders named ""easy,"" ""medium,"" and ""hard,"" corresponding to the input images.","f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided","f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided",
What evaluation metrics should be used for the segmentation part?,"For evaluating image segmentation, two key metrics are used:

1.  **Cosine Similarity**: This metric assesses the high-level similarity between the segmented images and the ground truth masks. The score ranges from 0 to 1, where a higher score indicates better similarity.
2.  **Accuracy**: This measures the pixel-wise accuracy between the segmented masks and the ground truth.

These metrics help in automatically evaluating and comparing different segmentation approaches.

**References:**
*   `output_context_reference`: ./documents/Guidelines for Using the Segmentation Evaluation Model.pdf
*   `output_context_reference`: ./documents/Metric for Image Segmentation.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Guidelines for Using the Segmentation Evaluation Model.pdf"" target=""_blank"">Guidelines for Using the Segmentation Evaluation Model</a>

<a href=""http://localhost:5003/documents/Metric for Image Segmentation.pdf"" target=""_blank"">Metric for Image Segmentation</a>

",The evaluation of the segmentation pipeline can be both qualitative and quantitative. The coursework suggests using mean IoU (Intersection over Union) as a metric.,"f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| ‚Äî MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results‚Ä¶.BeWhat we can do with metrics?‚Ä¢ Automatic evaluation‚Ä¢ Comparison between differenti* / ,*approaches‚Ä¢ Evaluation from multiple perspectives‚Ä¢ ‚Ä¶Sim√©oni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry √© √© 70 ‚Äòslgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the","f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| ‚Äî MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results‚Ä¶.BeWhat we can do with metrics?‚Ä¢ Automatic evaluation‚Ä¢ Comparison between differenti* / ,*approaches‚Ä¢ Evaluation from multiple perspectives‚Ä¢ ‚Ä¶Sim√©oni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry √© √© 70 ‚Äòslgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the",
What is the purpose of Dataset 1?,"I'm sorry, but I cannot answer your question as the provided context does not contain any information about ""Dataset 1"". If you are referring to the `torch.utils.data.Dataset` class, please clarify your question.

If you have questions related to image processing, feel free to ask, and I'll do my best to help!","Dataset 1 contains images of three species of flowers, categorized into ""easy,"" ""medium,"" and ""hard"" sub-folders, along with their corresponding ground truths. This dataset is to be used for developing and evaluating the image processing pipeline for the segmentation part of the coursework.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What is the purpose of Dataset 2?,"Dataset 2 is to be used for the semantic segmentation part of the coursework. It consists of 847 images with their corresponding ground truths. This dataset is specifically designated for tasks related to semantic segmentation, as opposed to Dataset 1 which is used for the segmentation part focusing on isolating a flower on a black background [cite: ./documents/Assessment Sheet COMP2032 Coursework 2025.pdf].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","Dataset 2, consisting of 847 images with ground truths, is to be used for the semantic part of the coursework, which involves classifying the entire flower dataset.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What are the requirements for the conference paper?,"The conference paper for this module has specific requirements:

*   **Length:** Maximum 2000 words.
*   **Format:** PDF.
*   **Page Limit:** 6 pages maximum. Be aware that additional pages will result in a deduction of 5% for every extra page for the entire group.
*   **Template:** You must use the provided template, `COMP2032-CW-GroupXXX.docx` (replace XXX with your group number). Do not change the formatting and headers; marks will be deducted if the submitted document does not follow the original formatting style. The provided table and text colored red and blue in the template are placeholders to be replaced by your own written content.

When writing your paper, ensure you cover the following points:

1.  **Methodology:** Detail the chosen method(s) and specific image processing technique(s) employed.
2.  **Justification:** Explain why you chose those technique(s) and method(s). You may include pseudocodes if applicable.
3.  **Results:** Present the results obtained from your practical implementation.
4.  **Critical Evaluation (Method):** Critically evaluate your method(s) based on the results, highlighting strengths and weaknesses. This section should explicitly reference features of your obtained results and use relevant metrics for segmentation to support your arguments/justifications.
5.  **Critical Evaluation (Classification):** Critically evaluate your classification results, highlighting the strengths and weaknesses of your entire pipeline by performing a pixel-wise classification. Use cosine similarity and accuracy metrics, generated from your semantic part, to further support your arguments.

(Source: retrieved_context_reference_1)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The conference paper should be a maximum of 6 pages (2000 words), detailing the chosen methods, justifying their selection, presenting the results, and critically evaluating the implemented pipeline and classification results. It must be submitted in PDF format using the provided template.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
Are we allowed to modify the provided Transformer and Model_input Python files?,"I cannot answer that question as the provided context does not contain any information about ""Transformer"" or ""Model_input"" Python files, nor does it specify rules about modifying provided files.

For questions regarding specific files or modification rules for the course, please contact the module convenor, Dr. Tissa Chandesa, at Tissa.Chandesa@nottingham.edu.my, or the teaching assistant, Mr. Irfan Yaqub, at hcxiy1@nottingham.edu.my. You could also attend their office hours if appropriate.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Laboratory Sheet 9 - CNN.pdf"" target=""_blank"">Laboratory Sheet 9   CNN</a>

","No, you are not allowed to modify any of the provided parameters in the transformer and Model_input files.","test images which are not in the training dataset. First, let‚Äôs create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","test images which are not in the training dataset. First, let‚Äôs create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the peer assessment form for?,"The peer assessment form is an additional submission that each student needs to make individually. If you fail to submit your peer assessment, you will be penalised 10% of your overall group mark. You can find the submission link for the ""Peer Assessment"" on Moodle.

This information is based on the ""Assessment Sheet for 2024-2025"" for the ""Introduction to Image Processing (IIP) / COMP 2032"" module.


**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",Each group member must individually submit a peer assessment form. Failure to do so will result in a 10% penalty on the overall group mark.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
What are the assessment criteria for the coursework?,"The assessment criteria for the coursework (which is 100% of the continuous assessment for COMP2032) are broken down as follows:

*   **Programming (Python application):** This contributes to the overall mark.
*   **2000-word conference paper:** This paper has several components:
    *   **Justification of chosen algorithms based on performed literature review:** 15%
    *   **Critical analysis and discussion of your findings:** 25%
    *   **Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result:** 30%
*   **In-lab test:** This is an individual test worth 50% and focuses on theories covered in lectures and practical exercises.

It's very important to note that **being unable to run codes will result in 0% being awarded** for the programming component.

<details><summary>Context References</summary>
*   ['./documents/Assessment Sheet COMP2032 Coursework 2025.pdf']</details>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The Python code is worth 30%, and the conference paper is worth 70%. The paper is broken down into justification of algorithms (15%), critical analysis of findings (25%), and discussion of strengths and weaknesses (30%).","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the format for the in-lab test?,"I'm sorry, but I cannot provide details about the format of the in-lab test. The provided context mentions an ""in lab test"" as part of the assessment but does not offer any information regarding its format or content.

For specific details about the in-lab test, I recommend reaching out to Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or attending his office hours.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",The in-lab test is an individual assessment worth 50% of the final grade. It focuses on theories covered in the lectures and practical exercises.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
When is the in-lab test?,"The in-lab test for COMP2032 is scheduled for **April 29, 2025**, from **2:30 pm to 3:30 pm** in **TCR 1**.
<output_context_reference>
./documents/Assessment Sheet COMP2032 Coursework 2025.pdf
</output_context_reference>

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The in-lab test is scheduled for 29 April 2025, from 2:30 pm to 3:30 pm in TCR 1.

QA about Out-of-the-Domain Questions","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150",
What is the capital of France?,"I can only assist with questions related to image processing for the ""Introduction to Image Processing"" module. I cannot answer general knowledge questions like ""What is the capital of France?"".","I'm sorry, I can only answer questions related to image processing.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t ‚Äú enough representatt ‚Äòaction process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane¬ª Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t ‚Äú enough representatt ‚Äòaction process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane¬ª Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you explain the theory of relativity?,"I can only assist with questions related to image processing, as that is the scope of this course. The theory of relativity falls outside of this domain.

If you have any questions about image processing, please feel free to ask!","I'm sorry, I can only answer questions related to image processing.","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What are the main causes of climate change?,"I can only assist with questions related to image processing and topics covered in the ""Introduction to Image Processing"" module. Your question about the main causes of climate change falls outside of this domain.

If you have any questions about image segmentation, histogram backprojection, convolutional neural networks, or other image processing concepts, please feel free to ask!","I'm sorry, I can only answer questions related to image processing.","connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
"Who wrote the play ""Hamlet""?","I can only assist with questions related to image processing and the course ""Introduction to Image Processing."" The question about who wrote ""Hamlet"" is outside my domain.

If you have any questions about image processing, please feel free to ask!","I'm sorry, I can only answer questions related to image processing.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )√º if q‚Äôs cost changed, make q point back to p√ò Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (l√©*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ‚Äù 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts ‚Äî__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )√º if q‚Äôs cost changed, make q point back to p√ò Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (l√©*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ‚Äù 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts ‚Äî__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!",
What is the chemical formula for water?,"I can only answer questions related to image processing, specifically topics covered in the ""Introduction to Image Processing"" module. I cannot provide information on other subjects like chemistry.

If you have any questions about image processing, please feel free to ask!","I'm sorry, I can only answer questions related to image processing.","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are",
Can you recommend a good restaurant in Nottingham?,"I can only assist with questions related to image processing for the ""Introduction to Image Processing"" module. I don't have information about restaurants in Nottingham.","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of‚Ä¶A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and ‚Äî1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 ‚Äî 2k + 2k?)(a(x) + 7) + 2(1 ‚Äî 2k) B(x)2a(x)_+ki ‚Äîkh)MMki ‚Äîkh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 ¬£82)(a(e) +7) +211 ‚Äî 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L‚Äîb*)6(@) , 2ba(x) by bM bM bM 6M (1+ ‚Äô)(¬´?(D ‚Äî W)a +17(D ‚Äî W)1) nTD1 b (1 ‚Äî B)17(D ‚Äî W)z 7 b1/D1 2b27(D‚ÄîW)z 2b17(D‚Äî W)1 ‚Äúopi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1‚Äî z)'(D‚ÄîW)(1+2) b17D1 T (+2) - 60 ‚Äî2)] @- W)[(1 +2) ‚Äî 41-2) ; b1¬∞D1Let D be an N x N diagonal matrix with d","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of‚Ä¶A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and ‚Äî1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 ‚Äî 2k + 2k?)(a(x) + 7) + 2(1 ‚Äî 2k) B(x)2a(x)_+ki ‚Äîkh)MMki ‚Äîkh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 ¬£82)(a(e) +7) +211 ‚Äî 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L‚Äîb*)6(@) , 2ba(x) by bM bM bM 6M (1+ ‚Äô)(¬´?(D ‚Äî W)a +17(D ‚Äî W)1) nTD1 b (1 ‚Äî B)17(D ‚Äî W)z 7 b1/D1 2b27(D‚ÄîW)z 2b17(D‚Äî W)1 ‚Äúopi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1‚Äî z)'(D‚ÄîW)(1+2) b17D1 T (+2) - 60 ‚Äî2)] @- W)[(1 +2) ‚Äî 41-2) ; b1¬∞D1Let D be an N x N diagonal matrix with d",
What is the current weather forecast?,"I apologize, but I cannot provide information about the current weather forecast. My purpose is to assist university students with questions about Image Processing for their module, ""Introduction to Image Processing,"" using the provided retrieved context. Weather forecasting falls outside of my domain.

If you have any questions related to image processing, I'd be happy to help!","I'm sorry, I can only answer questions related to image processing.","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the meaning of life?,"I can only assist with questions related to image processing for the ""Introduction to Image Processing"" module. The meaning of life is outside the scope of this course.

If you have any questions about image processing concepts, theories, or practical applications, feel free to ask!","I'm sorry, I can only answer questions related to image processing.","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term œà assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiÔ¨Åer trained using the SIFT descriptors. The pairwise term œÜ assigns to each pair of superpixels a potential to have similar or diÔ¨Äering labels based on the diÔ¨Äerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¬¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxÔ¨Çow algorithm to produce a Ô¨Ånal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048√ó1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term œà assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiÔ¨Åer trained using the SIFT descriptors. The pairwise term œÜ assigns to each pair of superpixels a potential to have similar or diÔ¨Äering labels based on the diÔ¨Äerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¬¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxÔ¨Çow algorithm to produce a Ô¨Ånal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048√ó1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How does a car engine work?,"I apologize, but I cannot answer that question. My expertise is limited to image processing topics relevant to the ""Introduction to Image Processing"" module. I can help you with questions about concepts like image segmentation, morphology, connected components, or related assignments.

If you have any questions about image processing, please feel free to ask!","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input",
What are the rules of chess?,"I cannot answer your question about the rules of chess as it is outside the scope of image processing. I can only assist with topics related to the ""Introduction to Image Processing"" module.

If you have any questions about image processing, feel free to ask!","I'm sorry, I can only answer questions related to image processing.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you tell me a joke?,"I'm sorry, but I don't have any jokes in my current knowledge base. My purpose is to assist university students with questions about Image Processing based on the provided context. Is there anything about image processing that I can help you with today?","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!",
What is the history of the internet?,"I'm sorry, but I cannot provide information on the history of the internet. My expertise is limited to topics within image processing, as covered in the ""Introduction to Image Processing"" module.

If you have any questions related to image processing, I'd be happy to help!","I'm sorry, I can only answer questions related to image processing.","tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding ‚Ç¨ Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ¬∞ ‚Äòe : ie ciewk pg toate om ‚Äòous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding ‚Ç¨ Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ¬∞ ‚Äòe : ie ciewk pg toate om ‚Äòous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
How do I learn to play the guitar?,"I apologize, but I can only provide assistance with questions related to image processing, as that is the scope of this university module. Learning to play the guitar falls outside of this domain.","I'm sorry, I can only answer questions related to image processing.","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let‚Äôs create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let‚Äôs initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f¬∞ University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:‚óè In the first interaction, the user can only draw a rectangle on the image.‚óè The user is encouraged to press Key 1 to start drawing the rectangle.‚óè The user can start segmentation after drawing rectangles or curves by pressing ‚ÄúEnter‚Äù.‚óè After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.‚óè After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.‚óè The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let‚Äôs create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let‚Äôs initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f¬∞ University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:‚óè In the first interaction, the user can only draw a rectangle on the image.‚óè The user is encouraged to press Key 1 to start drawing the rectangle.‚óè The user can start segmentation after drawing rectangles or curves by pressing ‚ÄúEnter‚Äù.‚óè After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.‚óè After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.‚óè The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5",
What are the best programming languages to learn in 2025?,"I can only provide information related to image processing topics covered in this module. For general questions about programming languages, I recommend consulting other resources.","I'm sorry, I can only answer questions related to image processing.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
Can you help me with my math homework?,"I can only assist with questions related to image processing, as that is the scope of this module. Unfortunately, I cannot help with general math homework.

If you have any questions about image processing concepts, your coursework for ""Introduction to Image Processing"", or anything else related to the module, feel free to ask!","I'm sorry, I can only answer questions related to image processing.

QA about Course-Related Questions","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
Who is the module convenor for COMP2032?,"The module convenor for COMP2032 is Dr. Tissa Chandesa.

You can contact him at Tissa.Chandesa@nottingham.edu.my or visit him at Room BB71 Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",The module convenor for COMPT2032 is Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my).,"MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
Where can I find the lecture slides?,"I can only assist with questions directly related to image processing topics based on the provided context. I cannot provide information about where to find lecture slides or other course materials.

For questions about course logistics, materials, or assignments, please contact your module convenor, Dr. Tissa Chandesa (Email: Tissa.Chandesa@nottingham.edu.my), or the Teaching Assistant, Mr. Irfan Yaqub (Email: hcxiy1@nottingham.edu.my). You can also visit Dr. Chandesa during his office hours at Room BB71 Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Lecture slides location&body=Dear Dr. Chandesa,\n\nI hope this email finds you well. \n\nI am writing to inquire about the location of the lecture slides for the Introduction to Image Processing module. I would appreciate it if you could provide information on where I can access them.\n\nThank you for your time and assistance.\n\nSincerely,\n[Your Name]"">Send email</a>",You can find the lecture slides on Moodle‚Äôs course page.,"slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16‚óè The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17‚óè Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16‚óè The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17‚óè Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!",
What are the office hours for the teaching assistants?,"I apologize, but the provided context does not contain information about the office hours for the teaching assistants. It only provides their contact email: Mr. Irfan Yaqub (Email: hcxiy1@nottingham.edu.my).

For information on teaching assistant office hours, I recommend you contact Mr. Irfan Yaqub directly via email.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:hcxiy1@nottingham.edu.my?subject=Question about Teaching Assistant Office Hours&body=Dear Mr. Yaqub,\n\nI hope this email finds you well. I am writing to inquire about your office hours for the Introduction to Image Processing module.\n\nSincerely,\n[Your Name]"">Send email</a>","I'm sorry, I don‚Äôt have access to the office hours for the teaching assistants.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How do I submit my coursework?,"The coursework for Introduction to Image Processing (COMP2032) requires both group and individual submissions.

**Group Submissions:**
You will need to submit the following as a group:
*   **Python codes:** This should be a ZIP folder containing all your `.py` files.
*   **A 6-page conference paper:** This must be a PDF file, with a maximum of 2000 words. Please note that additional pages will result in a 5% deduction for every extra page for the entire group.
*   **Result Images:** Submit these as a ZIP folder named ""Output"" containing a subfolder named ""Image Processing Pipeline"".

**Individual Submission:**
*   Each student must separately submit a **peer assessment form**. You can find the submission link for this on Moodle under ""Peer Assessment"". Failing to submit this will lead to a 10% penalty on your overall group mark.

**Important Notes:**
*   You will be provided with a conference paper template (`COMP2032-CW-GroupXXX.docx`). Remember to replace `XXX` with your group number (e.g., `013`). Do not change the formatting or headers, as marks will be deducted for non-compliance.
*   The red and blue colored text in the template should be replaced with your own content.

The submission deadline is **Friday, 25 April 2025, by 11:59 pm**. Please be aware that late submissions will incur a penalty of 5 marks (or 5% of the total 100 marks) for each late working day.

If you have further questions about the submission process, you can contact the module convenor, Dr. Tissa Chandesa (Email: Tissa.Chandesa@nottingham.edu.my), or the Teaching Assistant, Mr. Irfan Yaqub (Email: hcxiy1@nottingham.edu.my).

---
**References:**
*   output_context_reference
    *   ./documents/Assessment Sheet COMP2032 Coursework 2025.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",You submit your coursework through the Moodle assignment page.,"DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¬∂  ¬∂  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both ‚Äì before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x ‚Äì 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI‚Äô‚Äô(1) = (I‚Äô(1.5) - I‚Äô(0.5))/1  I‚Äô(0.5) = (I(1) - I(0))/1   and   I‚Äô(1.5) = (I(2) - I(1))/1  \  I‚Äô‚Äô(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the Ô¨Ålter order.Equivalencing of spatial frequencies is performed by Ô¨Årst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares Ô¨Åt can be used to create a normalized Butterworth Ô¨Ålter B(u, v, d0', n') (whered0' and n' are the Ô¨Åt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by Ô¨Åtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¬¢ x y,( ) = F 1- [ ( , , B u v d 0 ¬¢ n¬¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¬∂  ¬∂  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both ‚Äì before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x ‚Äì 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI‚Äô‚Äô(1) = (I‚Äô(1.5) - I‚Äô(0.5))/1  I‚Äô(0.5) = (I(1) - I(0))/1   and   I‚Äô(1.5) = (I(2) - I(1))/1  \  I‚Äô‚Äô(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the Ô¨Ålter order.Equivalencing of spatial frequencies is performed by Ô¨Årst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares Ô¨Åt can be used to create a normalized Butterworth Ô¨Ålter B(u, v, d0', n') (whered0' and n' are the Ô¨Åt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by Ô¨Åtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¬¢ x y,( ) = F 1- [ ( , , B u v d 0 ¬¢ n¬¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What is the assessment criteria for the coursework?,"The coursework for COMP2032, Introduction to Image Processing, is a group assessment worth 50% of your final grade. It involves developing a software solution in Python for an image processing problem and collaboratively producing a conference paper.

Here's a breakdown of the assessment criteria for the conference paper:
*   **Justification of chosen algorithms based on performed literature review:** 15%
*   **Critical analysis and discussion of your findings:** 25%
*   **Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result:** 30%

Additionally, **30%** of the mark for the programming component is dependent on the ability to run your code; unable to run codes will result in 0% being awarded for this part.

The conference paper should be a maximum of 2000 words and 6 pages in PDF format. Additional pages will result in a deduction.

The deliverables include:
1.  Python codes (.py files submitted as a ZIP folder)
2.  A 6-page conference paper (2000 words max, PDF format)

The coursework was released on Friday, March 7, 2025, and the submission deadline is Friday, April 25, 2025, by 11:59 pm. Late submissions will incur a penalty of 5 marks for each late working day. Marks and individual feedback will be returned via Moodle on the week commencing June 2, 2025.

(Source: retrieved_context)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The assessment criteria is as follows:
+Python code(s): 30% [unable to run codes will result in 0% being awarded]
+Conference Paper:
‚àí Justification of chosen algorithms based on performed literature review: 15%
‚àí Critical analysis and discussion of your findings: 25%
‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%.
+In-Lab Test Individual (50%) - Focuses on theories covered in the lectures and practical exercises","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction",
Can I get an extension for the coursework?,"I cannot provide an extension for the coursework. Coursework submission dates and late policies are set by the module convenor.

According to the provided context, the submission date for the coursework is **Friday, 25 April 2025, by 11:59pm**.

The late policy states: ""Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)"".

For any questions regarding extensions or specific circumstances, you should contact the module convenor, Dr. Tissa Chandesa, directly.

You can reach him at:
Email: Tissa.Chandesa@nottingham.edu.my
Office: Room BB71 Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Coursework Extension Request (COMP2032)&body="">Send email</a>","I‚Äôm not able to help you with an extension. You should contact the module convenor (Dr. Tissa Chandesa) and request a coursework extension, providing a reason for it.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n ¬ª |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n ‚Äî 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 ‚Äî 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2‚Äù quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is‚Äîusing equation (4)Proof. The points in E for whichn Sq=1 i=0form ann ‚Äî 1 dimensional subset of E, which we will call P. We can find the n ‚Äî 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)‚Äù (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‚ÄòV,,/Vp of the volume|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n ¬ª |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n ‚Äî 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 ‚Äî 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2‚Äù quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is‚Äîusing equation (4)Proof. The points in E for whichn Sq=1 i=0form ann ‚Äî 1 dimensional subset of E, which we will call P. We can find the n ‚Äî 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)‚Äù (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‚ÄòV,,/Vp of the volume|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the room number for the lectures?,"I apologize, but I cannot provide information about the room number for lectures. My knowledge base is limited to the concepts and theories of image processing as covered in the module materials.

For questions about lecture venues, I recommend you contact:

*   **Dr. Tissa Chandesa** (Module Convenor)
    *   Email: Tissa.Chandesa@nottingham.edu.my
    *   Office: Room BB71 Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia
*   **Mr. Irfan Yaqub** (Teaching Assistant)
    *   Email: hcxiy1@nottingham.edu.my

They will be able to provide you with the most accurate and up-to-date information regarding lecture locations.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Enquiry about Lecture Room Number&body=Dear Dr. Chandesa,

I hope this email finds you well.

I am writing to inquire about the room number for the Image Processing lectures. I could not find this information in the course materials.

Could you please provide me with this information?

Thank you for your time and assistance.

Sincerely,
[Your Name]
[Your Student ID]"">Send email</a>","The lectures are in F3A04 (Block F3, Level A, Room 04) on Wednesdays (09:00 to 11:00). Labs are in BB80 (Block B, Level B, Room 80) on Tuesdays (14:00 to 16:00)","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
How many people are in the group for the coursework?,"I can only provide information about image processing topics. For questions about coursework groups or other assignment-specific details, please refer to the assignment brief or contact your module convenor, Dr. Tissa Chandesa. You can reach him at Tissa.Chandesa@nottingham.edu.my.",You need to form a group of four people for the coursework.,"30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What programming language is used in this module?,"The programming language used in this module is Python. This is indicated by the use of libraries such as OpenCV and Matplotlib, which are Python libraries, and the mention of Python modules in the context of importing libraries.

**References:**
*   `./documents/Lab 1A.pdf`
*   `./documents/Lab 2.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 1A.pdf"" target=""_blank"">Lab 1A</a>

",This module provides practical experience of image processing using Python.,"import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.‚óè Aliasing using as. This modifies the name of the module during referencing. It is done when the module‚Äôs name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:‚óè Read it in using imread.‚óè Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).‚óè Write it to a new file using imwrite.shape can provide basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).‚óè Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).‚óè Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.‚óè Using import. The entire module is imported, and the module must be referenced when used.‚óè Using from‚Ä¶ import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The","import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.‚óè Aliasing using as. This modifies the name of the module during referencing. It is done when the module‚Äôs name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:‚óè Read it in using imread.‚óè Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).‚óè Write it to a new file using imwrite.shape can provide basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).‚óè Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).‚óè Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.‚óè Using import. The entire module is imported, and the module must be referenced when used.‚óè Using from‚Ä¶ import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The",
Where can I find the datasets for the coursework?,"I can only answer questions about image processing concepts based on the provided context. I cannot provide information about coursework datasets.

For questions regarding coursework, please contact:
*   Dr. Tissa Chandesa (Module Convenor) at Tissa.Chandesa@nottingham.edu.my
*   Mr. Irfan Yaqub (Teaching Assistant) at hcxiy1@nottingham.edu.my

You could also attend Dr. Chandesa's office hours.",You can download the datasets from Moodle‚Äôs coursework page.,"3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder‚Äôs name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image‚Äôs path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder‚Äôs name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image‚Äôs path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
