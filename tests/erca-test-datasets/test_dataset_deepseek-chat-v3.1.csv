input,actual_output,expected_output,retrieval_context,context,source_file
What is Thresholding?,"Based on the lecture materials, **thresholding** is a fundamental image processing technique used to convert a grayscale image into a binary image (an image with only two pixel values). Here's a clear breakdown:

### What is Thresholding?
1.  **Purpose:** It classifies each pixel in an image as either part of the **foreground (object)** or the **background**.
2.  **Process:** You choose a specific brightness value, called the **threshold value (T)**.
    *   For each pixel, if its brightness (intensity) is **less than T**, it is set to a value representing the object (often 0, black).
    *   If its brightness is **greater than or equal to T**, it is set to a value representing the background (often 255, white).
3.  **Result:** The output is a **binary image**, where pixels can only have one of two values (e.g., 0 or 255)[citation:1].

### Simple Example:
Imagine a dark object on a light background. Thresholding helps isolate the dark object by turning all pixels darker than `T` black and all lighter pixels white[citation:1].

### Key Points:
*   It's a primary method for **image segmentation** (identifying and separating objects)[citation:4].
*   The basic concept can also be extended to color images by defining sets of color values that correspond to objects of interest[citation:1].
*   Choosing the right threshold (`T`) is critical. Methods like **Otsu's thresholding** can automatically determine a good value by analyzing the image's histogram[citation:4].

For a hands-on application, you can practice this using OpenCV's `threshold()` function with the `THRESH_BINARY` flag, as described in your lab sheet[citation:4].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

",Thresholding is a method of image segmentation that creates binary images. It works by setting a threshold value (T) and classifying pixels as either object or background based on whether their brightness is above or below this threshold.,"University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.",
What is the difference between Otsu and Rosin thresholding methods?,"Based on the lecture materials, both Otsu and Rosin are thresholding methods used to binarize images, but they differ significantly in their approaches and the types of histograms they are best suited for.

**Otsu's Method** (cited from Lec-4-Thresholding and Binary Images.pdf):  
Otsu is designed for **bimodal histograms**—histograms with two distinct peaks, typically representing foreground and background. It works by exhaustively searching for the threshold that minimizes the intra-class variance (maximizes the separation between classes). It is widely used because it is automatic and works well when the image has clear foreground and background intensity distributions.

**Rosin's Unimodal Method** (cited from Lec-4-Thresholding and Binary Images.pdf):  
Rosin’s method, on the other hand, is intended for **unimodal histograms**—those with only one dominant peak (e.g., images where most pixels are one intensity, like text on a mostly white page). It:  
- Finds the peak of the histogram.  
- Draws a line from the peak to the top of the furthest bin.  
- The bin value that is farthest from this line is chosen as the threshold.  

This method is particularly useful when the histogram isn't clearly bimodal, such as in edge magnitude images or when foreground objects are small relative to the background.

**Key Differences Summary:**
| Feature                | Otsu                           | Rosin                          |
|------------------------|--------------------------------|--------------------------------|
| **Histogram Type**     | Bimodal (two peaks)            | Unimodal (one peak)            |
| **Method**             | Minimizes intra-class variance | Geometric line-based approach  |
| **Use Case**           | General foreground/background  | Small objects on large background, edge images |

If you’re working with an image that has a clearly bimodal histogram (e.g., a dark object on a light background), Otsu is likely more appropriate. For unimodal cases (e.g., sparse text or edges), Rosin may perform better.

Would you like guidance on how to apply these in code or compare results for a specific image?

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

","Otsu's method is used for bimodal histograms, assuming two peaks that can be separated by a single threshold. Rosin's method is for unimodal histograms, where there's only one peak, and it finds the threshold by drawing a line from the peak to the furthest bin.","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f’ =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¢ .° , ,* ,• Finds the peak• Draws a line from there to the top of thefurthest bin• Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin’s unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f’ =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods• Imaging conditions and object properties can vary within a single image as well as across sets of images'e '• Histograms can be too complex for any method’s|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f’ =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¢ .° , ,* ,• Finds the peak• Draws a line from there to the top of thefurthest bin• Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin’s unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f’ =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods• Imaging conditions and object properties can vary within a single image as well as across sets of images'e '• Histograms can be too complex for any method’s|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a structuring element in morphological operations?,"Of course! A structuring element is a fundamental component in morphological image processing operations.

In simple terms, a **structuring element** is a small, predefined shape or pattern—like a small binary image or mask—that you use to probe and interact with the input image. It defines the neighborhood around each pixel that will be considered during the operation.

Here are the key characteristics of a structuring element, based on the course materials:

1.  **Function:** It is analogous to the filter mask used in spatial filtering or convolution. It is the tool that defines the ""shape"" and ""extent"" of the morphological operation.
2.  **Shape:** It can be created in various shapes. Common examples from your lab work include:
    *   Rectangular
    *   Elliptic (or circular)
    *   Cross-shaped
3.  **Size:** The size of the structuring element is variable (e.g., 3x3, 5x5, 7x7). A larger structuring element will produce a more pronounced effect on the image.
4.  **Origin:** The origin (the center pixel used for alignment) does not necessarily have to be in the physical center of the element. It can be placed elsewhere to create asymmetric effects.

**How it works:**
You position the origin of the structuring element over a pixel in your binary image. The operation (like dilation or erosion) then uses the pattern of ones (foreground) and zeros (background) in the structuring element to determine how to change the value of that pixel in the output image. The choice of shape and size directly controls the result, such as how much an object grows (dilation) or shrinks (erosion).

*Source: './documents/Lec-5-Morphology.pdf'*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

",A structuring element is a small matrix or kernel used to probe an image in morphological operations like erosion and dilation. It defines the neighbourhood of a pixel and is used to modify the pixel's value based on the values of its neighbours.,"if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt",
Explain the process of Histogram Backprojection.,"Of course! I'd be happy to explain the process of Histogram Backprojection, a technique used to locate a specific object in an image based on its color characteristics.

Based on the provided context from the lecture materials, here is a clear, step-by-step breakdown of the process:

### The Goal of Histogram Backprojection
The main goal is to find the region in a new, often cluttered, image that has a color distribution most similar to a pre-defined ""model"" or ""target"" object. It is designed to be robust, meaning it works well even when the object is partially hidden (occluded) or when there are other objects in the scene with similar colors.

The process can be broken down into three key steps:

---

### Step 1: Create the Model Histogram
Before you can search for an object, you need to know what you're looking for.

1.  **Isolate the Target:** You start with a small image that contains *only* the object you want to find (e.g., a specific blue and white striped shirt).
2.  **Compute its Histogram:** You create a color histogram of this target object. A color histogram counts how many pixels in the image fall into each ""bin"" of a discrete color space (like Red, Green, Blue).
3.  **The Result:** You now have a statistical model, `M`, that represents the probability of each color appearing in your target object.

### Step 2: Ratio Histogram Backprojection
This is the core of the algorithm. You take your model histogram and a histogram of the entire search image to create a probability map.

1.  **Compute the Image Histogram:** Create a color histogram, `I`, for the entire large image you want to search through.
2.  **Calculate the Ratio:** For every color bin `j` in the histograms, you calculate a ratio:
    `Ratio[j] = min( M[j] / I[j], 1.0 )`
    *   **What this means:** This ratio measures how ""unusual"" or ""diagnostic"" a color is for your target.
    *   **Strong Response:** If a color is very common in your target (`M[j]` is high) but rare in the overall image (`I[j]` is low), the ratio will be high (e.g., close to 1). This color is a strong indicator of your target's location.
    *   **Weak Response:** If a color is common *both* in your target and in the background or other objects (e.g., the color white), the ratio will be low. This color is a weak indicator because it doesn't help narrow down the search.
3.  **Create the Backprojection Image:** You now create a new, single-channel image (the ""backprojection image"") that is the same size as your search image. For each pixel in the search image, you look up its color, find the corresponding ratio value for that color bin, and set the pixel's value in the backprojection image to that ratio.
    *   **The Result:** You now have a probability map where bright spots (high values) indicate areas where the colors are statistically more likely to belong to your target object.

### Step 3: Convolution (Peak Detection)
The backprojection image is often noisy. This final step smooths it out and finds the most probable location.

1.  **Apply a Smoothing Filter:** You convolve the backprojection image with a pre-defined smoothing kernel or window (the context mentions a typical size of 32x32 pixels). This has the effect of averaging the probability values over a small region.
2.  **Find the Peak:** You then search this smoothed probability map for the location with the highest value (the ""darkest black dot"" in the context's figures, indicating a peak). This location is the algorithm's best guess for the center of your target object in the search image.

---

### Why is it Effective?
*   **Robust to Occlusion:** It works even if the object is partially hidden because it only relies on the visible colors that match the model.
*   **Efficient:** The context notes it can run very quickly (e.g., four times a second), making it suitable for real-time applications like object tracking.
*   **Focus on Diagnostic Colors:** By using the ratio `M[j]/I[j]`, it intelligently weights colors, relying more on those that are unique to the target and less on those that are common throughout the scene.

I hope this breakdown makes the process clear! If you have any questions about a specific step, please feel free to ask.

**Source:**
`./documents/Swain1.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

","Histogram backprojection is a technique for locating an object in an image based on a model histogram of its appearance (e.g., color). Each pixel is assigned a probability based on how well it matches the model histogram, producing a probability map where high values indicate likely object locations.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main components of a CNN?,"Based on the provided lecture materials, the main components of a Convolutional Neural Network (CNN) are:

1.  **Convolutional Layer**: This is the core building block of a CNN. It uses filters (or kernels) to perform convolution operations on the input data, extracting features like edges, textures, and patterns. The filter hyperparameters (number of filters, stride, zero-padding) are required in this layer.
2.  **Pooling Layer** (often following a convolutional layer): This layer reduces the spatial dimensions (width and height) of the input volume, which decreases the computational power required and helps in controlling overfitting. Common types are Max Pooling and Average Pooling.
3.  **Fully Connected Layer**: Typically found at the end of the network, this layer connects every neuron in one layer to every neuron in the next layer. It is used to combine the features extracted by the previous layers to perform the final classification or regression task.

These components work together to allow CNNs to effectively process and understand visual data, capturing both local and global relationships within images.

**Source:** The information about the layers, particularly the requirement of filter hyperparameters in the Convolutional Layer, is derived from the provided context in `./documents/Practicals with Solutions - CNN.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","A Convolutional Neural Network (CNN) is composed of several layers, including the input layer, convolutional layer, pooling layer, activation function, fully connected layer, and output layer.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
What is the purpose of the pooling layer in a CNN?,"Based on the retrieved context from the lecture materials, the purpose of the pooling layer in a Convolutional Neural Network (CNN) is to **reduce the size of the feature maps** produced by the convolutional layers.

Specifically, pooling layers help in:
- Decreasing the spatial dimensions (height and width) of the feature maps
- Reducing the computational complexity for subsequent layers
- Providing a form of translation invariance (making the network less sensitive to the exact position of features)

In the provided example from the code snippet, a max-pooling layer is applied after each convolutional layer and activation function to downsample the feature maps.

This information is supported by the lecture materials, including the practical implementation details and the concepts covered in the CNN module.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Laboratory Sheet 9 - CNN.pdf"" target=""_blank"">Laboratory Sheet 9   CNN</a>

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","The pooling layer occurs after a convolutional layer and its purpose is to reduce the dimensionality of the resulting feature map. Common pooling operations include max pooling, average pooling, and global average pooling.","out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f° University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f° University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What is the difference between erosion and dilation in morphological image processing?,"Of course! That's a fundamental concept in morphological image processing. The key difference between erosion and dilation lies in their effect on the foreground objects in a binary image.

Based on the lecture materials, here is a breakdown of the difference:

### 1. Dilation

*   **Purpose:** To **grow** or **expand** the boundaries of the foreground (white) regions.
*   **Analogy:** It's like adding an extra layer of pixels around every object.
*   **Effect:**
    *   Fills in small holes and gaps within objects (e.g., connecting broken characters in text).
    *   Smooths object contours, often rounding off corners.
    *   Can cause separate, nearby objects to merge into one.
*   **How it works (Algorithm):** A structuring element (a small shape like a square or cross) is superimposed on each foreground pixel. If the origin of the structuring element touches a foreground pixel, the entire area of the structuring element is turned to foreground. As the provided context states: *""Dilation input image (note rounding of corners)""* [./documents/Lec-5-Morphology.pdf].

### 2. Erosion

*   **Purpose:** To **shrink** or **contract** the boundaries of the foreground (white) regions.
*   **Analogy:** It's like peeling away the outer layer of pixels from every object.
*   **Effect:**
    *   Removes small isolated noise points or thin protrusions.
    *   Can separate two objects that are weakly connected.
    *   shrinks the size of objects.
*   **How it works (Algorithm):** A structuring element is superimposed on each foreground pixel. *""If any of the ‘1’ pixels in the structuring element overlap (intersect) the background then the foreground pixel is also set to background""* [./documents/Lec-5-Morphology.pdf]. In simpler terms, a foreground pixel will only remain foreground if the entire structuring element fits within the foreground. Otherwise, it is eroded (turned to background).

### Summary Table

| Feature | Dilation | Erosion |
| :--- | :--- | :--- |
| **Primary Effect** | **Expands** foreground objects | **Shrinks** foreground objects |
| **Fills Gaps?** | **Yes** | No |
| **Removes Noise?** | No | **Yes** (small objects) |
| **Separates Objects?** | No (often merges them) | **Yes** |
| **Duality** | Eroding the foreground is equivalent to dilating the background | Dilating the foreground is equivalent to eroding the background |

### Practical Example from Your Lab
Your lab exercises provide perfect examples of their uses:
*   **Dilation:** You are instructed to use dilation on `text_broken.tif` to *""fill in the gaps between the broken characters""* [./documents/Lab 5.pdf].
*   **Erosion:** You are instructed to use erosion on `Squares.tif` to observe its effect, noting how it can remove or shrink smaller objects [./documents/Lab 5.pdf]. The context also notes its use for *""counting objects (cells, coins)""* by separating them if they touch [./documents/Lec-5-Morphology.pdf].

I hope this clarifies the difference! They are often used in sequence with other operations like **opening** (erosion followed by dilation) and **closing** (dilation followed by erosion) to achieve more complex filtering tasks.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

","Erosion removes pixels on object boundaries, shrinking bright regions and enlarging dark ones. Dilation adds pixels to the boundaries of objects, enlarging bright regions and shrinking dark ones.","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.● Read in the Squares.tif image from Moodle.● Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.● Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)● Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’ pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.● Read in the Squares.tif image from Moodle.● Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.● Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)● Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’ pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and",
What is the Hough Transform used for?,"Based on the lecture materials, the **Hough Transform** is a technique used in image processing to detect shapes that can be represented by mathematical equations, such as lines, circles, or other geometric primitives[^1][^2].

Specifically, it is used to:
1.  **Identify lines or other shapes** within an image after edge detection has been performed.
2.  Convert the problem from image space into a **parameter space** (e.g., using parameters like ρ and θ for lines).
3.  Implement a **""voting"" procedure** where edge points in the image vote for all possible parameter combinations that could define a shape passing through them. The shapes that receive the most votes are considered detected[^1].

This method is particularly useful because it can detect shapes even if they are incomplete or partially occluded, as it looks for collective evidence from individual points[^2].

[^1]: Source: Lab 6.pdf - Instructions for applying the Hough Line Transform to a Sudoku image.
[^2]: Source: Lec-6B-Hough Transform.pdf - Detailed explanation of the Hough Transform's purpose, how it works, and its advantages over template matching.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","The Hough Transform is a feature extraction technique used to find instances of objects with a certain shape, such as lines or circles, within an image. It works by transforming points from the image space to a parameter space, where peaks correspond to the parameters of the detected shapes.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.● Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a “voting” procedure on the points which are part of the edges in the image.● Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.● Apply the Hough Line Transform on the edge-detected image using HoughLines.● To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (𝜌,𝜃) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ……………………………. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e© Nottingham “i UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don’t know the line parameters – we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line’s   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.● Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a “voting” procedure on the points which are part of the edges in the image.● Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.● Apply the Hough Line Transform on the edge-detected image using HoughLines.● To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (𝜌,𝜃) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ……………………………. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e© Nottingham “i UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don’t know the line parameters – we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line’s   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines",
What is the Livewire segmentation algorithm?,"Based on the provided materials, the **Livewire segmentation algorithm** (also known as ""Intelligent Scissors"") is an interactive image segmentation method designed to help users accurately trace object boundaries in an image. Here's a clear breakdown:

### Key Concept:
Livewire allows a user to specify a starting point on an object’s boundary. As the user moves the cursor (the ""free point""), the algorithm dynamically computes and ""snaps"" the shortest path along strong image edges between the start point and the cursor position, effectively avoiding crossing object boundaries. This creates a precise segmentation with minimal user effort.

### How It Works (Major Steps):
1. **Image Processing**: Preprocess the image (e.g., compute gradients) to highlight edges.
2. **Convert Image to Graph**: Treat pixels as nodes in a graph, with edges between adjacent pixels weighted by a cost function (often based on image gradients and edge features).
3. **Shortest Path Algorithm**: Use an algorithm like Dijkstra’s to find the minimal cost path between points, favoring strong edges.
4. **Interactive Visualization**: Display the computed path in real-time as the user moves the cursor.
5. **Event Handling**: Manage user inputs (clicks, cursor movement) to update the segmentation.

### Why It’s Useful:
- It reduces manual tracing effort by leveraging image features (like edges) to guide the segmentation.
- It is particularly effective for extracting regions of interest (ROIs) with clear boundaries.

For more details, you can refer to the original paper:  
*Mortensen, E. N., & Barrett, W. A. (1995). Intelligent Scissors for Image Composition. In ACM SIGGRAPH.*

If you have further questions about implementation or related concepts, feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

","Livewire, also known as ""Intelligent Scissors,"" is an interactive image segmentation algorithm. It allows a user to trace an object's boundary, and the algorithm finds the minimum-cost path that follows object boundaries. It's based on graph theory and uses Dijkstra's algorithm to find the shortest path between user-selected points.","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the GrabCut algorithm?,"Of course! The GrabCut algorithm is an interactive image segmentation technique designed to efficiently and accurately extract the foreground of an image from its background.

Based on the provided materials, here is a breakdown of what it is and how it works:

### Core Concept

GrabCut is a practical implementation that uses a powerful optimization technique called **graph cuts** to perform foreground/background segmentation. It is specifically designed for the task of interactive foreground extraction, making it a popular tool for image editing.

### How it Works (Simplified)

The process typically involves the following steps, though it can be iterative for refinement:

1.  **User Input:** The process begins with minimal user interaction. A user simply draws a rectangle (**red rectangle**) loosely around the object they want to extract. This rectangle defines a region where the algorithm assumes the background is *outside* the rectangle and the foreground is *inside*.
2.  **Initialization:** The algorithm uses this initial input to create preliminary models (Gaussian Mixture Models or GMMs) for the foreground and background color distributions.
3.  **Graph Cut Optimization:** A graph is built where each pixel is a node. The algorithm calculates the cost (or ""energy"") of labeling each pixel as either foreground or background. This cost is based on:
    *   **Color Similarity (Region Properties):** How well a pixel's color fits the current foreground or background GMM.
    *   **Edge Information (Boundary Properties):** The contrast between adjacent pixels. A strong edge (big difference in color) between two pixels makes it expensive to assign them the same label (e.g., one foreground, one background).
4.  **Iteration:** A key advancement of GrabCut is that it is **iterative**. The algorithm repeats the steps of estimating the GMMs and then performing the graph cut optimization. With each iteration, the models of the foreground and background colors become more accurate, leading to a better segmentation. This iterative process is what allows it to work well even from a very rough initial user input.
5.  **Border Matting (Optional Refinement):** For a professional result, GrabCut can also perform ""border matting."" This estimates a smooth, semi-transparent alpha matte (an alpha channel) around the object's boundary to handle mixed pixels and blur, creating a clean, natural-looking cut-out free from jagged edges or ""background bleeding.""

### Key Advantages (Why it's ""Magic"")

*   **Minimal User Interaction:** It requires significantly less user effort than earlier tools like the ""Magic Wand"" (which uses only color) or ""Intelligent Scissors"" (which uses only edges).
*   **High Quality Results:** By combining both color *and* edge information through graph cuts, it achieves very accurate segmentations.
*   **Iterative Refinement:** The iterative process allows the algorithm to correct itself and improve the result automatically.
*   **Robust to Imperfect Initialization:** As noted in the context, GrabCut performs nearly as well as the standard Graph Cut algorithm even when some foreground training data is missing from the initial user input.

***Source:*** *The core description of GrabCut's functionality, including its iterative nature and use of graph cuts, is derived from the provided excerpt of the paper by Rother, Kolmogorov, & Blake (2004) found in `./documents/grab_cut.pdf`.*

**Cited Materials:**

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

","GrabCut is an interactive foreground extraction algorithm that uses iterated graph cuts. The user draws a rectangle around an object, and the algorithm automatically extracts the object from the background.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single “out- side” lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images’, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single “out- side” lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images’, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original",
What is histogram equalization?,"Based on the provided lecture materials, **histogram equalization** is a fundamental image processing technique used to improve the contrast of an image.

Here's a breakdown of what it is and how it works:

### Core Concept
Histogram equalization works by **redistributing the pixel intensities** in an image to achieve a more uniform distribution across the entire intensity range. This process effectively stretches the contrast, making dark areas darker and light areas lighter, which can reveal details that were not previously visible.

### The Algorithm
According to the lecture notes, the process involves three main steps:

1.  **Compute the Cumulative Distribution Function (CDF)** of the input image's histogram.
2.  **Use the CDF as a look-up table**: For each pixel in the input image, the corresponding output pixel intensity is calculated using this CDF.
3.  **Scale the output**: The CDF values will be in the range of 0–1, so the equalized image is scaled to fit the range supported by the output image format (e.g., 0 to 255 for an 8-bit image).

The result is an output image whose histogram will be approximately uniform.

### A Simple Example
The practical exercises provide a worked example. For a given set of input pixel values and their frequencies, you:
*   Calculate the normalized frequency (probability) for each intensity level.
*   Compute the cumulative density for each level.
*   Map each original intensity to a new value by multiplying its cumulative density by the maximum intensity value (e.g., 7 for a 3-bit image) and rounding to the nearest integer.

### Difference from Contrast Stretching
It's important to distinguish histogram equalization from another contrast enhancement technique, **contrast stretching**. While both aim to improve contrast:
*   Histogram equalization is a **non-linear** process that specifically aims for a uniform distribution of intensities.
*   Contrast stretching is a **linear** process that simply expands the original range of intensity values to a desired new range (e.g., [0, 255]).

In summary, histogram equalization is a powerful method for automatic contrast adjustment based on the image's own histogram.

**Source:** './documents/Lec-2-Histogram (Whole Image Understanding).pdf', './documents/Practicals - Histogram (Whole Image Understanding).pdf'

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",Histogram equalization is a technique used to improve the contrast of an image by redistributing the pixel intensities to achieve a more uniform distribution. This is done by computing the cumulative density function (CDF) and using it as a look-up table to map input pixel values to output pixel values.,"the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee —ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 – 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let’s BREAK this down…So, in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their","the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee —ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 – 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let’s BREAK this down…So, in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their",
What is the difference between sampling and quantization in digital image formation?,"Based on the lecture materials, sampling and quantization are two distinct but complementary processes in digital image formation:

### Sampling
- **Definition**: Digitization of the spatial coordinates (x,y) of an image
- **Purpose**: Determines the **spatial resolution** (how many pixels the image contains)
- **Key consideration**: Must follow the Nyquist rate - samples must be taken at twice the frequency of the highest frequency component to avoid aliasing artifacts
- **Problem**: Under-sampling (sampling below Nyquist rate) causes aliasing artifacts

### Quantization
- **Definition**: Digitization of the light intensity function (brightness/color values)
- **Purpose**: Determines the **radiometric resolution** (how many intensity/color levels are available)
- **Can be**: 
  - Uniform scalar quantization (equal intervals)
  - Non-uniform scalar quantization (variable intervals)
- **In color images**: Often involves vector quantization where RGB values are mapped to a color palette

### Key Difference
**Sampling deals with spatial resolution** (how many pixels), while **quantization deals with intensity/color resolution** (how many intensity levels or colors).

*Source: Lec-1B-Digital Images and Point Processes.pdf*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","Sampling is the digitization of spatial coordinates, determining the spatial resolution of the image (number of pixels). Quantization is the digitization of the light intensity function, determining the number of grey levels or colors that can be represented at each pixel.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE…Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE…Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:«< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‘@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression• Map vector values (R,G,B) onto scalar values• Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f’Building a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f’Building a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE…Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE…Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:«< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‘@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression• Map vector values (R,G,B) onto scalar values• Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f’Building a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f’Building a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2",
What is aliasing in the context of image sampling?,"Based on the provided lecture materials, **aliasing** in the context of image sampling refers to the visual artefacts that occur when an image is **undersampled**.

Here's a breakdown of the concept:

*   **What causes it?** Aliasing is introduced when the **sampling rate** (how often you measure the image) is **less than the Nyquist rate** of the original image signal. The Nyquist rate is essentially the minimum sampling rate required to accurately capture a signal without loss of information.
*   **What is the effect?** When undersampled, two different signals (or images) become indistinguishable from the sampled data. In the context of vision, the ""true"" image and the one reconstructed by your visual system from the undersampled data look the same, leading to visual artefacts like jagged edges (especially on diagonals), moiré patterns, or the appearance of false details.

In short, aliasing is a distortion that occurs because the sampling process was too coarse to capture all the detail in the original image.

**Source:** This explanation is based on content from `./documents/Lec-1B-Digital Images and Point Processes.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",Aliasing occurs when an image is under-sampled (sampled at a rate below the Nyquist rate). This results in artifacts where two different signals (images) become indistinguishable when sampled.,"fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing • Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original • Smooth out high frequency signals before sampling so its impossible to “see” the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? • When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels• Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can’t increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition “seamlessly” in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass ﬁltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter ﬁtting of a Butterworth low-pass ﬁlter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2","fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing • Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original • Smooth out high frequency signals before sampling so its impossible to “see” the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? • When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels• Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can’t increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition “seamlessly” in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass ﬁltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter ﬁtting of a Butterworth low-pass ﬁlter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2",
How can noise be reduced in an image?,"Of course! Noise reduction in images is a fundamental topic in image processing. Based on your lecture and lab materials, here are the primary methods, broken down into understandable parts.

The core idea is to replace the value of a noisy pixel with a new value calculated from the pixels around it. This process is called **spatial filtering**.

There are two main categories of filters used for this: **Linear** and **Non-Linear**.

### 1. Linear Filtering

This method uses a mathematical operation called **convolution**. A small matrix (called a **kernel** or **mask**) is slid over every pixel in the image. The new pixel value is calculated as the weighted sum of the neighbouring pixels and the corresponding weights in the kernel [./documents/Lec-3A-Linear Filters.pdf].

Two common types of linear filters for noise reduction are:

*   **Mean Filtering (or Averaging Filter):**
    *   **How it works:** This filter simply replaces a pixel's value with the average (mean) of all pixel values in a small surrounding area (e.g., a 3x3 or 5x5 window). It's very effective at reducing random noise because the random positive and negative noise values tend to cancel each other out when averaged [./documents/Lec-3A-Linear Filters.pdf].
    *   **Trade-off:** While it reduces noise, it also blurs the image and fine details (like edges) because it averages everything together.
    *   **As practiced in your lab:** You can implement this using OpenCV's `blur()` function with different kernel sizes (e.g., 3, 5, 7) [./documents/Lab 3.pdf].

*   **Gaussian Filtering:**
    *   **How it works:** This is a more sophisticated version of the mean filter. Instead of giving all neighbouring pixels the same weight, it uses a kernel based on the **Gaussian function** (a ""bell curve""). Pixels closer to the center of the kernel have a higher weight and contribute more to the final average, while pixels farther away have less influence [./documents/Lec-3A-Linear Filters.pdf].
    *   **Why it's better:** This results in much smoother noise reduction with less overall blurring of edges compared to a simple mean filter.
    *   **As practiced in your lab:** You can implement this using OpenCV's `GaussianBlur()` function, where you specify a `sigma` value (which controls the width of the bell curve and thus the amount of blurring) and an appropriate kernel size [./documents/Lab 3.pdf].

### 2. Non-Linear Filtering

These filters do not use a weighted sum and are often better at preserving sharp edges while removing noise.

*   **Median Filtering:**
    *   **How it works:** This filter is highly effective against ""salt-and-pepper"" noise (random white and black pixels). It works by replacing a pixel's value with the **median** value of the pixels in the surrounding window.
    *   **Why it's effective:** The median is not influenced by extremely high or low values (the salt and pepper noise), so those outlier pixels are effectively removed without significantly blurring the edges in the image.

### Summary of Methods:

| Method | Best For | How it Works | Key Advantage | Key Disadvantage |
| :--- | :--- | :--- | :--- | :--- |
| **Mean Filter** | General random noise | Replaces pixel with the average of its neighbours | Simple, effective for random noise | Blurs edges and fine details |
| **Gaussian Filter** | General random noise | Replaces pixel with a weighted average (more weight to closer pixels) | Smoother results, preserves edges better than mean filter | Still causes some blurring |
| **Median Filter** | **Salt-and-pepper** noise | Replaces pixel with the median of its neighbours | Excellent at removing outlier noise, preserves edges well | Less effective for general Gaussian noise |

**Pro Tip from your lab sheet:** The best way to understand these is to try them yourself! Load a noisy image (like the provided cameraman image with salt-and-pepper noise) and apply these filters with different parameters (kernel size, sigma) to see the results firsthand [./documents/Lab 3.pdf].

If you have a more specific type of noise or a particular scenario in mind, feel free to ask for a more detailed explanation

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3A-Linear Filters.pdf"" target=""_blank"">Lec 3A Linear Filters</a>

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

","Noise can be reduced using various filtering techniques. For additive noise like Gaussian noise, mean or Gaussian filtering can be effective. For ""salt and pepper"" noise, a median filter is often used.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I’) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‘‘1’* means the object has been correctly located, a “2” indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise• Noise is randomly added to each value• Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern – multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)×P(x-1,y-1)  + F(0,-1)×P(x,y-1)  + F(+1,-1)×P(x+1,y-1)  + F(-1,0)×P(x-1,y)  + …  + F(+1,+1)×P(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise••Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI• On average they give the right value• They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (μ) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2πσ  e  (x  --  2σ  μ)  2 μ28Gaussian NoiseThe level of noise is related to the Gaussian parameter, σσ = 1σ = 10σ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I’) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‘‘1’* means the object has been correctly located, a “2” indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise• Noise is randomly added to each value• Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern – multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)×P(x-1,y-1)  + F(0,-1)×P(x,y-1)  + F(+1,-1)×P(x+1,y-1)  + F(-1,0)×P(x-1,y)  + …  + F(+1,+1)×P(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise••Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI• On average they give the right value• They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (μ) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2πσ  e  (x  --  2σ  μ)  2 μ28Gaussian NoiseThe level of noise is related to the Gaussian parameter, σσ = 1σ = 10σ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.",
What is the difference between linear and non-linear filters?,"Of course! This is a fundamental concept in image processing. Based on the provided lecture materials, here is a clear breakdown of the differences between linear and non-linear filters.

### Core Difference

The primary difference lies in the mathematical operation they perform on the pixel values within the filter's neighborhood (kernel).

*   **Linear Filters** compute a **weighted sum** (a linear combination) of the pixel values.
*   **Non-Linear Filters** perform a **non-linear mathematical operation** (like sorting or calculating distances) on the pixel values.

---

### Linear Filtering

1.  **How it Works:** Linear filtering is implemented using **convolution**. A kernel (a small matrix of weights) is slid over every pixel in the image. For each position, the output pixel value is calculated as the sum of the products of the kernel weights and the corresponding pixel values beneath them.
2.  **Operation:** `Output = Weighted Sum of Neighbouring Pixels`
3.  **Common Examples:**
    *   **Mean Filter:** Simplest linear filter. Each weight in the kernel is the same (e.g., 1/9 for a 3x3 kernel), so it calculates the average of the neighborhood.
    *   **Gaussian Filter:** Uses a kernel with weights based on the Gaussian (bell-shaped) function. Pixels closer to the center have higher weights, leading to a smoother, more natural blurring effect compared to the mean filter.
4.  **Effect on Images:** They are excellent for smoothing an image and reducing certain types of noise (like Gaussian noise) but have a significant drawback: they **smooth sharp image changes**, such as edges, which can cause the image to become blurry [`./documents/Lec-3B-Non-Linear Filters.pdf`].

---

### Non-Linear Filtering

1.  **How it Works:** Non-linear filtering is performed **without convolution**. There is no weighted sum. Instead, the output pixel value is determined by a non-linear operation that considers the values in the neighborhood.
2.  **Operation:** The operation varies by filter type (e.g., sorting pixels and picking the median, or calculating photometric and geometric similarity).
3.  **Common Examples:**
    *   **Median Filter:** A very common non-linear filter. It works by sorting all the pixel values in the neighborhood and replacing the central pixel with the **median** (the middle value) of that sorted list. This is exceptionally good at removing **salt-and-pepper noise** without blurring edges as much as a linear filter [`./documents/Lec-3B-Non-Linear Filters.pdf`].
    *   **Bilateral Filter:** A more advanced filter that smooths images while **preserving edges**. It does this by considering both the geometric closeness of pixels and their photometric similarity (how similar their intensities/colors are) [`./documents/Lab 3.pdf`].
    *   **Anisotropic Diffusion:** A technique that reduces image noise without removing significant parts of the image content, typically edges or lines. It is an iterative process that applies a diffusion process which discourages smoothing across edges [`./documents/Lab 3.pdf`].
4.  **Effect on Images:** They tend to **preseve or even enhance sharp image changes** like edges and fine details while effectively removing noise [`./documents/Lec-3B-Non-Linear Filters.pdf`].

---

### Summary Table

| Feature | Linear Filtering | Non-Linear Filtering |
| :--- | :--- | :--- |
| **Core Operation** | Convolution (Weighted Sum) | Non-linear operation (e.g., sort, median) |
| **Mathematical Property** | Linear, follows superposition | Non-linear |
| **Effect on Edges** | Blurs and smoothes edges | Preserves or enhances edges |
| **Common Use Cases** | General blurring, reducing Gaussian noise | Removing salt-and-pepper noise, edge-preserving smoothing |
| **Example Filters** | Mean, Gaussian | Median, Bilateral, Anisotropic Diffusion |

**In short:** Use a **linear** filter (like Gaussian) when you want a general, uniform smoothness. Use a **non-linear** filter (like Median or Bilateral) when you need to remove noise without sacrificing the sharpness of important features like edges and details.

I hope this clarifies the difference! If you have any more questions about a specific filter, feel free to ask.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Linear filters, like mean and Gaussian filters, compute a linear function of a set of pixel values. Non-linear filters, such as the median filter, do not follow this linear relationship and tend to preserve or even enhance sharp image changes.","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.● Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.● Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 – 0.1), K (0 – 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.● Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 – 100) and sigma (50 – 250). You may keep both sigma values the same for simplicity’s sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  • Median filtering  • Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‘esau ad oe € THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i• A false saturation gives a white spot in the image (salt)• A failed response gives a black spot in the image (pepper)• Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.● Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.● Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 – 0.1), K (0 – 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.● Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 – 100) and sigma (50 – 250). You may keep both sigma values the same for simplicity’s sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  • Median filtering  • Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‘esau ad oe € THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i• A false saturation gives a white spot in the image (salt)• A failed response gives a black spot in the image (pepper)• Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the",
Explain the concept of superpixels.,"Of course! I'd be happy to explain the concept of superpixels.

### What are Superpixels?

In simple terms, **superpixels** are groups of pixels that share similar characteristics, such as color or texture. Instead of processing an image at the individual pixel level (which can be millions of tiny elements), superpixels allow you to work with a smaller number of more meaningful, perceptually consistent regions.

Think of it as a way to **oversegment** an image. You break the image down into small, coherent patches that adhere to image boundaries (like the edges of an object), but you don't necessarily segment the image into complete objects. This creates a kind of ""patchwork"" or ""puzzle"" of the original image.

---

### Key Characteristics of Superpixels

A ""good"" superpixel algorithm aims to produce regions that are:

1.  **Compact and Nearly Uniform:** The superpixels should be roughly similar in size and shape, making them easier to work with computationally.
2.  **Adherent to Image Boundaries:** The edges of the superpixels should line up as closely as possible with the actual edges in the image (e.g., the boundary between a person and the background).
3.  **Computationally Efficient:** The algorithm to generate them should be fast and not require excessive memory, even for large images.

---

### Why are Superpixels Useful?

Working with superpixels instead of raw pixels provides major advantages for many computer vision tasks:

*   **Reduces Complexity:** They drastically reduce the number of elements an algorithm has to process. An image with millions of pixels might be represented by only a few thousand superpixels. This is a form of **dimensionality reduction**.
*   **Provides a Meaningful Representation:** Superpixels carry more information than a single pixel. They can represent a small, homogeneous region, which is often more useful for tasks like object recognition or image segmentation.
*   **Improves Performance:** By providing a better foundation for processing, using superpixels has been shown to increase performance and efficiency in applications like:
    *   Object category recognition
    *   Depth estimation
    *   Medical image segmentation
    *   Body model estimation

As noted in the source material, superpixels are ""becoming increasingly popular for use in computer vision applications"" because they offer a better representation for subsequent processing[^1].

---

### How are They Created?

There are different algorithms for generating superpixels. The retrieved context mentions two main types:

1.  **Graph-based algorithms:** These treat pixels as nodes in a graph and use techniques to group them. An example is the method by Felzenszwalb and Huttenlocher (cited as GS04), which performs an ""agglomerative clustering of pixel nodes on a graph""[^1].
2.  **Gradient-ascent-based algorithms:** These start with an initial rough clustering and iteratively refine it. The cited **SLIC (Simple Linear Iterative Clustering)** algorithm is a famous and highly efficient example of this type. It is praised for its simplicity and low computational overhead, needing ""only the number of desired superpixels as the input parameter""[^1].

### Simple Analogy

Imagine a complex, detailed mosaic made of thousands of tiny, colored tiles (pixels). Superpixels would be like grouping these tiny tiles into larger, cohesive ""stones"" based on their color and pattern. You haven't assembled the final picture yet, but you now have a much smaller set of meaningful blocks to work with, making the final assembly much easier.

I hope this explanation helps! If you have any more questions about specific algorithms or applications, feel free to ask.

[^1]: Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk, *SLIC Superpixels*, École Polytechnique Fédérale de Lausanne (EPFL). ./documents/SLIC_Superpixels.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","Superpixels are groups of pixels that are similar in color and proximity. They provide a convenient primitive from which to compute local image features, capturing redundancy in the image and reducing the complexity of subsequent image processing tasks.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oﬀer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eﬃcacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eﬃciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiﬁcation model for segmentation. ICCV (2003) 10–172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oﬀer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by ﬁnding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oﬀer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eﬃcacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eﬃciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiﬁcation model for segmentation. ICCV (2003) 10–172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oﬀer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by ﬁnding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient",
What is the SLIC algorithm for generating superpixels?,"Based on the provided context from the lecture materials, here's a clear explanation of the SLIC algorithm for generating superpixels:

### SLIC Algorithm Overview

**SLIC (Simple Linear Iterative Clustering)** is a superpixel generation algorithm that clusters pixels in a combined five-dimensional color and image plane space. It was developed by researchers at EPFL and is designed to efficiently produce compact, nearly uniform superpixels.

### Key Characteristics:

1. **Input Parameter**: The algorithm is extremely easy to use - a single parameter specifies the desired number of superpixels.

2. **Clustering Space**: SLIC operates in a 5D space combining:
   - Color information (typically in CIELAB color space)
   - Spatial coordinates (x,y position in the image)

3. **Efficiency**: The algorithm has low computational overhead and is very practical for real-world applications. The technical report indicates it has O(N) complexity, making it efficient even for large images.

4. **Comparison Performance**: Experiments show SLIC produces superpixels at lower computational cost while achieving segmentation quality equal to or greater than four state-of-the-art methods (GS04, NC05, TP09, and QS09), as measured by boundary recall and under-segmentation error metrics.

5. **Specific Design**: Unlike general segmentation schemes, SLIC is specifically tailored for superpixel clustering using its unique distance measure and is much simpler to implement than many alternatives.

The algorithm works by iteratively clustering pixels based on both color similarity and spatial proximity, resulting in regular, compact superpixels that are useful for various computer vision applications.

**Source**: SLIC Superpixels technical report (EPFL) - './documents/SLIC_Superpixels.pdf'

For more detailed implementation specifics or mathematical formulation, I'd recommend consulting the full technical paper or contacting Dr. Tissa Chandesa during his office hours.

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","SLIC (Simple Linear Iterative Clustering) is an algorithm that clusters pixels in a combined five-dimensional color and image plane space to generate compact, nearly uniform superpixels. It is known for its efficiency and simplicity, with a single parameter specifying the number of superpixels.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciﬁc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oﬀer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481×321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciﬁc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oﬀer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481×321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required",
What is the purpose of the activation function in a CNN?,"Of course! I'd be happy to explain the purpose of an activation function in a Convolutional Neural Network (CNN).

Based on the provided lecture materials, the activation function is a crucial component of a CNN. Its primary purposes are:

1.  **Introducing Non-Linearity:** This is its most important job. The operations in a convolutional layer (filtering, dot product) and fully connected layers are fundamentally linear. If we only had linear functions, the entire network would be equivalent to a single linear layer, drastically limiting its ability to learn complex, real-world patterns. The activation function adds non-linearity, allowing the network to learn and model complex, non-linear relationships in the data, such as the curves and edges that make up an image.

2.  **Determining Neuron Output:** It decides whether a neuron should be ""activated"" or not. It takes the weighted sum of inputs from the previous layer and applies a mathematical function to determine the output signal that will be passed to the next layer.

To put it simply: **Without an activation function, a CNN would be a simple linear regression model, incapable of performing complex tasks like image recognition.**

Common examples of activation functions you will encounter include ReLU (Rectified Linear Unit), Sigmoid, and Tanh. The choice of which one to use depends on the specific layer and task.

**Source:**
This explanation is based on the structure of CNNs as outlined in the lecture slides `./documents/Lec-9-CNN.pdf`, which lists the ""Activation Function"" as a core component.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","The activation function, also known as the transfer function, determines the output of a neural network. It introduces non-linearity into the model, allowing it to learn more complex patterns. Common activation functions include ReLU, Sigmoid, and Softmax.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between overfitting and underfitting in the context of CNNs?,"Based on the retrieved context, here's the clear difference between overfitting and underfitting in CNNs:

**Underfitting:** This occurs when the model is **not learning enough**. The model fails to capture the underlying patterns in the training data, resulting in poor performance on both the training data and new, unseen data. It's essentially too simple for the complexity of the task.

**Overfitting:** This occurs when the model is **learning too much**, including the noise and random fluctuations in the training data. The model performs exceptionally well on the training data but fails to generalize to new data. It has essentially memorized the training set instead of learning to generalize from it.

The correct answer from your context is: **B: Underfitting: model is not learning enough; Overfitting: model is learning too much**

*Source: Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74*

The infographic from Keita, Z. (Nov 2023) also shows strategies to mitigate overfitting in CNNs, including techniques like dropout, batch normalization, and data augmentation.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on new, unseen data. Underfitting happens when the model is not complex enough to capture the underlying patterns in the data and performs poorly on both training and test data.","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What are some common CNN architectures?,"Based on the provided context, I can identify some common CNN architectures that are mentioned:

From the retrieved context, the following CNN architectures are referenced:
1. **R-CNN** (Region-based Convolutional Neural Network)
2. **Fast R-CNN**
3. **Faster R-CNN**

The context specifically states that all of these (R-CNN, Fast R-CNN, and Faster R-CNN) are indeed CNN architectures. These are particularly known for object detection tasks.

However, I should note that the provided context is limited and doesn't cover other well-known CNN architectures like AlexNet, VGG, ResNet, or Inception, which are also fundamental in the field of deep learning for image processing.

For a more comprehensive list and detailed explanations of CNN architectures, I would recommend:
- Consulting your lecture materials on CNN architectures
- Reaching out to Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) during office hours
- Contacting the teaching assistant Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my)

Would you like me to explain any specific aspects of the R-CNN family architectures mentioned in the context?


For more detailed information about specific CNN architectures, I recommend contacting Dr. Chandesa directly:

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about CNN Architectures - Introduction to Image Processing&body=Dear Dr. Chandesa,

I am a student in your Introduction to Image Processing module and I have a question about common CNN architectures. Could you please provide guidance on:

1. Which CNN architectures are covered in our course?
2. Where I can find detailed information about their structures and applications?
3. Any recommended resources for studying different CNN architectures?

Thank you for your assistance.

Best regards,
[Your Name]
[Your Student ID]"">Email Dr. Chandesa</a>","Some well-known CNN architectures include LeNet, AlexNet, VGG, ResNet, GoogLeNet, and MobileNet. There are also region-based CNNs (R-CNN, Fast R-CNN, Faster R-CNN) used for object detection.","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:",
What is the watershed algorithm for segmentation?,"Based on the provided lecture materials, the **watershed algorithm** is a segmentation method that falls under the category of **edge-based** approaches. It emphasizes finding boundaries between regions by performing **gradient ascent from local minima** in the image plane. Here's a breakdown of how it works:

### Key Concept:
The watershed algorithm treats the image as a topographic surface, where pixel intensities represent elevations. The goal is to find ""watersheds""—lines that separate ""catchment basins"" (regions). These basins are formed by starting from local minima (low-intensity points) and ""flooding"" the image outward until the boundaries between regions meet.

### Steps Involved (as illustrated in the practical exercise):
1. **Sort pixels by intensity** (and coordinates, if needed for ordering).
2. For each pixel (processed in order):
   - If all its neighbors are unlabeled, assign it a **new label** (i.e., start a new region).
   - If it has labeled neighbors, it may be assigned to an existing region or form part of a boundary.

### Example from the Exercise:
In the provided image fragment:
```
3  4  2
2  2  3
3  1  1
2  3  2
11 2  3
```
The solution involves labeling regions with letters (e.g., starting from minima like intensity 1) and growing regions based on adjacency and intensity similarity.

### Why Use Watershed?
- It is effective for separating objects that are touching or overlapping, especially in images with clear intensity gradients.
- A fast version based on **queuing of pixels** was proposed by Vincent and Soille [17], making it computationally efficient.

### Comparison to Other Methods:
Unlike clustering-based methods (e.g., SLIC, which groups pixels in color and spatial space), watershed is **region-based** and focuses on physical connectivity and boundaries.

For more details, you can refer to the practical exercises on segmentation (Source: `Practicals with Solutions - Segmentation.pdf`). If you have further questions or need clarification, feel free to ask! 😊

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","The watershed algorithm is a region-based segmentation method. It treats the gradient magnitude image as a topographic surface, flooding basins from local minima until watershed lines (boundaries) are formed.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eﬃcient level-set based geometric ﬂow. The geometric ﬂow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the ﬁve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are – views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I°v0 “ACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -°boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eﬃcient level-set based geometric ﬂow. The geometric ﬂow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the ﬁve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are – views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I°v0 “ACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -°boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!",
What is the purpose of a structuring element in morphological operations?,"Based on the lecture materials, the purpose of a structuring element in morphological operations is to serve as a binary mask or template that defines the shape and size of the neighborhood to be considered when processing a binary image. 

Specifically:
- Structuring elements are similar to filter masks used in spatial filtering, but they identify pixels rather than weight them.[^1]
- They combine with the image using set-theoretic operations to perform tasks like dilation (expanding the foreground) or erosion (shrinking the foreground).[^2]
- The shape and size of the structuring element determine the effect of the morphological operation. For example, larger structuring elements produce more extreme effects, and different shapes (e.g., elliptic, rectangular, cross-shaped) affect features like corners and holes differently.[^3]

In essence, the structuring element acts as a ""probe"" that interacts with the image to extract or modify geometric structures based on its design.

[^1]: Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. (Lab 5.pdf)
[^2]: Mathematical Morphology uses set theoretic operations to process images, combining the image set with a structuring element set. (Lec-5-Morphology.pdf)
[^3]: Compare differences when using different shapes and sizes of structuring elements. Notice effects on holes and borders (particularly corners). (Lab 5.pdf)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

",A structuring element is a small kernel used in morphological operations like erosion and dilation. It defines the shape of the neighborhood of a pixel that is considered when applying the operation.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony",
What is the difference between opening and closing in morphological image processing?,"Based on the retrieved context from the lecture materials, here's the difference between opening and closing in morphological image processing:

**Opening** is an erosion operation followed by a dilation operation. It's useful for removing small objects or thin lines from an image while preserving the shape and size of larger objects. Opening tends to break narrow connections between objects and eliminate small protrusions.

**Closing** is a dilation operation followed by an erosion operation. It's useful for filling small holes or gaps in objects while preserving the shape and size of the overall structure. Closing tends to connect nearby objects and smooth contours by filling small concavities.

The key distinction is:
- **Opening** (erode then dilate) removes small white objects/features
- **Closing** (dilate then erode) fills small black holes/gaps

Both operations use a structuring element, and the size of this element determines what constitutes ""small"" features that will be affected by the operation.

Source: Practical Exercises Morphology materials from the course.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

","Opening is an erosion followed by a dilation, which can remove small objects and smooth object contours. Closing is a dilation followed by an erosion, which can fill small holes and connect nearby objects.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left",
How can color histograms be used for image retrieval?,"Based on the retrieved context, color histograms are an excellent representation for image retrieval and object identification. Here's how they can be used:

## Color Histograms for Image Retrieval

**1. Histogram Representation:** A color histogram counts how much of each color occurs in an image by dividing the color space into discrete bins. This creates a statistical representation of the color distribution in an image.

**2. Key Properties:** Color histograms possess several advantageous properties for retrieval:
- **Invariant to translation and rotation** about an axis perpendicular to the image
- **Change only slowly** with rotation about other axes, occlusion, and changes in distance to the object
- **Distinctive** - histograms for different objects can differ markedly, making them excellent for identification

**3. Histogram Intersection Method:** This technique compares image and model histograms by calculating their intersection. It's particularly suited for recognition because it doesn't require accurate separation of objects from their background or handling occluding objects[^1].

**4. Database Indexing:** The method can efficiently index into large databases, eliminating most possible matches and leaving only a small number for further consideration[^1]. A modified version called Incremental Intersection allows efficient indexing into very large databases.

**5. Histogram Backprojection:** This algorithm uses model histograms to find the location of known objects in images by identifying regions where the colors in the model histogram appear[^1].

**6. Practical Considerations:** 
- The technique works with various color spaces (opponent color axes are mentioned)
- Only a small number of histograms are needed to represent 3D objects
- Color constancy algorithms may be needed for variable lighting conditions

The retrieved research shows that histogram intersection can distinguish models from large databases and is robust to occlusion as well as image and histogram resolution variations[^1].

[^1]: Source: Swain1.pdf - Research on color indexing and histogram-based recognition methods

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

","Color histograms can be used to represent the color distribution of an image. By comparing the color histogram of a query image with the histograms of images in a database using a similarity measure like histogram intersection, similar images can be retrieved.","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is",
What is histogram intersection?,"Based on the provided lecture materials, **histogram intersection** is a method used to compare two histograms (typically an image histogram and a model histogram) to measure how much they overlap. It is particularly useful for object recognition in images.

Here's a clear breakdown of what it is and how it works:

### Definition
Given two histograms, **J** (often representing an image) and **M** (often representing a model), each with `n` bins, the histogram intersection is defined as:

\[
HI(J, M) = \sum_{i=1}^{n} \min(J_i, M_i)
\]

### What It Measures
*   It measures the **number of pixels from the model that have corresponding pixels of the same color in the image**.
*   Essentially, it answers the question: **""How much do the two representations overlap?""**

### Normalization
To get a match value between 0 and 1 (where 1 is a perfect match), the result is often normalized by the total number of pixels in the model histogram:

\[
H(J, M) = \frac{\sum_{i=1}^{n} \min(J_i, M_i)}{\sum_{i=1}^{n} M_i}
\]

### Key Advantage
A major advantage highlighted in the context is that the normalized histogram intersection match value is **not reduced by distracting pixels in the background**. This is desirable because reliably segmenting an object from its background before recognition is often very difficult. The match value is only increased by a background pixel if that pixel has the same color as a color in the model *and* the number of pixels of that color in the object is less than the number in the background.

### Example from the Lecture
An example calculation is provided:
*   Histogram H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)
*   Histogram H2 = (0, 40, 0, 0, 0, 6, 0, 110, 0)
*   `HI(H1, H2) = min(10,0) + min(0,40) + min(0,0) + min(0,0) + min(100,0) + min(10,6) + min(30,0) + min(0,110) + min(0,0) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0 + 0 = 6`

### Practical Use
According to the source material, this technique was shown to be very effective for object recognition, achieving high recognition rates even when images varied in view, resolution, or were partially occluded.

**Source:** The description and formula are derived from the provided lecture notes and the paper by Swain and Ballard (1991) referenced in `./documents/Lec-2-Histogram (Whole Image Understanding).pdf` and `./documents/Swain1.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram intersection is a method for comparing two histograms. It calculates the sum of the minimum values for each corresponding bin in the two histograms. This measure is robust to occlusion and changes in image resolution.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‘(and ‘-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  å  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R – G - BY = 2 * B – R – G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly— distractions in the background of the object,— viewing the object from a variety of viewpoints,— occlusion,— varying image resolution,— varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‘(and ‘-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  å  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R – G - BY = 2 * B – R – G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly— distractions in the background of the object,— viewing the object from a variety of viewpoints,— occlusion,— varying image resolution,— varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to",
What are the different types of redundancy in image compression?,"Based on the provided lecture materials, there are three main types of redundancy in image compression:

1.  **Coding Redundancy**: This occurs when the code (e.g., the number of bits) used to represent image data is not optimal. For example, using a fixed-length code like a simple 8-bit binary number for every pixel value, even though some values occur more frequently than others, is inefficient. The average number of bits per pixel can be reduced by using variable-length codes (like Huffman coding), where more frequent values are assigned shorter codes.

2.  **Spatial Redundancy**: This refers to the correlation between neighbouring pixels in an image. Adjacent pixels often have very similar values. Compression techniques can exploit this by predicting a pixel's value based on its neighbours and then only storing the difference (the prediction error), which typically requires fewer bits.

3.  **Psychovisual Redundancy**: This is redundancy that exists relative to the human visual system (HVS). The HVS is more sensitive to certain types of information (like changes in luminance) and less sensitive to others (like fine colour details or high-frequency noise). A compression system can remove this psychologically redundant information, resulting in a loss of data that is, in theory, not perceptible to a human viewer.

**Source:** `./documents/Lec-10-Image Compression.pdf` (Slides 4-5) and `./documents/Practicals - Image Compression.pdf` (Slide 1)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

","There are three main types of redundancy in image compression: coding redundancy, spatial redundancy, and psychovisual redundancy.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=— =0,2,...,L—1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1å() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=— =0,2,...,L—1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1å() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1",
What is Huffman coding?,"Of course! Huffman coding is a fundamental concept in image compression. Here's a clear and concise explanation.

### What is Huffman Coding?

Huffman coding is a **lossless data compression algorithm**. It is used to reduce the size of data, such as an image, by assigning shorter binary codes to more frequent symbols (like pixel values) and longer codes to less frequent ones.

#### How it Works: The Core Idea

1.  **Frequency Analysis:** The algorithm starts by analyzing the data and calculating the frequency (or probability) of each symbol. In an image, a ""symbol"" is often a specific pixel intensity value.
2.  **Building a Huffman Tree:** It builds a binary tree from the bottom up:
    *   Each symbol becomes a leaf node, weighted by its frequency.
    *   The two nodes with the *smallest* frequencies are combined to create a new parent node. The weight of this new node is the sum of the frequencies of its two children.
    *   This process repeats, combining the two smallest available nodes (whether they are original symbols or combined nodes) until all nodes are merged into a single root node. This final structure is the **Huffman Tree**.
3.  **Assigning Codes:** To generate the code for each symbol, you traverse the tree from the root to the leaf node containing that symbol. For every left branch you take, you add a `0` to the code; for every right branch, you add a `1`.

The key principle is that the most frequent symbols will be located closer to the root of the tree, resulting in very short codes (e.g., `01`), while the least frequent symbols will be deeper in the tree and have longer codes (e.g., `11010`).

#### Example from Your Course Material

Your practical exercise provides a perfect example <sup>[Source: Practicals - Image Compression.pdf]</sup>:

An image has a normalized histogram (a list of pixel values and their probabilities):

| Pixel Value | Normalised Frequency |
| :---------- | :------------------- |
| 0           | 0.1                  |
| 1           | 0.1                  |
| 2           | 0.15                 |
| 3           | 0.35                 |
| 4           | 0.2                  |
| 5           | 0                    |
| 6           | 0.05                 |
| 7           | 0.05                 |

Following the Huffman algorithm, you would sort these by frequency and iteratively combine the two smallest probabilities. The solution (from your provided materials) shows the final codes would be something like:

*   Pixel 3 (most frequent, 0.35): Likely a very short code like `0`
*   Pixels 6 & 7 (least frequent, 0.05 each): Longer codes like `1110` and `1111`

This ensures that the overall number of bits needed to represent the image is minimized.

#### Why is it Important in Image Processing?

*   **Reduces Coding Redundancy:** It efficiently removes one of the three main types of redundancy in images, directly reducing file size.
*   **Lossless:** No information is lost during compression and decompression. The decoded image is identical to the original.
*   **Foundation for Other Methods:** It's a key component in widespread compression standards like JPEG (though JPEG also uses other techniques to handle different types of redundancy).

If you'd like to walk through the steps of building the Huffman tree for the exercise above, I'd be happy to guide you through it!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Image Compression.pdf"" target=""_blank"">Practicals with Solutions   Image Compression</a>

","Huffman coding is a lossless data compression algorithm. It assigns variable-length codes to input characters, with the lengths of the assigned codes being based on the frequencies of corresponding characters. More frequent characters get shorter codes.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
How does JPEG compression work?,"Based on the provided lecture materials, JPEG compression works by exploiting different types of redundancy in images and applying a series of specific processing steps. Here's how it works:

### How JPEG Compression Works

JPEG compression follows a systematic approach to reduce file size while attempting to maintain acceptable image quality:

1. **Exploits Redundancy**: JPEG targets three types of redundancy:
   - **Coding redundancy**: Uses more efficient coding schemes
   - **Spatial redundancy**: Exploits correlations between neighboring pixels
   - **Psychovisual redundancy**: Removes information that human vision is less sensitive to

2. **Key Processing Steps**:
   - **Discrete Cosine Transform (DCT)**: The image is divided into 8×8 blocks, and each block undergoes DCT transformation to convert spatial information into frequency information
   - **Quantization**: The DCT coefficients are quantized (reduced in precision), with higher frequency components typically receiving more aggressive quantization since human vision is less sensitive to them
   - **Zigzag Coding**: The quantized DCT coefficients are ordered in a zigzag pattern from low to high frequency, which produces long sequences of zeros
   - **Run-length Encoding**: These zero sequences are efficiently encoded using run-length encoding
   - **Huffman Coding**: Finally, the data undergoes Huffman coding for additional compression

3. **Quality vs. Compression Trade-off**: 
   - Increasing quantization reduces file size but introduces artifacts (blocking becomes visible)
   - The amount of compression can be controlled to balance file size and image quality

Source: './documents/Lec-10-Image Compression.pdf'

The JPEG standard represents a complete compression system that effectively combines these techniques to achieve significant file size reduction while maintaining reasonable image quality for most practical applications.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

<a href=""http://localhost:5003/documents/Lec-11-Finale and Revision.pdf"" target=""_blank"">Lec 11 Finale and Revision</a>

","JPEG compression is a lossy compression method that involves dividing an image into blocks, applying a Discrete Cosine Transform (DCT) to each block, quantizing the DCT coefficients, and then using Huffman coding to compress the quantized coefficients.","Compression—26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 —3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.—.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -— . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf” te . = om «File size: } 49K es 100 dpi high JPEG compression ~ — ye ‘ ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compression🤔48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well…the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE…CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing – theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f’(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = æ ç ç è 1 MN M - 1 N - 1 åå x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 ö ÷ ÷ ø 2/1 SNR = M - 1 N - 1 åå ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - åå ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" 𝑇 𝑟 = ( 𝑝"" 𝑤 𝑑𝑤 !𝑑𝑠 𝑑𝑟 = 𝑑𝑇(𝑟) 𝑑𝑟 = 𝑑 𝑑𝑟 "" ( ! 𝑝"" 𝑤 𝑑𝑤 = 𝑝""(𝑟)𝑝# 𝑠 = 𝑝"" 𝑟 𝑑𝑟 𝑑𝑠 = 𝑝"" 𝑟 1 𝑝"" 𝑟 = 1 0 ≤ 𝑠 ≤ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram","Compression—26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 —3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.—.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -— . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf” te . = om «File size: } 49K es 100 dpi high JPEG compression ~ — ye ‘ ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compression🤔48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well…the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE…CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing – theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f’(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = æ ç ç è 1 MN M - 1 N - 1 åå x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 ö ÷ ÷ ø 2/1 SNR = M - 1 N - 1 åå ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - åå ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" 𝑇 𝑟 = ( 𝑝"" 𝑤 𝑑𝑤 !𝑑𝑠 𝑑𝑟 = 𝑑𝑇(𝑟) 𝑑𝑟 = 𝑑 𝑑𝑟 "" ( ! 𝑝"" 𝑤 𝑑𝑤 = 𝑝""(𝑟)𝑝# 𝑠 = 𝑝"" 𝑟 𝑑𝑟 𝑑𝑠 = 𝑝"" 𝑟 1 𝑝"" 𝑟 = 1 0 ≤ 𝑠 ≤ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram",
What is the difference between RGB and HSV color spaces?,"Based on the lecture materials, here are the key differences between RGB and HSV color spaces:

## RGB (Red, Green, Blue)
- **Additive color model** - Colors are created by mixing red, green, and blue light components together
- Represents colors based on light components rather than human perception
- Each pixel has three values (R, G, B) ranging from 0-255

## HSV (Hue, Saturation, Value)
- **Perceptually-based color model** that separates color information from intensity
- Three components:
  - **Hue**: The actual color (angle around the color wheel)
  - **Saturation**: How strong/pure the color is (color intensity)
  - **Value**: How bright or dark the color is (brightness/lightness)

## Key Differences
1. **Color vs Light**: HSV is based on color perception, while RGB is based on light mixing
2. **Illumination Sensitivity**: HSV separates color from intensity, making it **less sensitive to illumination changes** than RGB
3. **Application**: HSV is particularly useful when you want to work with specific color characteristics (like when working with plants, you might use just green or ""greenness"": G - (R+B)/2)

The lecture notes show that HSV provides a more intuitive way to work with color in image processing applications where illumination conditions may vary or where specific color properties need to be isolated.

Source: Lec-1B-Digital Images and Point Processes.pdf (Slides 22-25)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","RGB (Red, Green, Blue) is an additive color model based on light, commonly used in displays. HSV (Hue, Saturation, Value) is a color model based on human perception of color. Hue represents the color, saturation represents the intensity of the color, and value represents the brightness. HSV separates color information from intensity, making it less sensitive to illumination changes.","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie• RGB is OK, but there are other general &application-specific colour spaces…•If you want to work with plants, you might use|just G, or ‘greenness’:mGG – (R+B)/2HSV is based on colour rather   than light  • Hue - what general colour is it  • Saturation – how strongly coloured is it  • Value – how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject’s surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie• RGB is OK, but there are other general &application-specific colour spaces…•If you want to work with plants, you might use|just G, or ‘greenness’:mGG – (R+B)/2HSV is based on colour rather   than light  • Hue - what general colour is it  • Saturation – how strongly coloured is it  • Value – how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject’s surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is gamma correction?,"Based on the provided lecture materials, **gamma correction** is a non-linear operation used to adjust the intensity values of an image to account for the way display hardware interprets input signals.

### Key Points About Gamma Correction:

1. **Purpose**: It compensates for the non-linear response of display devices (like monitors) where the displayed light intensity (L) is related to the input voltage (V) by a power function: L ≈ V^γ. The exact gamma value depends on the specific display device.

2. **The Operation**: 
   - It's applied using the formula: **O = (I / 255)^γ × 255** (from './documents/Lab 1B.pdf')
   - This maps input intensity values (I) to new output values (O)
   - γ (gamma) is the correction value that determines the shape of the mapping curve

3. **Effect of Different Gamma Values** (as shown in Figure 1 of './documents/Lab 1B.pdf'):
   - **γ < 1**: Mapping is weighted toward brighter output values
   - **γ = 1**: Linear mapping (no correction)
   - **γ > 1**: Mapping is weighted toward darker output values

4. **Practical Application**: By applying gamma correction to an image before display, we ensure that what we see on screen matches our intended intensity values. For example, if a display has γ ≈ 2.5, we would apply g(x,y) = f(x,y)^(1/2.5) to get the correct display result.

Gamma correction is a fundamental point process operation that works on each pixel independently and falls under the category of non-linear intensity transformations.

*Source: './documents/Lab 1B.pdf' and './documents/Lec-1B-Digital Images and Point Processes.pdf'*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",Gamma correction is a non-linear operation used to adjust the brightness and contrast of an image. It is often used to compensate for the non-linear response of display devices.,"a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾 is the gamma value.𝛾𝑂 = 𝐼 255 × 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if’ University of Nottingham —~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values 𝐼 to create 𝑂 (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:● Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.● Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.• When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform• You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think°.RAMP=25L ≈ V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction• We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each","a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾 is the gamma value.𝛾𝑂 = 𝐼 255 × 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if’ University of Nottingham —~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values 𝐼 to create 𝑂 (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:● Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.● Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.• When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform• You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think°.RAMP=25L ≈ V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction• We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each",
What is anisotropic diffusion?,"Based on the provided lecture materials, **anisotropic diffusion** is a non-linear filtering technique used in image processing for edge-preserving smoothing.

Here's a breakdown of the concept:

### Core Idea
Anisotropic diffusion makes each pixel in an image more like those neighbors that it is **already similar to**. This is in contrast to standard diffusion processes like mean or Gaussian filters, which make each pixel more like *all* of its neighbors indiscriminately, often blurring edges in the process.

### How It Works
The process uses a **similarity function**, s(p,q), which has values ranging from 0 to 1:
*   If pixels `p` and `q` are similar, `s(p,q)` is close to **1**.
*   If pixels `p` and `q` are different, `s(p,q)` is close to **0**.

A new value for a pixel `p` is computed as a weighted average of its neighbors `q`, where the weights are determined by this similarity function:

`p' = [ ∑ (I_q * s(p,q)) ] / [ ∑ s(p,q) ]`

(Source: `./documents/Lec-3B-Non-Linear Filters.pdf`)

### Key Property: ""Anisotropic""
The term ""anisotropic"" means ""not the same in all directions."" This describes the filter's behavior perfectly, as the amount of smoothing applied depends on the direction and the similarity of neighboring pixels, preventing smoothing across edges where pixel values change significantly.

### Relationship to Other Filters
*   If the similarity function `s(p,q)` is always 1, the operation simplifies to a **mean filter**.
*   **Bilateral filtering** is a closely related technique that modifies Gaussian smoothing in a similar way, using one Gaussian to weight pixels based on spatial proximity and another to weight them based on intensity similarity.

In summary, anisotropic diffusion is a powerful technique for reducing noise in an image while preserving and even enhancing important structural features like edges.

**Source:** The explanation is derived from concepts found in `./documents/Lec-3B-Non-Linear Filters.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Anisotropic diffusion is an edge-preserving smoothing technique. Unlike isotropic filters like Gaussian blur that smooth uniformly in all directions, anisotropic diffusion encourages smoothing within regions of similar intensity while inhibiting it across strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is bilateral filtering?,"Of course! Bilateral filtering is a sophisticated and powerful non-linear filtering technique used in image processing to smooth an image while preserving its edges.

In simple terms, it's often described as a ""smart"" blur. While a standard filter like a Gaussian blur will average all pixels in a neighborhood, a bilateral filter is selective. It averages pixels based on two criteria:

1.  **Spatial Closeness (like a Gaussian Blur):** It gives more weight to pixels that are physically close to the center pixel of the filter window.
2.  **Intensity Similarity (the ""smart"" part):** It also gives more weight to pixels that have a similar intensity or color value to the center pixel.

This second criterion is the key. A pixel on the other side of a sharp edge (e.g., a bright sky next to a dark roof) will have a very different intensity. The bilateral filter will assign this dissimilar pixel a low weight, preventing it from being included in the average. This is how the edge is preserved while the flat, noisy regions within the roof or the sky are smoothed.

### How It Works (The Formula)

The operation for a bilateral filter is defined by this equation (from your lecture notes):

$BF[I]_p = \frac{1}{W_p} \sum_{q \in S} G_{\sigma_s}(||p - q||) G_{\sigma_r}(|I_p - I_q|) I_q$

Let's break this down:
*   `BF[I]_p` is the new, filtered intensity value at pixel `p`.
*   `S` is the set of all pixels in the neighborhood around `p` (e.g., a 5x5 window).
*   `q` is a neighboring pixel within `S`.
*   `G_σs(||p - q||)` is the **spatial Gaussian weight**. It decreases as the physical distance between pixels `p` and `q` increases.
*   `G_σr(|Ip - Iq|)` is the **range Gaussian weight**. It decreases as the difference in intensity between pixels `p` and `q` increases.
*   `W_p` is a normalization factor, the sum of all the weights for the neighborhood, to ensure the output brightness is consistent.

**A key point from your materials:** If the range/similarity function were always 1, the equation would simplify into a standard Gaussian mean filter. The range weight is what makes it a bilateral filter.

### Visual Example

Imagine a 1D image—a simple plot of intensity values. A standard Gaussian filter would smooth the entire plot, blurring the sharp step (the edge). A bilateral filter would smooth the flat areas on either side of the step but would leave the sharp step itself largely unchanged because the pixels across the step are too dissimilar to be averaged together.

### Summary of Advantages:
*   **Effective noise reduction** in textured or smooth regions.
*   **Excellent edge preservation**, preventing the ""bleeding"" of colors across boundaries.
*   **Non-iterative**, meaning it often produces good results in a single pass.

### Summary of Disadvantages:
*   **Computationally more expensive** than simple linear filters because it must calculate weights for each pixel in the neighborhood based on both space and intensity.
*   Requires choosing two parameters (`σ_s` for space and `σ_r` for range) to get the desired effect.

**Source:** This explanation is based on the concepts covered in the lecture slides `./documents/Lec-3B-Non-Linear Filters.pdf` and `./documents/Practicals - Linear and Non-Linear Filters.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

<a href=""http://localhost:5003/documents/Practicals - Linear and Non-Linear Filters.pdf"" target=""_blank"">Practicals   Linear and Non Linear Filters</a>

","Bilateral filtering is a non-linear, edge-preserving smoothing filter. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels. The weights depend not only on the spatial distance of pixels but also on the difference in their intensity values.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding…Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x—)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ° Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding…Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x—)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ° Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3",
What is the Canny edge detector?,"Based on the provided lecture materials, the **Canny edge detector** is a sophisticated and widely used edge detection algorithm developed by John Canny. It is designed to be an optimal edge detector that addresses key criteria for finding edges in images, particularly in the presence of noise.

### Key Features of the Canny Edge Detector:
1. **Good Detection**: It aims to mark all the true edges while minimizing false positives (i.e., it should only mark edges).
2. **Good Localization**: The edges detected should be as close as possible to the actual edges in the image.
3. **Minimal Response**: Each edge should be reported only once (avoiding multiple responses to a single edge).

### How It Works:
The Canny operator involves multiple steps to achieve robust edge detection:
1. **Gaussian Smoothing**: First, the image is smoothed using a Gaussian filter to reduce noise. This helps in preventing spurious edges caused by noise.
2. **Gradient Calculation**: The gradient of the image is computed using Sobel filters (or similar operators) in both the x and y directions. This step identifies the intensity changes (edges) by calculating the magnitude and direction of gradients.
3. **Non-Maximum Suppression**: This step thins the edges by preserving only the local maxima in the gradient direction. It ensures that edges are represented by thin lines.
4. **Hysteresis Thresholding**: Two thresholds (a high threshold and a low threshold) are used to classify edges:
   - Pixels with gradient magnitudes above the high threshold are considered strong edges.
   - Pixels below the low threshold are discarded as non-edges.
   - Pixels between the two thresholds are considered weak edges and are only kept if they are connected to strong edges. This helps in linking broken edges and reducing false detections.

### Why It's Effective:
- The Canny detector is considered optimal because it approximates the first derivative of a Gaussian, which balances sensitivity to edges and robustness to noise.
- It outperforms simpler edge detectors (like Sobel or Laplacian) by producing cleaner, connected edges with better localization.

For more details, you can refer to **Lecture 6A: Derivative and Edges** (source: `./documents/Lec-6A-Derivative and Edges.pdf`). If you have further questions about implementing it or tuning parameters, feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","The Canny edge detector is a multi-stage edge detection algorithm. It involves noise reduction using a Gaussian filter, finding intensity gradients, non-maximum suppression to thin edges, and hysteresis thresholding to connect weak edges to strong ones.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian",
What is unsharp masking?,"Based on the lecture materials, **unsharp masking** is an image processing technique used for **edge enhancement** or **image sharpening**. Its purpose is to make edges in an image appear more obvious without significantly affecting the rest of the image.

The process involves four main steps, as detailed in the lecture slides from `'./documents/Lec-6A-Derivative and Edges.pdf'`:

1.  **Take the original image**
2.  **Gaussian smooth it** – This creates a blurred version of the original image.
3.  **Subtract the smoothed version from the original** – This difference creates what is called the ""unsharp mask.""
4.  **Add the mask to the original** – Adding this mask back to the original image enhances the edges, making them more noticeable.

The amount of sharpening is controlled by the standard deviation (σ) of the Gaussian filter used in step 2. A larger σ value results in a more heavily blurred image, which, when subtracted, creates a mask that enhances larger-scale edges and structures. A smaller σ value will enhance finer details and sharper edges.

It's important to note that while unsharp masking is effective, it can also enhance noise if too much sharpening is applied.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Unsharp masking is an image sharpening technique. It works by subtracting a blurred version of the image from the original image to create a mask of the image's high-frequency components, which is then added back to the original image to sharpen it.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -———__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -——__, I eee C ooo —_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask — En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make’ ° the edge appear more obvious -—™—_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  •  Even if they are noise  •  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f’Derivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I• Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKA σ = 1.5Bσ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -———__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -——__, I eee C ooo —_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask — En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make’ ° the edge appear more obvious -—™—_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  •  Even if they are noise  •  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f’Derivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I• Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKA σ = 1.5Bσ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is a structuring element in morphology?,"In mathematical morphology, a **structuring element** is a small binary mask or shape used to probe and interact with an input image. It is similar to a filter kernel in spatial filtering but is used to define the neighborhood structure for morphological operations rather than to perform weighting. 

### Key Characteristics:
1. **Shape**: It can have various shapes, such as rectangular, elliptical, cross-shaped, or even custom asymmetric forms.
2. **Size**: The size (e.g., 3×3, 5×5, 7×7) determines the extent of the morphological effect—larger sizes produce more pronounced changes.
3. **Origin**: The origin (reference point) of the structuring element does not necessarily have to be at the center; it can be positioned elsewhere, influencing asymmetric effects.
4. **Function**: It defines which pixels in the image are considered neighbors during operations like dilation or erosion. For example, in dilation, it expands the foreground based on the structuring element's pattern, while in erosion, it shrinks the foreground.

### Example:
- A 3×3 square structuring element might be used to uniformly expand or contract regions.
- An elliptical structuring element can produce smoother, more natural-shaped transformations.

Structuring elements are fundamental in operations such as dilation, erosion, opening, and closing, which are used for tasks like noise removal, gap filling, object separation, or edge detection in binary images.

*Source: ./documents/Lec-5-Morphology.pdf*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

","A structuring element is a small matrix of pixels, typically with a defined shape (e.g., square, disk, cross), used as a probe in morphological operations. It defines the neighborhood of a pixel to be processed.","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B• The boundaries between foreground and background are often smoothed in the process• The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I• Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B• The boundaries between foreground and background are often smoothed in the process• The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I• Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’",
How can you use a binary image as a mask?,"Of course! I can explain how to use a binary image as a mask.

Based on the provided context from your course materials, a binary image is used as a mask to **select which pixels an operation is applied to**.

Here’s a breakdown of the concept:

### What is a Mask?

A mask is typically a binary image (pixel values are either 0 or 255) where:
*   **White pixels (value 255)** define the **Region of Interest (ROI)**. These are the pixels you want to keep, process, or analyze.
*   **Black pixels (value 0)** define the background. These are the pixels you want to ignore or set to zero.

### How to Use It: The Basic Principle

The most common way to use a mask is through a pixel-wise logical `AND` operation. You apply the mask to your original (usually grayscale or color) image.

*   Where the mask is white (255), the original pixel value from your image is kept.
*   Where the mask is black (0), the pixel value in the output image is set to 0 (black).

This effectively ""cuts out"" the region of interest from the original image.

**Simple Example in Python with OpenCV:**
```python
import cv2
import numpy as np

# Load your original image and convert it to grayscale if it's color
original_image = cv2.imread('your_image.jpg', 0) # 0 for grayscale

# Create or obtain your binary mask (e.g., from thresholding)
# This is just an example creating a simple rectangular mask
height, width = original_image.shape
mask = np.zeros((height, width), dtype=np.uint8)
cv2.rectangle(mask, (100, 50), (300, 200), 255, -1) # Draw a white rectangle

# Apply the mask using bitwise_and
masked_image = cv2.bitwise_and(original_image, original_image, mask=mask)

# Now, 'masked_image' will show the original image only within the white rectangle of the mask. The rest will be black.
```

### Practical Applications from Your Course

The context mentions this is particularly useful for tasks like:
1.  **Selective Processing:** You can apply filters, adjustments, or other operations only to specific parts of an image defined by the mask, leaving the rest untouched.
2.  **Analysis:** After using **Connected Components** to label objects in a binary image, you can create a mask for each individual object (component) to analyze its properties (like area, centroid, etc.) in the original image.

*Source: Based on information from Practicals - Morphology.pdf*.

I hope this clears things up! Is there a specific application of masking you're working on for an assignment?

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

","A binary image can be used as a mask to apply an operation to a specific region of another image. Pixels in the binary mask with a value of 1 (or True) indicate the region where the operation should be applied, while pixels with a value of 0 (or False) are ignored.","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing’ a ume. For each subse€; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing’ a ume. For each subse€; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!",
What is the Nyquist rate?,"Based on the provided lecture materials, the **Nyquist rate** is defined as the minimum sampling rate required to accurately reconstruct a signal without aliasing. Specifically, it states that samples must be taken at a rate that is **twice the frequency of the highest frequency component** present in the signal (or image)[citation:1].

In simpler terms:
- If an image contains fine details (high frequencies), you need to sample it densely enough to capture those details.
- The Nyquist rate gives you that minimum sampling density: sample at least twice per the smallest detail you want to preserve.
- **Undersampling** (sampling below the Nyquist rate) leads to **aliasing**, where high-frequency components are misrepresented as lower frequencies, causing artefacts like moiré patterns or jagged edges in images[citation:1].

For example, if the highest frequency in your image is \( f \) cycles per unit distance, you should sample at a rate of at least \( 2f \) samples per unit distance to avoid aliasing.

If you'd like more details or examples, feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",The Nyquist rate is the minimum sampling rate required to avoid aliasing. It is twice the frequency of the highest frequency component in the signal (or image) being sampled.,"rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between a point process and a spatial filter?,"Based on the provided lecture materials, here's the difference between a point process and a spatial filter:

### Point Process
A point process is an operation where each target pixel value depends **only** on the corresponding source pixel and possibly some parameters. As shown in the lecture slides, this means that the transformation is applied independently to each pixel without considering its neighbors.

**Key characteristics:**
- Target pixel value = f(source pixel value, parameters)
- Examples include: intensity transforms, color space transformations
- Linear transforms like g(x,y) = a·f(x,y) + b (where a controls contrast and b controls brightness)
- Color space conversions (e.g., RGB to HSV)

### Spatial Filter
A spatial filter (also called neighborhood operation) processes each pixel based on **both** its own value and the values of its neighboring pixels. This involves operations that consider the spatial relationships between pixels.

**Key characteristics:**
- Target pixel value = f(source pixel + neighboring pixels)
- Examples include: mean filters, Gaussian filters, median filters
- Can be linear (like Gaussian smoothing) or non-linear (like median filtering or anisotropic diffusion)
- Used for operations like noise reduction, edge preservation, and image smoothing

### Key Difference
The fundamental difference is that **point processes operate on individual pixels independently**, while **spatial filters operate on pixel neighborhoods**, considering the spatial context around each pixel.

Source: Lec-1B-Digital Images and Point Processes.pdf and Lec-3B-Non-Linear Filters.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","A point process operates on each pixel independently, where the output pixel value depends only on the corresponding input pixel value. A spatial filter operates on a local neighborhood of pixels, where the output pixel value depends on the values of the input pixel and its neighbors.","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in’ the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‘action process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in’ the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‘tapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in’ the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‘action process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in’ the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‘tapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
How can you convert an RGB image to grayscale?,"Based on the lecture materials, there are two main methods to convert an RGB image to grayscale:

1. **Simple Averaging**: You calculate the grayscale intensity (I) by taking the average of the red (R), green (G), and blue (B) channel values for each pixel. The formula is:
   \[
   I = \frac{R + G + B}{3}
   \]

2. **Weighted Averaging (Luminosity Method)**: This method accounts for the human eye's different sensitivity to colors. The formula uses specific weights:
   \[
   I = 0.30R + 0.59G + 0.11B
   \]
   This is because our eyes are more sensitive to green light, so it contributes more to the perceived brightness.

You can implement these conversions in Python using loops for explicit control or, more efficiently, using NumPy's array operations. For example, after reading an image with OpenCV (which stores it as a BGR array by default), you can convert it to grayscale using `cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)`, which internally uses a weighted method similar to the one above[^1][^2].

Would you like a code example or more details on implementing these methods?

[^1]: Lecture Slides: Lec-1B-Digital Images and Point Processes.pdf - Discusses color to grayscale conversion methods.
[^2]: Lab Sheet: Lab 1B.pdf - Provides practical guidance on image processing operations in Python, including grayscale conversion.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lab 1B.pdf"" target=""_blank"">Lab 1B</a>

","An RGB image can be converted to grayscale by taking a weighted average of the red, green, and blue channels. A common formula is:
Grayscale = 0.30*R + 0.59*G + 0.11*B, which takes into account the human eye's different sensitivities to different colors.","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.● Display its red, green and blue components as three separate grey-level images.● Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)● Perform the above again but using NumPy’s array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)● Produce and display an image of the atrium showing the ‘greenness’ value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.● Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel • Makes processing easier • Reduces the amount of information • Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so…ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*• Most common starting point• Retinal cells are sensitive tothree primary colours R, G, B• Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.● Display its red, green and blue components as three separate grey-level images.● Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)● Perform the above again but using NumPy’s array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)● Produce and display an image of the atrium showing the ‘greenness’ value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.● Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel • Makes processing easier • Reduces the amount of information • Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so…ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*• Most common starting point• Retinal cells are sensitive tothree primary colours R, G, B• Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255",
What is the purpose of histogram backprojection in object tracking?,"Based on the provided context from the lecture materials, the purpose of **histogram backprojection** in object tracking is to **efficiently locate and track objects** in crowded scenes, even when the object is moving relative to the observer (e.g., a robot). 

Here's a breakdown of its key roles:

1. **Efficiency in real-time tracking**:  
   The algorithm is highly efficient, allowing it to be executed multiple times per second (e.g., four times a second for a 32x32 pixel convolution window). This makes it suitable for real-time applications like tracking moving objects.

2. **Robustness to occlusion**:  
   Histogram backprojection remains effective even when objects are partially occluded. For example, the context mentions that it can locate objects even when only four-ninths of the object remains visible, though performance may degrade for heavily occluded targets.

3. **Operation in crowded scenes**:  
   It works well in cluttered environments by leveraging color histograms to distinguish the target object from distractions. The ratio histogram backprojection step emphasizes colors unique to the target (e.g., a rare color like a specific blue hue gives a strong response, while common colors like white give weaker responses).

4. **Integration with convolution for localization**:  
   After backprojection, a convolution step is used to identify the most probable location of the object within the image. The darkest peak in the convolved image corresponds to the correct object location, enabling precise tracking.

In summary, histogram backprojection provides a fast, occlusion-resistant method for continuously locating and tracking objects based on their color distribution, making it ideal for dynamic vision systems.

**Source**:  
This explanation is derived from the techniques and examples discussed in the provided context (Swain1.pdf).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram backprojection produces a probability map of object locations based on a model histogram. This probability map is commonly used with tracking algorithms such as mean-shift to estimate and update the object's position across frames.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‘gained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‘gained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between first and second derivative methods for edge detection?,"Based on the lecture materials, here are the key differences between first and second derivative methods for edge detection:

### First Derivative Methods (e.g., Sobel, Roberts)
- **Detect edges as peaks** in the first derivative signal
- **Strong response at edges** but also respond to noise
- **Require careful peak detection and threshold selection**
- **More commonly used in practical applications**

### Second Derivative Methods (e.g., Laplacian)
- **Detect edges as zero crossings** in the second derivative signal
- **Provide well-defined, easy-to-detect edge locations**
- **Form smooth, connected contours** but tend to round off corners
- **Less common in practice compared to first derivative methods**

The main distinction is that first derivative methods identify edges where the intensity change is greatest (peaks), while second derivative methods find edges where the rate of change itself changes direction (zero crossings). According to the lecture notes, first derivative methods are ""much more common in practical applications,"" partly due to the influence of John Canny's work on optimal edge detection.

**Source:** Lecture 6A - Derivative and Edges (Slides 41-42)

Would you like me to elaborate on any specific aspect of these edge detection methods?

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","First derivative methods (e.g., Sobel, Roberts) look for peaks in the image gradient to find edges. Second derivative methods (e.g., Laplacian of Gaussian) look for zero-crossings in the second derivative of the image to find edges.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in",
What are the main steps in the Canny edge detection algorithm?,"Based on the provided lecture materials, the main steps in the Canny edge detection algorithm are as follows:

1.  **Noise Reduction using Gaussian Smoothing:** The first step is to reduce noise in the image by applying a Gaussian filter. This helps to prevent the detection of spurious edges caused by noise.

2.  **Gradient Calculation using Sobel Filters:** The intensity gradient of the smoothed image is calculated. This is done using Sobel filters (or similar operators) in both the horizontal (x) and vertical (y) directions to find the gradient magnitude and direction at each pixel.

3.  **Non-Maximum Suppression:** This is a thinning process. The algorithm scans the image to remove any pixels that are not considered to be part of an edge. For each pixel, it checks if it is a local maximum in its neighborhood in the direction of the gradient. If it is not the local maximum, it is suppressed (set to zero), resulting in thinner edges.

4.  **Hysteresis Thresholding:** This final step uses two threshold values (a high threshold and a low threshold) to decide which edges are truly edges and which are not.
    *   Any pixel with a gradient value **above the high threshold** is immediately classified as a strong edge.
    *   Any pixel with a gradient value **below the low threshold** is immediately rejected as a non-edge.
    *   Pixels with values **between the two thresholds** are classified as weak edges. These are only accepted as part of a final edge if they are connected to a strong edge. This helps to ensure connectivity of edges while ignoring noisy, weak edges that are not connected to strong ones.

**Source:**
*   `./documents/Lab 6.pdf`: The provided context from the lab notes explicitly states these four steps: ""The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters...To obtain thin edges, non-maximum suppression is performed...Lastly, hysteresis thresholding selects the edges by using two threshold values.""

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","The Canny edge detection algorithm involves: 1) Gaussian smoothing to reduce noise, 2) finding the intensity gradient, 3) non-maximum suppression to thin edges, and 4) hysteresis thresholding to link edges.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the “objects” in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¢ EX 0 if ||X@) — XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the “objects” in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¢ EX 0 if ||X@) — XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing",
What is the purpose of non-maximum suppression in the Canny edge detector?,"Based on the provided lecture materials, the purpose of **non-maximum suppression** in the Canny edge detector is to **obtain thin edges** by saving only the local maximums of the gradient in the gradient direction. This process effectively thins the edges, ensuring that the detected edges are represented by a single, sharp line rather than a thick, blurred response. This is a crucial step for producing clean, well-defined edges before the final thresholding stage.[^1]

[^1]: Source: Lecture notes from `./documents/Lec-6A-Derivative and Edges.pdf` and `./documents/Lab 6.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Non-maximum suppression is used to thin the edges found by the gradient calculation. For each pixel, it checks if the gradient magnitude is the largest in the direction of the gradient. If not, the pixel is suppressed (set to zero).","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is hysteresis thresholding?,"Of course! I'd be happy to explain hysteresis thresholding, which is a crucial technique in edge detection.

Based on the provided lecture materials, **hysteresis thresholding** is a sophisticated method used to decide which potential edge pixels in an image are true edges and which are not. It is famously used in the Canny edge detector.

Here’s a breakdown of how it works and why it's used:

### The Problem with Simple Thresholding
A simple approach to edge detection is to apply a single threshold to a gradient magnitude image (which highlights areas of rapid intensity change). Pixels with a value above the threshold are labeled as edges. However, this has a major drawback:
*   A **high threshold** will only catch the strongest, most certain edges but can break continuous edges into fragments, leaving gaps.
*   A **low threshold** will preserve more of a continuous edge but will also include a lot of ""noise"" – weak gradients that are not true edges.

### The Hysteresis Solution
Hysteresis thresholding solves this by using **two thresholds** instead of one: a **high threshold (T_high)** and a **low threshold (T_low)**.

The process works in two stages:

1.  **Strong Edges:** All pixels with a gradient magnitude above `T_high` are immediately classified as strong, definite edge pixels.

2.  **Weak Edges:** All pixels with a gradient magnitude between `T_low` and `T_high` are classified as weak, potential edge pixels.

3.  **The Connecting Rule:** The final, crucial step is to check all weak edge pixels. A weak pixel is **only promoted to a strong edge pixel** if it is connected to a strong edge pixel (e.g., it is adjacent to one in its 8-neighborhood). If a weak pixel is not connected to a strong edge, it is discarded as noise.

### Why It's Effective
As stated in the lecture slides, the effect is to **""keep weak edges if they connect strong edges.""** This elegantly combines the benefits of both high and low thresholds:
*   It maintains the precision of a high threshold by rejecting isolated noise.
*   It achieves the continuity of a low threshold by filling in the gaps between strong edge segments.

**In summary, hysteresis thresholding is an industry-standard method that uses two thresholds and a connectivity check to produce clean, continuous edge maps from noisy gradient data.**

*Source: `./documents/Lec-6A-Derivative and Edges.pdf` (Slides 49-52)*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Hysteresis thresholding is a technique used in the Canny edge detector to connect weak edges to strong edges. It uses two thresholds, a high and a low. Pixels with a gradient magnitude above the high threshold are considered strong edges, and pixels between the two thresholds are considered weak edges. Weak edges are only kept if they are connected to strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of",
What is the difference between the Sobel and Roberts edge detectors?,"Based on the provided lecture materials, here are the key differences between the Sobel and Roberts edge detectors:

### 1. **Computation and Complexity**
- **Roberts (Cross Operator):** Very quick to compute – uses only 4 pixels and involves only subtractions and additions. However, it is very sensitive to noise and only gives a strong response to very sharp edges. [Source: './documents/Lec-6A-Derivative and Edges.pdf']
- **Sobel:** Uses a larger kernel (typically 3x3) and involves more computations compared to Roberts. Larger Sobel operators are more stable in noise. [Source: './documents/Lec-6A-Derivative and Edges.pdf']

### 2. **Noise Sensitivity**
- **Roberts:** Highly sensitive to noise due to its small kernel size, making it less reliable in noisy images.
- **Sobel:** More robust to noise because of its larger kernel, which provides smoothing effects alongside edge detection.

### 3. **Common Use**
- **Roberts:** Less common nowadays due to its noise sensitivity and limitation to detecting only very sharp edges. [Source: './documents/Lec-6A-Derivative and Edges.pdf']
- **Sobel:** Still widely used in practice for its balance between simplicity and effectiveness, especially in applications where noise is a concern.

### 4. **Output Quality**
- Both methods use a super-supplied threshold (a threshold applied after gradient computation) to finalize edge maps. However, Sobel generally produces thicker and more connected edges, while Roberts may yield thinner edges but with more gaps and noise artifacts.

In summary, while both are first-derivative edge detectors, Sobel is preferred for its noise robustness and reliability, whereas Roberts is faster but更适合 for high-contrast, noise-free images with very sharp edges. If you're working on an assignment, consider the image noise levels and edge sharpness when choosing between them!

If you need further clarification or help with implementing these detectors, feel free to ask! 😊

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","The Sobel and Roberts edge detectors are both first-derivative edge detectors, but they use different kernels to approximate the gradient. The Sobel operator uses 3x3 kernels, while the Roberts operator uses 2x2 kernels.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute – 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e© Nottingham “i UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V°L f(x. ») * G(x. y)] = V°G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <——<——37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute – 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e© Nottingham “i UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V°L f(x. ») * G(x. y)] = V°G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <——<——37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a color histogram?,"A color histogram is a representation of the distribution of colors in an image. It works by discretizing the color space—for example, using red, green, and blue (RGB) axes—and then counting how many pixels in the image fall into each discrete color bin. Essentially, it groups similar colors together and tallies their frequency, providing a statistical summary of color occurrence in the image.

Key points about color histograms:
- They are **invariant to translation and rotation** about the viewing axis, meaning that moving or rotating the object in the image won't significantly change the histogram.
- They change **slowly** under variations like changes in the angle of view, scale, or partial occlusion, making them robust for tasks like object recognition.
- Histograms treat each bin as a feature, which can be used in pattern recognition to compare images based on color similarity.

For example, if you have an image with many shades of blue and green, the histogram would show high counts in the blue and green bins, providing a ""fingerprint"" of the image's color content.

This concept is foundational in techniques like **Histogram Intersection** or **Histogram Backprojection**, which are used for object identification and segmentation in image processing.

**Source:**  
- Swain, M.J., & Ballard, D.H. (1991). *Color Indexing*. International Journal of Computer Vision. [./documents/Swain1.pdf]  

If you'd like a practical example or more details on how to compute one, just let me know!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",A color histogram is a representation of the distribution of colors in an image. It counts the number of pixels for each color in a discrete color space.,"is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,","is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,",
How can you use a histogram to improve image contrast?,"Of course! This is a fundamental topic in image processing. Based on the lecture materials, histograms are used to improve contrast primarily through two key techniques: **Histogram Equalisation** and **Contrast Stretching**.

Here’s how you can use a histogram for this purpose:

### 1. Understanding the Histogram First

Before you can improve contrast, you need to analyze the image's histogram. As stated in the lecture notes:
> ""Histograms provide useful global information about the image... The histogram of a digital image with grey levels in the range [0, L-1] is a discrete function... `P(r_k) = n_k / n`"" `['./documents/Lec-2-Histogram (Whole Image Understanding).pdf']`

A quick look at the histogram tells you about the image's contrast:
*   **Low Contrast:** The pixel values are clustered in a narrow range of the histogram (e.g., mostly mid-tones with no pure blacks or whites).
*   **High Contrast:** The pixel values are spread out across the entire range of the histogram, from 0 to 255. `['./documents/Lec-2-Histogram (Whole Image Understanding).pdf']`

The goal of contrast enhancement is to transform a low-contrast histogram into a high-contrast one.

### 2. Technique 1: Histogram Equalisation

This is the more powerful and automated method.

*   **What it does:** Histogram equalisation aims to ""flatten"" the histogram and redistribute the pixel values to achieve a **uniform distribution** across the entire intensity range. `['./documents/Lec-2-Histogram (Whole Image Understanding).pdf']` The lecture notes describe it as a technique ""to transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values."" `['./documents/Lec-2-Histogram (Whole Image Understanding).pdf']`

*   **How it uses the histogram:** The process uses the original image's histogram (specifically, the **Cumulative Distribution Function (CDF)** derived from it) as a mapping function. It reassigns intensity values so that the probability of each intensity level becomes approximately equal.

*   **Practical Application:** In your lab work, you would:
    1.  Convert your image to greyscale.
    2.  Compute and display its original histogram.
    3.  Use OpenCV's `equalizeHist()` function to perform the transformation.
    4.  Display the new, equalized image and its new histogram. You will see the histogram is much flatter and more spread out, resulting in an image with significantly improved contrast. `['./documents/Lab 2.pdf']`

### 3. Technique 2: Contrast Stretching (Normalization)

This is a simpler, more controlled linear method.

*   **What it does:** Contrast stretching linearly expands the original narrow range of intensity values to cover a desired wider range (typically the full [0, 255] range). It does *not* aim for a perfectly uniform histogram like equalization does; it just performs a linear ""stretch.""

*   **How it uses the histogram:** You identify the minimum (`I_min`) and maximum (`I_max`) intensity values present in the original image from its histogram. A linear transformation function is then applied to map the range `[I_min, I_max]` to `[0, 255]`.

*   **Practical Application:** In your lab, you would:
    1.  Use OpenCV's `normalize()` function with the `NORM_MINMAX` flag to perform contrast stretching on your greyscale image.
    2.  Display the resulting image and its histogram. The histogram will be stretched to fill the available space, improving contrast. `['./documents/Lab 2.pdf']`

### Key Difference Between the Two Methods

As shown in your lab materials `['./documents/Lab 2.pdf']`, the key difference is in the transformation process:
*   **Contrast Stretching** is a **linear** operation. It's simple but can be ineffective if the image already has pixels at both extremes (0 and 255), as there's nothing to stretch.
*   **Histogram Equalization** is a **non-linear** operation based on the image's cumulative histogram. It is more adaptive and powerful, as it can improve contrast even in images where the intensities are not confined to a narrow band, but it can sometimes lead to a somewhat unnatural-looking ""over-enhanced"" result.

In summary, you use the histogram to **diagnose** the contrast issue and then apply a transformation (either equalization or stretching) that **redistributes the pixel values** shown in the histogram to cover a wider, more uniform range, thereby enhancing the image's contrast.

I hope this clears things up! Let me know if you'd like a step-by-step walkthrough of the code for either method.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",Histogram equalization can be used to improve image contrast by redistributing the pixel intensities to create a more uniform histogram.,"the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image • To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values • More general than linear or piecewise contrast stretching - No parameters to specify, “apply a transform that makes the output histogram FLAT” Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability","the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image • To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values • More general than linear or piecewise contrast stretching - No parameters to specify, “apply a transform that makes the output histogram FLAT” Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability",
What is a binary image?,"A binary image is an image where each pixel can have only one of two possible values: typically 0 (representing black) or 255 (representing white) in an 8-bit system, though in some contexts, these values are simply 0 and 1. 

It is the result of a process called **thresholding**, where a grey-level (or colour) image is converted into a binary one by classifying each pixel as either foreground (object) or background based on a chosen threshold value, T.

- If a pixel's intensity is **less than T**, it is set to the object value (e.g., 0/black).
- If a pixel's intensity is **greater than or equal to T**, it is set to the background value (e.g., 255/white).

Binary images are fundamental in image processing for tasks like segmentation, edge detection, and as a precursor to operations like morphological processing. They often require further processing, such as noise removal or enhancement, to be truly useful.

*Source: './documents/Lec-4-Thresholding and Binary Images.pdf' and './documents/Lab 4.pdf'*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image is an image where each pixel can have only one of two values, typically 0 or 1 (or black and white).","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform",
How can you create a binary image from a grayscale image?,"To create a binary image from a grayscale image, you use a process called **thresholding**. This technique classifies pixels as either foreground or background based on their intensity values. 

## Steps to Create a Binary Image:

1. **Select a threshold value**: Choose a value between 0-255 (for 8-bit images) that will serve as the dividing line between black and white pixels.

2. **Apply thresholding**:
   - Pixels with intensity values **above** the threshold are set to 255 (white)
   - Pixels with intensity values **below or equal** to the threshold are set to 0 (black)

## Example using OpenCV:
```python
import cv2

# Read image in grayscale
gray_image = cv2.imread('image.png', cv2.IMREAD_GRAYSCALE)

# Apply thresholding (example with threshold value of 127)
_, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)
```

## Advanced Methods:
- **Otsu's thresholding**: Automatically determines the optimal threshold value
- **Adaptive thresholding**: Uses different threshold values for different regions of the image

The result is a binary image where pixels are either completely black (0) or completely white (255), which is useful for applications like edge detection and image segmentation[^1].

[^1]: Source: Laboratory Sheet 4: Thresholding & Binary Images - COMP2032

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image can be created from a grayscale image using thresholding. A threshold value is chosen, and all pixels with an intensity value above the threshold are set to one value (e.g., 1), while all pixels below the threshold are set to the other value (e.g., 0).","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
What are connected components in a binary image?,"Based on the provided lecture materials, connected components in a binary image refer to **connected sets of pixels** that share the same value (typically foreground/object pixels) and are adjacent to each other.

Here's a breakdown of the concept:

1. **Definition**: In a binary image where black pixels are typically objects and white pixels are background, a connected component is a set of black pixels that are connected to each other through a path of adjacent pixels.

2. **Connectivity Types**: Components can be defined using different connectivity rules:
   - **4-neighbors**: Pixels are connected if they share an edge (up, down, left, right)
   - **8-neighbors**: Pixels are connected if they share an edge or a corner

3. **Labeling Process**: Connected component algorithms assign a unique label (typically an integer) to each distinct connected set of foreground pixels, converting the binary image into a label image where each component has its own identifier.

4. **Purpose**: This process allows individual objects in the image to be identified and analyzed separately. Properties like size, shape, and position can then be calculated for each component.

The labeling is typically done through algorithms that scan the image (often in two passes) and assign labels based on the connectivity of neighboring pixels that have already been processed.

*Source: Lecture 5 - Morphology slides (./documents/Lec-5-Morphology.pdf) and Practical Exercises with Solutions - Morphology (./documents/Practicals with Solutions - Morphology.pdf)*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

","Connected components are sets of connected pixels in a binary image that share the same value (usually the ""on"" value). They represent distinct objects or regions in the image.","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 – 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) – 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‘ —s Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython’s array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 – 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) – 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‘ —s Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython’s array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -",
How can you use connected components analysis?,"Of course! Connected components analysis is a fundamental technique in image processing used to identify and label distinct, separate regions (or ""blobs"") in a binary image.

Based on the course materials, here is how and why you use it.

### What is Connected Components Analysis?

It is an algorithm that scans a binary image (where pixels are either foreground, often `1` or `255`, or background, `0`) and groups adjacent foreground pixels into distinct objects. Each group of connected pixels is then assigned a unique label.

As defined in the lecture notes:
> ""If black pixels are objects & white are background, give each connected set of black pixels a different label.""
*(Source: `Lec-5-Morphology.pdf`)*

### How to Use It

The primary use case, as covered in your practical exercises, is to **convert a binary image into a label image**. This allows you to move from simply knowing *where* the foreground is to understanding *how many* distinct objects there are and to analyze the properties of each one individually.

**A typical workflow involves:**

1.  **Preprocessing:** First, you need a binary image. This is often achieved by applying a thresholding operation (e.g., using Otsu's method) or edge detection on a grayscale image to separate the objects of interest from the background.
2.  **Labeling:** Apply the connected components algorithm to this binary image.
3.  **Analysis:** Once you have a label map, you can analyze the properties of each labeled component.

### Key Applications and Uses

From the context provided, here are the main ways you can use connected components analysis:

1.  **Object Counting:** This is one of the most straightforward applications. The number of unique labels (minus the background label, usually 0) gives you the count of distinct objects in the image.

2.  **Calculating Object Properties:** After labeling, you can calculate properties for each individual object, which is incredibly powerful for analysis. These properties can include:
    *   Area (number of pixels)
    *   Centroid (center of mass)
    *   Bounding box coordinates
    *   Perimeter
    *   Solidity, eccentricity, and other shape descriptors.

3.  **Creating Masks for Further Processing:** You can use the label image to create a mask for a specific object. For example, you could isolate the largest component or any component that meets certain criteria (e.g., area > 100 pixels). As noted in the practicals:
    > ""Binary images can be used as masks, to select which pixels an operation is applied to.""
    *(Source: `Practicals - Morphology.pdf`)*

4.  **Filtering and Cleaning Binary Images:** You can use component properties to remove noise. For instance, you could decide to keep only components with an area above a certain threshold, effectively filtering out small noise pixels that were incorrectly identified as foreground.

### Implementation in Python (OpenCV)

A common way to perform connected components analysis in Python is using the `cv2.connectedComponentsWithStats()` function.

```python
import cv2
import numpy as np

# Load an image and convert it to a binary image (example using simple thresholding)
image = cv2.imread('your_image.jpg', cv2.IMREAD_GRAYSCALE)
_, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)

# Apply connected components analysis
num_labels, label_map, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)

# num_labels: Total number of labels (background is label 0)
# label_map: A 2D array the same size as the image, where each pixel has its component's label
# stats: Statistics for each label, including area, bounding box coordinates.
# centroids: The center (x, y) of each component.

print(f""Number of objects found: {num_labels - 1}"") # Subtract 1 to exclude the background

# Example: Get the area of the first real object (label 1)
area_of_first_object = stats[1, cv2.CC_STAT_AREA]
print(f""Area of the first object: {area_of_first_object} pixels"")
```

**Note on Performance:** As mentioned in the lecture slides, these algorithms can be computationally intensive: ""Connected component algorithms are slow and often a bottleneck.""
*(Source: `Lec-5-Morphology.pdf`)*

### Summary

You use connected components analysis to **transition from a simple binary image to an understanding of the individual objects within it**. It is a critical step for tasks like counting objects, filtering based on size, feature extraction, and isolating specific regions for more advanced processing, which is highly relevant to your coursework on image segmentation.

For any further clarification on the algorithm's implementation for your specific coursework problem, I recommend contacting the teaching assistant, **Mr. Irfan Yaqub** (`hcxiy1@nottingham.edu.my`).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

","Connected components analysis is used to identify and label the distinct objects in a binary image. Once labeled, properties of each component, such as its area, perimeter, and shape, can be calculated.","basic information about the image.● Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.● Use rotate to rotate the image by 90 degrees. Display the result.● Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.● imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated","basic information about the image.● Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.● Use rotate to rotate the image by 90 degrees. Display the result.● Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.● imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated",
What is the difference between 4-connectivity and 8-connectivity?,"Based on the provided context, I don't see specific information about 4-connectivity and 8-connectivity in image processing. These concepts are fundamental to digital image processing and relate to how pixels are connected to their neighbors, but they aren't covered in the retrieved materials you've provided.

For questions about connectivity concepts in image processing, I recommend:

**Contacting the module convenor:**
- Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my)
- Office: Room BB71 Block B, Malaysia Campus

**Or the teaching assistant:**
- Mr Irfan Yaqub (hcxiy1@nottingham.edu.my)

They can provide you with the specific lecture materials or explanations about pixel connectivity that would be most relevant for your course. You might also want to check if this topic is covered in other lecture notes or textbooks recommended for your module.

Would you like me to help you with any other image processing concepts that might be covered in the provided materials?


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about 4-connectivity vs 8-connectivity&body=Dear Dr. Chandesa,

I'm a student in your Introduction to Image Processing module and I have a question about connectivity concepts in image segmentation. Could you please explain the difference between 4-connectivity and 8-connectivity, and when each should be used?

Thank you for your help.

Best regards,
[Your Name]"">Email Dr. Chandesa</a>","In a 2D grid, 4-connectivity considers only the top, bottom, left, and right neighbors of a pixel. 8-connectivity considers these four neighbors as well as the four diagonal neighbors.","< k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than “ie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an—-2am—«£ +1)-—k+3an—- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) — assoc(A, A) assoc(A, V) 4 assoc(B, V) — assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2— Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","< k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than “ie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an—-2am—«£ +1)-—k+3an—- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) — assoc(A, A) assoc(A, V) 4 assoc(B, V) — assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2— Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
What is a Region of Interest (ROI)?,"Based on the retrieved context from the lecture materials, a **Region of Interest (ROI)** is a very powerful concept in image processing. It refers to a specific subset or area within an image that is selected for further processing or analysis. 

The key points about ROIs are:

1. They allow code to be written so that **the output of one process can determine where another is applied**. This means you can focus processing only on relevant parts of an image.

2. ROIs can be **defined in various ways**, including by color (as mentioned in the context: ""ROIs defined by colour"") or through other segmentation methods.

3. The concept enables **adaptive processing** where different techniques or parameters can be applied to different regions of the same image. For example, the context shows how different thresholds can be calculated and applied to mask regions (light and dark areas).

The ROI approach is fundamental because it allows for more efficient and targeted image processing, rather than applying the same operation to the entire image uniformly.

*Source: Practicals with Solutions - Thresholding and Binary Images.pdf*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - Thresholding and Binary Images.pdf"" target=""_blank"">Practicals with Solutions   Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

",A Region of Interest (ROI) is a selected area of an image that is targeted for a specific purpose or analysis.,"pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‘pendent high-levet requires that, first, the MIVURU representat : ‘action process be contnes aramelers corresponding £ domain, is Of Images is a paradigm alec sroals. A feahire space ig “dA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon § domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‘ the feature of interest im into a point in thes parameter. After the entiaction process be et rame‘ers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each…8 & &€ 8 8 8 8 8B 7250 150o §@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds,","pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‘pendent high-levet requires that, first, the MIVURU representat : ‘action process be contnes aramelers corresponding £ domain, is Of Images is a paradigm alec sroals. A feahire space ig “dA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon § domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‘ the feature of interest im into a point in thes parameter. After the entiaction process be et rame‘ers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each…8 & &€ 8 8 8 8 8B 7250 150o §@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds,",
How is a graph used in the Livewire segmentation algorithm?,"Based on the retrieved context, the Livewire segmentation algorithm uses a graph representation of the input image in a crucial way. Here's how the graph is utilized:

## Graph Representation in Livewire

The algorithm converts the input image into a graph where:
- **Nodes/Pixels**: Each pixel in the image becomes a node in the graph
- **Edges/Connections**: Edges connect neighboring pixels (typically 4 or 8-connected neighbors)
- **Edge Weights**: Each edge is assigned a cost/weight based on image properties

## Purpose of the Graph

The graph structure enables the algorithm to:
1. **Calculate optimal paths** between user-selected points
2. **Find boundaries** by avoiding high-cost edges that cross object boundaries
3. **Snap to edges** by following low-cost paths along image boundaries

## How It Works

When a user places seed points, Livewire uses Dijkstra's algorithm (a shortest path algorithm) on this graph to find the optimal path between points. The algorithm prioritizes paths that follow strong image edges by assigning lower costs to edges that:
- Have high gradient magnitude (strong edges)
- Cross zero-crossings (edge locations)
- Align with object boundaries

This graph-based approach allows the ""intelligent scissors"" to snap to and follow object boundaries as the user moves the cursor, creating precise segmentation boundaries.

*Source: COMP2032 Laboratory Sheet 8b - Livewire.pdf*

Would you like me to elaborate on any specific aspect of how the graph construction or pathfinding works in Livewire?

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

<a href=""http://localhost:5003/documents/Practicals - Interactive Segmentation.pdf"" target=""_blank"">Practicals   Interactive Segmentation</a>

","In the Livewire algorithm, the image is converted into a graph where each pixel is a node. The edge weights between adjacent pixels are determined by the image's edge information, such as intensity differences.","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the role of Dijkstra's algorithm in Livewire?,"Based on the provided lecture materials, Dijkstra's algorithm plays a **fundamental role** in the Livewire (Intelligent Scissors) segmentation process. Its primary function is to find the **optimal path** between two points on an image, ensuring the boundary ""snaps"" to the edges of the object you want to segment.

Here's a breakdown of its specific role:

1.  **Graph Representation:** The image is first converted into a graph where each **pixel is a node**.
2.  **Edge Weights:** The connections (edges) between adjacent pixels are given **weights**. These weights are based on the image's edge information, typically representing the **intensity difference or gradient** between pixels. A low cost (weight) is assigned to edges that are strong image boundaries (which you want the live-wire to follow), and a high cost is assigned to weak or non-existent edges.
3.  **Finding the Optimal Path:** Once the user sets a starting point (""seed point"") and moves their cursor (the ""free point""), **Dijkstra's algorithm is used to compute the shortest path** from the seed point to *every other pixel* in the image based on the cumulative cost of the edges. This creates a map of the lowest-cost path from the seed to anywhere.
4.  **Interactive Snapping:** As you move your cursor, the algorithm doesn't need to recalculate everything from scratch. It uses the precomputed path costs from the seed point. The live-wire you see on screen is the **optimal path from the current cursor position back to the seed point**, following the pointers set by Dijkstra's algorithm. This is what causes the wire to dynamically ""snap"" to and follow the strongest object boundaries as you move the mouse.

In essence, Dijkstra's algorithm is the computational engine that makes the live-wire ""intelligent"" by constantly finding the lowest-cost path, which corresponds to the most pronounced visual boundary in the image.

**Source:**
*   The steps of graph conversion and Dijkstra's use are detailed in the laboratory sheet `./documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf`.
*   The concept of the path adapting and snapping to the boundary is further illustrated in the figure from `./documents/mort-sigg95.pdf`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

",Dijkstra's algorithm is used in the Livewire algorithm to find the shortest path between a user-selected start point and end point on the image graph. This shortest path corresponds to the object boundary.,"f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.• Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if’ University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:• The image is converted into a graph where each pixel represents a node.• The edge weights between adjacent pixels are determined based on the image's edge information.• Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:• The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively “snap” and “wrap” the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. Speciﬁcally, note the live-wire segments corresponding to user-speciﬁed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-ﬂy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.• Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if’ University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:• The image is converted into a graph where each pixel represents a node.• The edge weights between adjacent pixels are determined based on the image's edge information.• Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:• The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively “snap” and “wrap” the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. Speciﬁcally, note the live-wire segments corresponding to user-speciﬁed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-ﬂy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input",
What are the main steps in the GrabCut algorithm?,"Based on the provided lecture materials, the main steps of the GrabCut algorithm involve an iterative process that combines user interaction with graph cuts for interactive foreground extraction. Here’s a concise breakdown:

1. **Initial User Interaction**: The user defines a region of interest by drawing a rectangle (or using brush strokes) around the foreground object. In the initial implementation, a rectangle is used to roughly indicate the object.

2. **Initialization**:  
   - A mask is initialized to distinguish between foreground and background regions.  
   - Gaussian Mixture Models (GMMs) are used to model the color distributions of the foreground and background based on the initial user input.

3. **Graph Construction and Min-Cut/Max-Flow Optimization**:  
   - A graph is built where pixels (or groups of pixels) are nodes, and edges represent penalties based on color similarity and spatial proximity.  
   - A graph cut (min-cut) algorithm (e.g., Dinic’s algorithm) is applied iteratively to separate foreground from background by minimizing an energy function that balances data (color fit) and smoothness (edge consistency) terms.

4. **Iterative Refinement**:  
   - The algorithm refines the segmentation over multiple iterations. After each graph cut, the GMMs are updated based on the current segmentation, improving the model for the next cut.  
   - This iterative process continues until convergence (minimal change between iterations).

5. **Optional Additional Interaction**:  
   - If the result isn’t satisfactory, the user can provide additional input (e.g., marking misclassified areas with foreground/background brushes), and the algorithm repeats the process using the updated mask.

6. **Border Matting (Optional)**:  
   - For smoother edges, GrabCut can estimate an alpha matte along object boundaries, blending foreground and background naturally to avoid jagged edges.

This approach efficiently combines color and edge information, reducing the need for detailed user input while producing high-quality segmentations.

For more details, you can refer to the original paper by Rother, Kolmogorov, & Blake (2004) or the provided lecture slides on interactive segmentation. If you have further questions, feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

<a href=""http://localhost:5003/documents/Laboratory Sheet 8 - Interactive Segmentation.pdf"" target=""_blank"">Laboratory Sheet 8   Interactive Segmentation</a>

","The GrabCut algorithm involves an iterative process of: 1) assigning GMM components to pixels, 2) learning GMM parameters from the data, 3) estimating the segmentation using a min-cut algorithm, and repeating until convergence.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let’s try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‘input’ to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if’ University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let’s try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‘input’ to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if’ University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:",
What is the purpose of the Transformer in the Vision Transformer (ViT) model?,"Based on the provided context, the purpose of the Transformer in the Vision Transformer (ViT) model is to process sequences of image patches for image recognition tasks. Specifically:

1. The image is divided into fixed-size patches (e.g., 16x16 pixels), which are linearly projected into embeddings.
2. These patch embeddings, along with positional encodings and an extra learnable [class] embedding, are fed into a standard Transformer encoder architecture.
3. The Transformer encoder uses self-attention mechanisms to model relationships between all patches, allowing it to capture global context and dependencies across the entire image, unlike Convolutional Neural Networks (CNNs) which primarily focus on local features through convolutional filters.

This approach enables ViT to achieve state-of-the-art performance in image recognition, especially when pre-trained on large-scale datasets, as referenced in the arXiv preprint (Dosovitskiy et al., 2020)[citation:./documents/Swain1.pdf].

If you'd like more details on how ViT compares to CNNs or its components, feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

",The Transformer in a Vision Transformer (ViT) model is used to capture both local and global relationships within an image by treating the image as a sequence of patches.,"words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNN🤔22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNN🤔22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the main difference between CNNs and ViTs?,"Based on the retrieved context from the course materials, I can explain the main differences between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).

**Main Differences Between CNNs and ViTs:**

1. **Architectural Approach:**
   - CNNs use convolutional layers that apply filters to local regions of an image, capturing spatial hierarchies and local features through weight sharing and pooling.
   - ViTs, as shown in the context, use a Transformer Encoder architecture. They split the image into fixed-size patches, linearly project them into embeddings (with added positional information), and process them using self-attention mechanisms to capture global context.

2. **Inductive Biases:**
   - CNNs have built-in inductive biases for images, such as translation invariance and locality, making them data-efficient for vision tasks.
   - ViTs have fewer built-in inductive biases for images; they rely on self-attention to learn spatial relationships from data, which often requires large-scale training.

3. **Handling of Image Data:**
   - CNNs process images in their raw spatial form through convolutional operations.
   - ViTs treat image patches as sequences (similar to words in NLP), enabling them to model long-range dependencies effectively.

**Is CNN better than ViT?**
According to the retrieved context (from Dosovitskiy et al., 2020), the answer is **False** (B: False). This suggests that ViTs can outperform CNNs in certain scenarios, especially when trained on large datasets, though the performance depends on the specific task and data size.

For more details, you might refer to the lecture materials or contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or the teaching assistant Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my) during office hours.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - CNN.pdf"" target=""_blank"">Practicals   CNN</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","CNNs use convolutional layers to extract features from an image, which are inherently local operations. ViTs, on the other hand, divide the image into patches and use a Transformer architecture to learn relationships between these patches, allowing them to capture more global context.","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by",
What is the purpose of zero-padding in convolutional layers?,"Based on the retrieved context from the lecture materials, the purpose of zero-padding in convolutional layers is **to add zeros to every side of the input boundaries**. This ensures that the filters fit the input image properly during convolution operations. 

More specifically, zero-padding helps in:
- Preserving the spatial dimensions of the input (preventing the output feature map from shrinking too quickly)
- Allowing the filter to properly cover the edges and corners of the input image
- Maintaining structural information at the borders

This technique is a key hyperparameter in convolutional layers, alongside the number of filters and stride, to control the size and shape of the output feature maps.

**Source:** Lecture notes (Lec-9-CNN.pdf) – ""Zero-padding adds zeroes to every side of the input boundaries – ensures filters fit the input image.""

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

",Zero-padding is the process of adding zeros around the border of an input image before applying a convolutional filter. This is done to control the spatial size of the output feature map and to ensure that the filter can be applied to pixels at the edge of the image.,"Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep","Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep",
What is stride in a convolutional layer?,"Of course! Let's break down what ""stride"" means in a convolutional layer.

In simple terms, **stride** is the number of pixels the filtering window (or kernel) moves across the input image after each convolution operation.

Here's a more detailed explanation:

*   When a filter (e.g., a 3x3 kernel) performs a convolution, it starts at the top-left corner of the image.
*   After calculating the dot product for that position, it needs to move to the next location.
*   The **stride** defines the size of this step or ""jump.""

The most common stride values are:

*   **Stride = 1**: The filter moves one pixel at a time. This results in a large, detailed output feature map that is only slightly smaller than the input.
*   **Stride = 2**: The filter moves two pixels at a time. This downsamples the output, making the feature map smaller and reducing its dimensionality.

**Why is stride important?**
*   **Larger Strides (e.g., 2)**: Reduce the spatial size of the output feature maps, which decreases the computational load and the number of parameters in subsequent layers. It also helps to provide a form of translation invariance.
*   **Smaller Strides (e.g., 1)**: Preserve more spatial information, leading to more detailed feature maps but requiring more computation.

This definition is supported by the retrieved context from the lecture materials, which states:
> ""Stride - # of pixels the filtering window moves after each operation"" (Source: `./documents/Lec-9-CNN.pdf`)

I hope this clarifies the concept! Let me know if you have any other questions.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","Stride is the number of pixels by which the filter moves across the image at each step. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time, resulting in a smaller output feature map.
QA about Material Content","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights",
What are the deliverables for the coursework?,"Based on the retrieved context from the assessment sheet, the deliverables for the COMP 2032 Introduction to Image Processing coursework are as follows:

**Per group submission (groups of 4 students):**
1. **Python codes** - These should be submitted as `.py` files, contained within a ZIP folder.
2. **A 6-page conference paper** - This should be a maximum of 2000 words and submitted in PDF format. Please note that submitting additional pages beyond the 6-page limit will result in a mark deduction.

This coursework is worth **50%** of your overall module grade.

For the most detailed and official information, please always refer to the latest version of the **Assessment Sheet COMP2032 Coursework 2025.pdf** on Moodle or contact the module convenor, Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), if you have any specific questions.

**Source:** `./documents/Assessment Sheet COMP2032 Coursework 2025.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The coursework requires a group submission of Python code files (as a ZIP folder), a 6-page conference paper (2000 words max, PDF format), and result images (as a ZIP folder). There is also an individual submission of a peer assessment form.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the deadline for the coursework submission?,"Based on the provided assessment information, the deadline for the COMP2032 Introduction to Image Processing coursework submission is:

**Friday, 25 April 2025, by 11:59pm**

This is a group submission that includes both Python code files and a conference paper. Please note that late submissions will be subject to a penalty of 5 marks (5%) for each late working day.

*Source: Assessment Sheet COMP2032 Coursework 2025.pdf*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The submission date for the coursework is Friday, 25 April 2025, by 11:59 pm.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the penalty for late submission?,"I'm sorry, but I don't have information about the penalty for late submission in the provided context or in my knowledge base for the Introduction to Image Processing module. 

For specific questions regarding assessment policies, submission deadlines, or penalties, I recommend you contact the module convenor directly:

**Dr. Tissa Chandesa**  
Email: Tissa.Chandesa@nottingham.edu.my  
Office: Room BB71 Block B, Malaysia Campus

Alternatively, you could check your course handbook or the official module information on the university's learning platform for detailed submission guidelines.


*You can use the following button to email Dr. Chandesa directly to ask about late submission penalties:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about Late Submission Penalties&body=Dear Dr. Chandesa,

I would like to inquire about the penalty for late submission of coursework for the Introduction to Image Processing module.

Could you please clarify what the policy is regarding late submissions?

Thank you for your assistance.

Best regards,
[Your Name]"">Email Dr. Chandesa</a>",Work submitted after the deadline is subject to a penalty of 5 marks for each late working day.,"reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3","reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3",
What is the topic of the coursework?,"Based on the retrieved context, the coursework for the Introduction to Image Processing module (COMP2032) is assessed by a 100% coursework component.

The topic involves:
*   **Programming:** Writing a Python application.
*   **A 2000-word conference paper:** This paper should include a literature review, an explanation of your work, and an evaluation of your results.

This information is sourced from the lecture slides: `./documents/Lec-1A-Introduction.pdf`.

For the most specific and detailed guidelines on the coursework, including the exact problem statement and submission deadlines, you should consult the official assignment brief on Moodle or contact the module convenor, **Dr. Tissa Chandesa** (Tissa.Chandesa@nottingham.edu.my).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

",The coursework involves developing a software solution in Python for the semantic segmentation of flowers. This includes creating an image processing pipeline to separate flowers from the background and then classifying them.,"practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main parts of the coursework?,"Based on the provided context, the main parts of the COMP2032 coursework are as follows:

1.  **A Programming Component**: You are required to develop a Python application.
2.  **A 2000-word Conference Paper**: This is a written report in a specific format (a 6-page max PDF) that should include a literature review, an explanation of your work, and an evaluation of your results.
3.  **An In-Lab Test**: This is a component you must complete, answering all questions. The context suggests this may be done individually.
4.  **Peer Assessment**: Each group member must individually submit a peer assessment form. Failure to do so will result in a 10% penalty on the overall group mark.
5.  **Result Images**: You must submit a ZIP folder containing the output images from your work.

This is a **group assessment**, though the in-lab test and peer assessment are individual components.

For the most precise and official instructions, please always refer to the latest version of the **""Assessment Sheet COMP2032 Coursework 2025.pdf""** provided on Moodle[^citation3].

[^citation3]: Information sourced from: ./documents/Assessment Sheet COMP2032 Coursework 2025.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The coursework has two main parts: a SEGMENTATION PART where you design and implement an image processing pipeline to segment flowers from images in Dataset 1, and a SEMANTIC PART where you classify the entire flower dataset (Dataset 2) using a provided Transformer-based model.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
What are the requirements for the output images in the segmentation part?,"Based on the provided context, the requirements for the output images in the segmentation part are as follows:

1. **File Format**: Output images must be saved as `.jpg` or `.png` files.  
2. **Image Size & Resolution**: Output images should ideally be **256×256 pixels**. If your segmentation output is a different size, it will be automatically resized, which may cause distortions.  
3. **Content**: The output image must have a **black background with only the segmented flower** visible.  
4. **Folder Structure**:  
   - Output images should be saved in an output folder, categorized into three sub-folders: `easy`, `medium`, and `hard`.  
   - Each output image should have the same name as the corresponding input image.  
5. **Automatic Processing**: Your solution should operate on all provided images without user intervention, using one parameter set for all.  

These requirements ensure that your segmentation results can be properly evaluated using metrics like **Cosine Similarity** and **Accuracy**, as outlined in the guidelines.

If you need further clarification, feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The output images must have a black background with only the segmented flower. They should be saved as .jpg files in an ""output"" folder, categorized into sub-folders named ""easy,"" ""medium,"" and ""hard,"" corresponding to the input images.","f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided","f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided",
What evaluation metrics should be used for the segmentation part?,"Based on the provided lecture materials, the segmentation evaluation model uses two specific metrics to assess your segmentation results:

1. **Cosine Similarity**: This metric measures the high-level similarity between your segmented images and the provided ground truth masks. It produces a score ranging from 0 to 1, where a higher score indicates better performance.

2. **Accuracy**: This metric measures pixel-wise accuracy between your segmented masks and the ground truth, evaluating how many pixels are correctly classified.

These metrics are designed to evaluate segmentation quality from both a structural (cosine similarity) and a detailed pixel-level (accuracy) perspective.

For your coursework submission, ensure your segmented images and ground truth masks meet the format requirements (.jpg or .png) and are properly organized in the specified folder structure as outlined in the guidelines.

*Source: Guidelines for Using the Segmentation Evaluation Model*

**Cited Materials:**

<a href=""http://localhost:5003/documents/Guidelines for Using the Segmentation Evaluation Model.pdf"" target=""_blank"">Guidelines for Using the Segmentation Evaluation Model</a>

",The evaluation of the segmentation pipeline can be both qualitative and quantitative. The coursework suggests using mean IoU (Intersection over Union) as a metric.,"f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| — MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results….BeWhat we can do with metrics?• Automatic evaluation• Comparison between differenti* / ,*approaches• Evaluation from multiple perspectives• …Siméoni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry é é 70 ‘slgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the","f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| — MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results….BeWhat we can do with metrics?• Automatic evaluation• Comparison between differenti* / ,*approaches• Evaluation from multiple perspectives• …Siméoni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry é é 70 ‘slgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the",
What is the purpose of Dataset 1?,"I'm sorry, but I don't have enough context to answer your question about the purpose of ""Dataset 1."" The retrieved materials provided don't contain specific information about different datasets or their purposes.

For clarification on the specific datasets used in your image processing module, I recommend:

1. **Contacting the module convenor:** Dr. Tissa Chandesa (Email: Tissa.Chandesa@nottingham.edu.my)
2. **Speaking with the teaching assistant:** Mr. Irfan Yaqub (Email: hcxiy1@nottingham.edu.my)
3. **Reviewing your course materials** or checking the official module documentation for details about specific datasets

They will be able to provide you with accurate information about Dataset 1 and its purpose in your image processing coursework.


You can contact the module convenor directly for clarification on Dataset 1:

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about Dataset 1 in Image Processing Module&body=Dear Dr. Chandesa,

I have a question regarding Dataset 1 mentioned in our image processing module. Could you please clarify what Dataset 1 refers to and what its specific purpose is?

Thank you for your assistance.

Best regards,
[Your Name]"">Email Dr. Chandesa</a>","Dataset 1 contains images of three species of flowers, categorized into ""easy,"" ""medium,"" and ""hard"" sub-folders, along with their corresponding ground truths. This dataset is to be used for developing and evaluating the image processing pipeline for the segmentation part of the coursework.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What is the purpose of Dataset 2?,"Based on the provided context from the Assessment Sheet, **Dataset 2** is used for the **semantic segmentation part** of the coursework.

Specifically, the context states:
> ""Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).""

Therefore, the purpose of Dataset 2 is to provide the images and their corresponding ground truth labels (masks) that you will use to develop, train, and evaluate your semantic segmentation pipeline for this assignment.

Source: `./documents/Assessment Sheet COMP2032 Coursework 2025.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","Dataset 2, consisting of 847 images with ground truths, is to be used for the semantic part of the coursework, which involves classifying the entire flower dataset.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What are the requirements for the conference paper?,"Based on the provided context from the assessment sheet, the requirements for the conference paper are as follows:

1.  **Format:** It must be a 6-page conference paper, submitted in PDF format. Please note that there is a strict page limit; any additional pages will result in a penalty of **5% deducted from the entire group's mark for every extra page**.

2.  **Content:** When writing your paper, you must address these points:
    *   Detail the chosen image processing method(s) and specific technique(s) you employed.
    *   Explain why you chose those specific techniques and methods. You may include pseudocode for them if applicable.
    *   Present the results obtained from your practical implementation.
    *   Critically evaluate your method(s) based on those results, discussing their strengths and weaknesses. This evaluation must make explicit reference to features in your results and use the **relevant metric for segmentation** to support your arguments.
    *   Critically evaluate your classification results, highlighting the strengths and weaknesses of your entire processing pipeline by performing a **pixel-wise classification**. You must use the **cosine similarity and accuracy metrics** generated from your semantic part to support your arguments.

3.  **Template:** You must use the provided template document (`COMP2032-CW-GroupXXX.docx`). You are to replace the ""XXX"" in the filename with your three-digit group number (e.g., Group 13 would use `COMP2032-CW-Group013.docx`). **It is very important that you do not change the formatting and headers** of this template, as marks will be deducted for incorrect formatting. Text colored in red and blue in the template is placeholder text that you must replace with your own content.

This information is sourced from the **Assessment Sheet COMP2032 Coursework 2025.pdf**.

For any further clarification on these requirements, your primary contact is the module convenor, **Dr. Tissa Chandesa** (Tissa.Chandesa@nottingham.edu.my). You can also consult the teaching assistant, **Mr. Irfan Yaqub** (hcxiy1@nottingham.edu.my).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The conference paper should be a maximum of 6 pages (2000 words), detailing the chosen methods, justifying their selection, presenting the results, and critically evaluating the implemented pipeline and classification results. It must be submitted in PDF format using the provided template.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
Are we allowed to modify the provided Transformer and Model_input Python files?,"Based on the provided context, I don't have specific information about whether you're allowed to modify the Transformer and Model_input Python files for your image processing module. 

The context mentions that in certain data loading implementations (like the MNIST dataset class), you're expected to maintain specific method names (`__init__`, `__getitem__`, `__len__`) but doesn't provide guidelines about modifying other files like Transformer or Model_input.

For clarification on whether you can modify these specific files, I recommend:

1. **Contacting Dr. Tissa Chandesa** (Module Convenor)
   - Email: Tissa.Chandesa@nottingham.edu.my
   - Office: Room BB71 Block B, Malaysia Campus

2. **Contacting the Teaching Assistant**
   - Mr Irfan Yaqub: hcxiy1@nottingham.edu.my

3. **Checking your assignment instructions or module guidelines** for any specific rules about code modification

4. **Attending office hours** if available to get direct clarification

It's always best to confirm with your instructors before modifying provided code files, as assignment requirements can vary significantly.


*You can use the following button to email Dr. Chandesa for clarification:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about modifying Transformer and Model_input Python files&body=Dear Dr. Chandesa,

I have a question regarding the Python files provided for the image processing module. Specifically, I would like to know if we are allowed to modify the Transformer and Model_input Python files, or if there are specific constraints on what changes we can make to these files.

Could you please clarify the modification guidelines for these files?

Thank you for your assistance.

Best regards,
[Your Name]
[Your Student ID]"">Email Dr. Chandesa</a>","No, you are not allowed to modify any of the provided parameters in the transformer and Model_input files.","test images which are not in the training dataset. First, let’s create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","test images which are not in the training dataset. First, let’s create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the peer assessment form for?,"Based on the provided context, the peer assessment form is for you to evaluate your group members' contributions to the coursework project.

According to the assessment sheet, each student must submit this form individually. The purpose is to provide a fair assessment of individual effort within the group work. **Failure to submit the peer assessment will result in a penalty of 10% of your overall group mark.**

You can find and submit the form by clicking on the “Peer Assessment” link on your Moodle page for the COMP2032 module.

For any specific questions about how to complete the form or what criteria it assesses, it is best to contact the module's teaching assistant, Mr. Irfan Yaqub, via email at `hcxiy1@nottingham.edu.my`.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",Each group member must individually submit a peer assessment form. Failure to do so will result in a 10% penalty on the overall group mark.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
What are the assessment criteria for the coursework?,"Based on the retrieved context from the assessment sheet, here are the coursework criteria for COMP2032:

**Assessment Breakdown (100% Coursework):**
This assessment consists of two main components:

1.  **Group Project: Programming + 2000-word conference paper (50%)**
    *   Development of a Python application.
    *   A written report comprising a literature review, an explanation of your methodology, and an evaluation of your results.

2.  **In-Lab Test (Individual) (50%)**
    *   This test will focus on the theories covered in the lectures and practical exercises.
    *   **Date, Time & Venue:** 29 April 2025 | 2:30pm till 3:30pm | TCR 1
    *   You must answer ALL questions.

**Justification and Discussion (from the conference paper criteria):**
The context also provides more detailed criteria that are likely part of the evaluation for the conference paper portion of the group project. These include:
*   **Justification of chosen algorithms** based on a performed literature review: **15%**
*   **Critical analysis and discussion of your findings:** **25%**
*   **Discussion of the strengths and weaknesses** of your proposed pipeline (based on your findings), prior to concluding on the classification result: **30%**

**Very Important Note:**
The context also contains a critical warning regarding the programming component:
**""[unable to run codes will result in 0% being awarded]""**

This means that if the Python application you submit does not run, it will result in a score of 0% for that entire component of the coursework.

For the most precise and official breakdown, you should always refer to the official **Assessment Sheet COMP2032 Coursework 2025.pdf** document provided on Moodle. If you have any specific questions about the expectations for the literature review or the application, your first point of contact should be:

**Dr. Tissa Chandesa** (Module Convenor)
*   **Email:** Tissa.Chandesa@nottingham.edu.my

**Source:** `./documents/Assessment Sheet COMP2032 Coursework 2025.pdf`

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The Python code is worth 30%, and the conference paper is worth 70%. The paper is broken down into justification of algorithms (15%), critical analysis of findings (25%), and discussion of strengths and weaknesses (30%).","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the format for the in-lab test?,"Based on the retrieved context from the assessment documentation, the in-lab test for COMP2032 is described as follows:

**Format:**
*   It is a **group** assessment.
*   Students are required to **answer ALL questions**.

The provided context does not specify further details such as the test's duration, question types (e.g., practical coding, theory), or specific topics covered.

For the most accurate and complete information regarding the test's structure, I recommend you:
1.  Check the full assessment brief on Moodle.
2.  Contact the module convenor, **Dr. Tissa Chandesa** (Tissa.Chandesa@nottingham.edu.my), for clarification. He will be able to provide you with the definitive format and any other specifics you need to prepare.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

",The in-lab test is an individual assessment worth 50% of the final grade. It focuses on theories covered in the lectures and practical exercises.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
When is the in-lab test?,"Based on the provided context, the in-lab test for COMP2032 (Introduction to Image Processing) is scheduled for:

**Date:** 29 April 2025  
**Time:** 2:30pm till 3:30pm  
**Venue:** TCR 1

This information is from the assessment details in your course materials [./documents/Assessment Sheet COMP2032 Coursework 2025.pdf]. Please make sure to double-check any updates from your lecturer or official university announcements as schedules can occasionally change. Good luck with your preparation!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The in-lab test is scheduled for 29 April 2025, from 2:30 pm to 3:30 pm in TCR 1.

QA about Out-of-the-Domain Questions","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150",
What is the capital of France?,"I'm here to assist with questions related to the **Introduction to Image Processing** module. Your question about the capital of France falls outside the scope of this course. 

If you have any questions about image processing topics—such as segmentation, morphology, histogram operations, or algorithms like Watershed or Histogram Backprojection—feel free to ask! For other subjects, I recommend consulting relevant resources or your course instructor for that specific topic.

Best regards!","I'm sorry, I can only answer questions related to image processing.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t “ enough representatt ‘action process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane» Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t “ enough representatt ‘action process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane» Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you explain the theory of relativity?,"I'm sorry, but I can only assist with questions related to the **Introduction to Image Processing** module. The theory of relativity is outside the scope of this course. If you have any questions about image processing topics—such as image derivatives, edge detection, segmentation, or histogram-based methods—I’d be happy to help!

If you're looking for help with physics or other subjects, I recommend reaching out to the appropriate lecturer or department at your university.","I'm sorry, I can only answer questions related to image processing.","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What are the main causes of climate change?,"I'm sorry, but I can only assist with questions related to the **Introduction to Image Processing** module. Your question about the main causes of climate change is outside the scope of this course.

If you have any questions about image processing topics—such as image segmentation, histogram analysis, or convolutional neural networks (CNNs)—feel free to ask, and I'll be happy to help!","I'm sorry, I can only answer questions related to image processing.","connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
"Who wrote the play ""Hamlet""?","I'm here to assist with questions related to the **Introduction to Image Processing** module, such as topics from lectures, practicals, or course materials. Your question about the author of ""Hamlet"" is outside the scope of this subject.

For literature-related inquiries, I recommend consulting resources specific to English literature or reaching out to the relevant department at your university. If you have any questions about image processing—like thresholding, edge detection, segmentation, or concepts from the course materials—feel free to ask!","I'm sorry, I can only answer questions related to image processing.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )ü if q’s cost changed, make q point back to pØ Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (lé*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ” 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts —__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )ü if q’s cost changed, make q point back to pØ Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (lé*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ” 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts —__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!",
What is the chemical formula for water?,"I'm sorry, but I can only assist with questions related to the Image Processing module, such as topics from the lectures, slides, and other provided materials. For questions about chemistry, like the chemical formula for water, I recommend consulting your chemistry course materials or reaching out to the relevant lecturer for that subject.","I'm sorry, I can only answer questions related to image processing.","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are",
Can you recommend a good restaurant in Nottingham?,"I'm sorry, but I can only assist with questions related to the Introduction to Image Processing module. I'm not able to provide recommendations for restaurants in Nottingham.

If you have any questions about image processing concepts, segmentation techniques, watershed algorithms, or other topics from your course materials, I'd be happy to help!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-7-Segmentation.pdf"" target=""_blank"">Lec 7 Segmentation</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of…A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and —1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 — 2k + 2k?)(a(x) + 7) + 2(1 — 2k) B(x)2a(x)_+ki —kh)MMki —kh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 £82)(a(e) +7) +211 — 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L—b*)6(@) , 2ba(x) by bM bM bM 6M (1+ ’)(«?(D — W)a +17(D — W)1) nTD1 b (1 — B)17(D — W)z 7 b1/D1 2b27(D—W)z 2b17(D— W)1 “opi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1— z)'(D—W)(1+2) b17D1 T (+2) - 60 —2)] @- W)[(1 +2) — 41-2) ; b1°D1Let D be an N x N diagonal matrix with d","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of…A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and —1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 — 2k + 2k?)(a(x) + 7) + 2(1 — 2k) B(x)2a(x)_+ki —kh)MMki —kh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 £82)(a(e) +7) +211 — 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L—b*)6(@) , 2ba(x) by bM bM bM 6M (1+ ’)(«?(D — W)a +17(D — W)1) nTD1 b (1 — B)17(D — W)z 7 b1/D1 2b27(D—W)z 2b17(D— W)1 “opi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1— z)'(D—W)(1+2) b17D1 T (+2) - 60 —2)] @- W)[(1 +2) — 41-2) ; b1°D1Let D be an N x N diagonal matrix with d",
What is the current weather forecast?,"I'm sorry, but I'm only able to assist with questions related to the **Introduction to Image Processing** module. Your question about the weather forecast falls outside of this scope.

If you have any questions about image processing concepts, techniques, or your coursework, I'd be happy to help!","I'm sorry, I can only answer questions related to image processing.","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the meaning of life?,"That's a profound philosophical question that has been contemplated for centuries! However, I'm here specifically to assist with topics related to the **Introduction to Image Processing** module.

If you have any questions about image processing concepts, techniques, algorithms, or assignments for this course, I'd be happy to help. For broader philosophical inquiries, you might want to consult resources or experts in philosophy or related fields.

Feel free to ask me anything about histogram operations, segmentation methods, min-cut/max-flow algorithms, or other image processing topics!","I'm sorry, I can only answer questions related to image processing.","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term ψ assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiﬁer trained using the SIFT descriptors. The pairwise term φ assigns to each pair of superpixels a potential to have similar or diﬀering labels based on the diﬀerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxﬂow algorithm to produce a ﬁnal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048×1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term ψ assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiﬁer trained using the SIFT descriptors. The pairwise term φ assigns to each pair of superpixels a potential to have similar or diﬀering labels based on the diﬀerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxﬂow algorithm to produce a ﬁnal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048×1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How does a car engine work?,"I'm here to help with questions related to the **Introduction to Image Processing** module. Your question about how a car engine works falls outside the scope of this course, which focuses on topics like image segmentation, morphology, connected components, and related techniques.

For this topic, I'd recommend consulting resources specific to automotive engineering or mechanical systems. If you have any questions about image processing—such as segmentation methods, morphological operations, or practical exercises from your course—feel free to ask!","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input",
What are the rules of chess?,"I'm here to help with questions about the **Introduction to Image Processing** module, such as topics from lectures, slides, and related materials. Your question about the rules of chess is outside the scope of this course.

If you have any questions about image processing concepts—like segmentation, histogram methods, or other topics covered in your module—feel free to ask! For chess rules, you might want to check a dedicated resource or guide.","I'm sorry, I can only answer questions related to image processing.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you tell me a joke?,"I'm here to help with questions related to your Introduction to Image Processing module! Unfortunately, I can't tell jokes as that falls outside the scope of image processing topics. 

If you have any questions about segmentation, thresholding, watershed algorithms, or other image processing concepts from your course materials, I'd be happy to help!","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!",
What is the history of the internet?,"I'm here to help with questions related to the **Introduction to Image Processing** module, such as topics from the lecture notes, slides, or practical materials. Your question about the history of the internet is outside the scope of this course.

If you have any questions about image processing concepts—like histogram intersection, adaptive thresholding, or other techniques covered in the module—feel free to ask! For assistance with the history of the internet, you might want to consult general computing resources or reach out to a relevant department at your university.","I'm sorry, I can only answer questions related to image processing.","tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding € Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ° ‘e : ie ciewk pg toate om ‘ous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding € Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ° ‘e : ie ciewk pg toate om ‘ous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
How do I learn to play the guitar?,"I'm sorry, but I can only assist with questions related to the Introduction to Image Processing module. Your question about learning to play the guitar is outside the scope of this course.

If you have any questions about image processing topics, such as histogram equalization, convolutional neural networks, interactive segmentation, or any other concepts from your module, I'd be happy to help!","I'm sorry, I can only answer questions related to image processing.","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let’s create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let’s initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f° University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:● In the first interaction, the user can only draw a rectangle on the image.● The user is encouraged to press Key 1 to start drawing the rectangle.● The user can start segmentation after drawing rectangles or curves by pressing “Enter”.● After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.● After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.● The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let’s create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let’s initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f° University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:● In the first interaction, the user can only draw a rectangle on the image.● The user is encouraged to press Key 1 to start drawing the rectangle.● The user can start segmentation after drawing rectangles or curves by pressing “Enter”.● After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.● After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.● The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5",
What are the best programming languages to learn in 2025?,"I'm here to help with questions specifically about the **Introduction to Image Processing** module (COMP 2032), such as concepts, practicals, or coursework-related topics. Your question about the best programming languages to learn in 2025 is outside the scope of this module.

For general programming language advice, I recommend consulting resources from computer science departments or career guidance services. If you have questions about image processing, like using Python for practical work (as mentioned in the coursework for this module), feel free to ask!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","I'm sorry, I can only answer questions related to image processing.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
Can you help me with my math homework?,"I'm here to assist with topics related to the **Introduction to Image Processing** module, but I'm afraid I can't help with general math homework questions that fall outside this scope. If your question involves mathematical concepts directly applicable to image processing—such as segmentation algorithms, matrix operations for image transformations, or metrics like mean Intersection over Union (mIoU)—please feel free to ask!

If you're working on an image processing assignment and need help with the math involved (e.g., understanding graph cuts, normalization in segmentation, or evaluating results), just provide more details, and I’ll do my best to assist.

For other math-related inquiries, I recommend reaching out to your math tutor or lecturer, or using dedicated academic support services at your university. Let me know how I can help with image processing!","I'm sorry, I can only answer questions related to image processing.

QA about Course-Related Questions","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
Who is the module convenor for COMP2032?,"Based on the provided context, the module convenor for COMP2032 (Introduction to Image Processing) is **Dr. Tissa Chandesa**.

His contact details are:
- **Email:** Tissa.Chandesa@nottingham.edu.my  
- **Office:** Room BB71 Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia

You can also contact the teaching assistant, Mr. Irfan Yaqub, at hcxiy1@nottingham.edu.my if you have questions about the module.",The module convenor for COMPT2032 is Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my).,"MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
Where can I find the lecture slides?,"Based on the retrieved context, I don't have access to information about where the lecture slides are physically located or how they are distributed to students.

For questions about accessing lecture slides, I recommend:

- Checking your university's online learning platform (Moodle, etc.)
- Contacting the module convenor, Dr. Tissa Chandesa, at Tissa.Chandesa@nottingham.edu.my
- Asking the teaching assistant, Mr. Irfan Yaqub, at hcxiy1@nottingham.edu.my
- Visiting Dr. Chandesa's office hours in Room BB71 Block B

They will be able to provide you with the most current information about how to access the lecture materials for this module.


You can contact Dr. Chandesa directly using this email button:

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about Lecture Slides Access&body=Dear Dr. Chandesa,

I'm a student in your Introduction to Image Processing module and I'm trying to find where the lecture slides are available. Could you please let me know where I can access them?

Thank you for your help.

Best regards,
[Your Name]"">Email Dr. Chandesa</a>",You can find the lecture slides on Moodle’s course page.,"slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16● The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17● Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16● The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17● Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!",
What are the office hours for the teaching assistants?,"I'm sorry, but I don't have information about the specific office hours for the teaching assistants in the provided context. 

For details regarding office hours, I recommend reaching out directly to the teaching assistant:

- **Mr Irfan Yaqub**  
  Email: hcxiy1@nottingham.edu.my

You can also contact the module convenor, Dr. Tissa Chandesa, for further guidance or to be directed to the appropriate schedule:

- **Dr. Tissa Chandesa**  
  Email: Tissa.Chandesa@nottingham.edu.my  
  Office: Room BB71 Block B, Malaysia Campus

They will be able to provide you with the most accurate and up-to-date information regarding office hours and availability.


*You can use the following button to email Mr. Irfan Yaqub directly to inquire about his office hours:*

 <a class=""email-btn"" href=""mailto:hcxiy1@nottingham.edu.my?subject=Teaching Assistant Office Hours Inquiry&body=Dear Mr. Yaqub,

I hope this email finds you well. I'm a student in the Introduction to Image Processing module (COMP2032) and I would like to inquire about your office hours for this semester.

Could you please let me know:
1. Your regular office hours schedule
2. Your office location
3. Whether appointments are required or if drop-ins are welcome

Thank you for your assistance.

Best regards,
[Your Name]
[Your Student ID]"">Email Teaching Assistant</a>","I'm sorry, I don’t have access to the office hours for the teaching assistants.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How do I submit my coursework?,"Based on the retrieved context, here's how to submit your coursework for the Introduction to Image Processing (COMP2032) module:

**Group Submission (to be submitted once per group):**
1. **Python codes**: Submit your .py files as a ZIP folder.
2. **Conference paper**: A 6-page PDF document (max 2000 words). Use the provided template `COMP2032-CW-GroupXXX.docx` (replace XXX with your 3-digit group number). Do not change the formatting style. Additional pages beyond 6 will result in a 5% deduction per extra page for the entire group.
3. **Result Images**: Submit as a ZIP folder with specific folder names: ""Output"" and ""Image Processing Pipeline"".

**Individual Submission:**
4. **Peer Assessment Form**: Each group member must submit this separately via the ""Peer Assessment"" link on Moodle. Failure to submit will result in a 10% penalty on your overall group mark.

**Submission Deadline:**  
Friday, 25 April 2025, by 11:59 PM.  
Late submissions are penalized at 5 marks (5% of total) per working day late.

**Feedback:**  
Marks and individual feedback will be returned via Moodle during the week of 2 June 2025.

If you have further questions about submission logistics, please contact the module convenor, Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), or the teaching assistant, Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",You submit your coursework through the Moodle assignment page.,"DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¶  ¶  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both – before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x – 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI’’(1) = (I’(1.5) - I’(0.5))/1  I’(0.5) = (I(1) - I(0))/1   and   I’(1.5) = (I(2) - I(1))/1  \  I’’(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the ﬁlter order.Equivalencing of spatial frequencies is performed by ﬁrst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares ﬁt can be used to create a normalized Butterworth ﬁlter B(u, v, d0', n') (whered0' and n' are the ﬁt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by ﬁtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¢ x y,( ) = F 1- [ ( , , B u v d 0 ¢ n¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¶  ¶  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both – before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x – 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI’’(1) = (I’(1.5) - I’(0.5))/1  I’(0.5) = (I(1) - I(0))/1   and   I’(1.5) = (I(2) - I(1))/1  \  I’’(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the ﬁlter order.Equivalencing of spatial frequencies is performed by ﬁrst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares ﬁt can be used to create a normalized Butterworth ﬁlter B(u, v, d0', n') (whered0' and n' are the ﬁt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by ﬁtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¢ x y,( ) = F 1- [ ( , , B u v d 0 ¢ n¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What is the assessment criteria for the coursework?,"Based on the retrieved context, the assessment criteria for the COMP2032 coursework (which is worth 50% of the overall module grade) is broken down as follows for the **Group Submission (Programming and Conference Paper)**:

*   **Justification of chosen algorithms based on performed literature review:** 15%
*   **Critical analysis and discussion of your findings:** 25%
*   **Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result:** 30%
*   **(Implied from context) Functioning Python code / application:** The document states that being ""unable to run codes will result in 0% being awarded,"" indicating that a significant portion of the remaining 30% is allocated to a working software solution.

The deliverables for this group coursework are:
1.  Python code files (submitted as a .ZIP folder).
2.  A 6-page conference paper (2000 words maximum, in PDF format). Submitting additional pages will result in a mark deduction.

**Important Note on the In-Lab Test:**
The overall module assessment is 100% coursework, split into two parts:
1.  **Group Programming and Conference Paper:** 50% (detailed above).
2.  **Individual In-Lab Test:** 50%. This test will focus on theories covered in the lectures and practical exercises. It is scheduled for **29 April 2025, from 2:30pm to 3:30pm**.


**Source:** The assessment breakdown and details are taken from the module's official assessment sheet and supporting documents (`Assessment Sheet COMP2032 Coursework 2025.pdf`).

For the most precise and detailed marking scheme for the code itself, it is highly recommended to consult the official rubric provided by the module convenor, Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), or the teaching assistant, Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my). They can provide the specific criteria used to evaluate the functionality and quality of the Python application.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The assessment criteria is as follows:
+Python code(s): 30% [unable to run codes will result in 0% being awarded]
+Conference Paper:
− Justification of chosen algorithms based on performed literature review: 15%
− Critical analysis and discussion of your findings: 25%
− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%.
+In-Lab Test Individual (50%) - Focuses on theories covered in the lectures and practical exercises","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction",
Can I get an extension for the coursework?,"Based on the retrieved context, I can see that the coursework submission deadline for Introduction to Image Processing (COMP 2032) is **Friday, 25 April 2025, by 11:59pm**.

However, the provided context does not include any information about extension policies or procedures. This is a matter that falls outside the scope of image processing topics and is related to administrative and assessment rules.

For questions about extensions, you should directly contact the module convenor:

**Dr. Tissa Chandesa**  
Email: Tissa.Chandesa@nottingham.edu.my  
Office: Room BB71 Block B, Malaysia Campus

He will be able to provide you with the official university policy on extensions and guide you through the process if applicable. It's best to reach out to him as soon as possible with your request.


*You can use the following button to contact Dr. Chandesa directly about your extension request:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Coursework Extension Request - COMP2032 Introduction to Image Processing&body=Dear Dr. Chandesa,

I am writing to request an extension for the COMP2032 Introduction to Image Processing coursework due on Friday, 25 April 2025.

[Please briefly explain your circumstances here]

Could you please advise on the extension policy and whether my request can be considered?

Thank you for your consideration.

Best regards,
[Your Name]
[Your Student ID]"">Email Dr. Chandesa</a>","I’m not able to help you with an extension. You should contact the module convenor (Dr. Tissa Chandesa) and request a coursework extension, providing a reason for it.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n » |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n — 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 — 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2” quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is—using equation (4)Proof. The points in E for whichn Sq=1 i=0form ann — 1 dimensional subset of E, which we will call P. We can find the n — 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)” (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‘V,,/Vp of the volume|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n » |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n — 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 — 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2” quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is—using equation (4)Proof. The points in E for whichn Sq=1 i=0form ann — 1 dimensional subset of E, which we will call P. We can find the n — 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)” (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‘V,,/Vp of the volume|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the room number for the lectures?,"I'm sorry, but I don't have that information in my current context. For specific details about lecture room numbers or the schedule, I recommend you check the official module information on Moodle or contact the module convenor, Dr. Tissa Chandesa, directly at Tissa.Chandesa@nottingham.edu.my. He will be able to provide you with the most accurate and up-to-date information.


For the most accurate information about lecture room locations, I recommend contacting the module convenor directly:

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Lecture Room Inquiry&body=Dear Dr. Chandesa,

I would like to inquire about the room number for the Introduction to Image Processing lectures. Could you please provide this information?

Thank you,
[Your Name]
[Your Student ID]"">Send email</a>","The lectures are in F3A04 (Block F3, Level A, Room 04) on Wednesdays (09:00 to 11:00). Labs are in BB80 (Block B, Level B, Room 80) on Tuesdays (14:00 to 16:00)","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
How many people are in the group for the coursework?,"I'm sorry, but I cannot answer that question based on the provided context. The retrieved materials focus on image processing concepts like histogram backprojection, segmentation, and assessment criteria, but they don't contain information about group sizes for the coursework.

For specific details about group composition for the COMP2032 coursework, please contact:

- **Dr. Tissa Chandesa** (Module Convenor)
- Email: Tissa.Chandesa@nottingham.edu.my
- Office: Room BB71 Block B, Malaysia Campus

- **Mr Irfan Yaqub** (Teaching Assistant)
- Email: hcxiy1@nottingham.edu.my

They will be able to provide you with the most accurate and up-to-date information about coursework requirements and group arrangements.


*You can use the following button to send an email to Dr. Chandesa:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about COMP2032 Coursework Group Size&body=Dear Dr. Chandesa,

I hope this email finds you well. I have a question regarding the COMP2032 Introduction to Image Processing coursework:

Could you please clarify how many people should be in each group for the coursework assignment?

Thank you for your assistance.

Best regards,
[Your Name]
[Your Student ID]"">Email Dr. Chandesa</a>",You need to form a group of four people for the coursework.,"30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What programming language is used in this module?,"Based on the provided context from the lab sheets, this module uses **Python** as the programming language for image processing tasks. Specifically, the OpenCV and Matplotlib libraries are utilized for image operations and data visualization, respectively.

If you have any more questions about Python, OpenCV, or image processing concepts, I'm happy to help!

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 1A.pdf"" target=""_blank"">Lab 1A</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",This module provides practical experience of image processing using Python.,"import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.● Aliasing using as. This modifies the name of the module during referencing. It is done when the module’s name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:● Read it in using imread.● Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).● Write it to a new file using imwrite.shape can provide basic information about the image.● Use shape on the image and print the results. How many values do you get, and what|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).● Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).● Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.● Using import. The entire module is imported, and the module must be referenced when used.● Using from… import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The","import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.● Aliasing using as. This modifies the name of the module during referencing. It is done when the module’s name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:● Read it in using imread.● Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).● Write it to a new file using imwrite.shape can provide basic information about the image.● Use shape on the image and print the results. How many values do you get, and what|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).● Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).● Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.● Using import. The entire module is imported, and the module must be referenced when used.● Using from… import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The",
Where can I find the datasets for the coursework?,"Based on the retrieved context from the course materials, the datasets for your coursework appear to be located in a specific directory structure on your local machine or the system you are using for the module. 

According to the provided code snippet from `DataLoader.py` (likely from Laboratory Sheet 9 - CNN), the dataset for the MNIST example is expected to be found at:
- `""./MNIST/mnist_train/""` for the training partition
- `""./MNIST/mnist_test/""` for the test partition

This suggests that you should have a folder named `MNIST` in your current working directory, containing the `mnist_train` and `mnist_test` subfolders.

**If you cannot locate these datasets:**
1. They may have been provided to you via the module's Moodle page or another university system.
2. You might need to download them from an official source as instructed in your coursework brief.

For specific guidance on where to find or download the exact datasets required for your assignment, I recommend:
- Checking the Moodle page for the ""Introduction to Image Processing"" module
- Reviewing your coursework specification document
- Contacting the teaching assistant, Mr. Irfan Yaqub (Email: hcxiy1@nottingham.edu.my), for direct assistance with dataset access

The module convenor, Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), could also provide clarification if needed.


*Since I don't have specific information about dataset locations, I recommend contacting Dr. Chandesa directly:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about coursework datasets&body=Dear Dr. Chandesa,

I am a student in your Introduction to Image Processing module and I'm trying to locate the datasets needed for our coursework assignments. Could you please provide information on where to access these datasets?

Thank you for your assistance.

Best regards,
[Your Name]"">Email Dr. Chandesa</a>",You can download the datasets from Moodle’s coursework page.,"3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder’s name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image’s path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder’s name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image’s path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
