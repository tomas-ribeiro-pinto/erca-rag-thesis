input,actual_output,expected_output,retrieval_context,context,source_file
What is Thresholding?,"Thresholding is a simple segmentation technique that classifies image pixels into two classes‚Äîforeground and background‚Äîbased on their intensity. It converts a grayscale (or color) image into a binary image, where pixels are set to 0 or 1 (often 0 or 255) depending on whether they are below or above a chosen threshold T.

- Basic idea (global thresholding): choose a single value T for the whole image. For a dark object on a light background, a pixel is labeled ‚Äúobject‚Äù if its intensity < T; otherwise it‚Äôs ‚Äúbackground.‚Äù The inequality is reversed if the object is lighter than the background. This yields a binary image.
- Automatic global thresholding (Otsu): automatically selects T by maximizing the separability between foreground and background intensities (minimizing intra-class variance).
- Adaptive (local) thresholding: T varies across the image (computed per neighborhood/window) to handle non-uniform illumination.
- Hysteresis thresholding (in edge detection): uses two thresholds (high and low) to keep weak responses only if they connect to strong ones, improving edge continuity (as in the Canny detector).
- Notes: 
  - Binary images may still require noise removal and enhancement (e.g., morphological operations).
  - The concept extends to color by defining ranges or sets of color values corresponding to the object class.

If you‚Äôd like, I can show a short OpenCV example using THRESH_BINARY, THRESH_OTSU, and adaptiveThreshold.

Sources: Lecture 4 ‚Äì Thresholding & Binary Images; Lab Sheet 4; Lecture 6A ‚Äì Derivative and Edges.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

",Thresholding is a method of image segmentation that creates binary images. It works by setting a threshold value (T) and classifying pixels as either object or background based on whether their brightness is above or below this threshold.,"University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.",
What is the difference between Otsu and Rosin thresholding methods?,"Short answer:
- Otsu: Assumes the image histogram is roughly bimodal (foreground + background). It searches all possible thresholds and picks the one that minimizes the within-class variance (equivalently maximizes the between-class variance). Best when there are two well-separated modes.
- Rosin: A unimodal ‚Äúknee‚Äù/‚Äúelbow‚Äù method. It finds the histogram peak, draws a straight line to the end of the histogram tail, and chooses the bin that is farthest from this line (the knee) as the threshold. Best when the histogram is unimodal and decays toward the background.

Key differences:
- Histogram assumption:
  - Otsu: Bimodal two-class model.
  - Rosin: Unimodal with a long tail (e.g., skewed distributions).
- How threshold is chosen:
  - Otsu: Variance-based optimization over all thresholds.
  - Rosin: Geometric ‚Äúfarthest-from-line‚Äù knee detection.
- Typical use:
  - Otsu: Global binarization when foreground/background occupy sizable, separable ranges.
  - Rosin: Cases where a single dominant mode exists, such as histograms of edge/gradient magnitudes or text images with one dominant intensity and a small amount of the other.
- Behavior with class imbalance or overlap:
  - Otsu can be biased when one class is very small or variances are very unequal.
  - Rosin does not require two modes; it often handles skew/unbalanced cases better by finding the histogram‚Äôs knee.
- Both are global (non-adaptive) methods.

Example intuition:
- Document image with clear dark text on bright background (two peaks): Otsu usually works well.
- Gradient magnitude histogram (often unimodal with a tail): Rosin is commonly preferred to separate ‚Äútrue edges‚Äù (tail) from ‚Äúnon-edges‚Äù (peak).

Sources:
- Lec-4-Thresholding and Binary Images (Otsu vs Rosin; Rosin‚Äôs unimodal method and its ‚Äúpeak-to-tail line‚Äù knee rule; examples on edge magnitude histograms)
- Lab 4: Thresholding & Binary Images (global thresholding and Otsu usage)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","Otsu's method is used for bimodal histograms, assuming two peaks that can be separated by a single threshold. Rosin's method is for unimodal histograms, where there's only one peak, and it finds the threshold by drawing a line from the peak to the furthest bin.","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f‚Äô =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¬¢ .¬∞ , ,* ,‚Ä¢ Finds the peak‚Ä¢ Draws a line from there to the top of thefurthest bin‚Ä¢ Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin‚Äôs unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f‚Äô =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods‚Ä¢ Imaging conditions and object properties can vary within a single image as well as across sets of images'e '‚Ä¢ Histograms can be too complex for any method‚Äôs|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f‚Äô =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¬¢ .¬∞ , ,* ,‚Ä¢ Finds the peak‚Ä¢ Draws a line from there to the top of thefurthest bin‚Ä¢ Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin‚Äôs unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f‚Äô =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods‚Ä¢ Imaging conditions and object properties can vary within a single image as well as across sets of images'e '‚Ä¢ Histograms can be too complex for any method‚Äôs|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a structuring element in morphological operations?,"A structuring element (SE) is a small binary pattern used to ‚Äúprobe‚Äù an image during morphological operations (like dilation, erosion, opening, and closing). Think of it as a shape you slide over the image to decide which pixels to add or remove.

Key points:
- It is a binary mask that identifies pixels rather than weighting them (unlike convolution filters).
- Its shape, size, and anchor (origin) control the effect and direction of the operation; the origin need not be at the center.
- Larger SEs produce stronger effects; repeating operations with a small SE can approximate a larger one.
- SEs can be symmetric or asymmetric (e.g., elongated for directional effects).

Common shapes and uses:
- Square/rectangle: general-purpose smoothing and gap filling.
- Disk/ellipse: more isotropic effects; good for rounding corners.
- Cross: emphasizes connections along horizontal/vertical directions.

Example (OpenCV): getStructuringElement with RECT, ELLIPSE, or CROSS creates SEs of sizes like 3√ó3, 5√ó5, 7√ó7.

Sources: University of Nottingham COMP2032 materials on Morphology (Lec-5-Morphology; Lab 5; Practicals ‚Äì Morphology). If you‚Äôd like, I can show a short code snippet creating and visualizing different SEs.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

",A structuring element is a small matrix or kernel used to probe an image in morphological operations like erosion and dilation. It defines the neighbourhood of a pixel and is used to modify the pixel's value based on the values of its neighbours.,"if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt",
Explain the process of Histogram Backprojection.,"Histogram Backprojection is a color-based localization method that finds where in an image the colors of a target object occur together.

Core idea:
- Build a color histogram of the target (the model).
- Compare it to the image‚Äôs colors to weight each color by how characteristic it is of the target.
- ‚ÄúBackproject‚Äù these weights onto every pixel to make a likelihood map, then sum evidence over a window the size of the object to find peaks (likely locations).

Step-by-step:
1) Choose and quantize a color space
- Use a discrete color space (e.g., RGB) and bin colors into a 3D histogram.
- Histograms are invariant to translation and rotation about the viewing axis and change only slowly under scale, angle, and occlusion.

2) Build histograms
- Model histogram M: from a cropped example of the target.
- Image histogram I: from the full image (or from a region if searching locally).

3) Compute a ratio histogram
- For each color bin k, compute a weight R(k) ‚âà M(k) / (I(k) + Œµ), typically clipped to [0, 1].
- Intuition: colors common in the target but rare in the image get large weights; colors that are common everywhere (e.g., white) get small weights. This ‚Äúrelies more on those colors that show up about as much as expected than those which show up much more.‚Äù

4) Backproject to form a likelihood map
- For each pixel x in the image, find its quantized color bin k and set B(x) = R(k).
- The result B is a grayscale map: brighter (or higher) means the pixel‚Äôs color is more indicative of the target.

5) Convolution (spatial aggregation)
- Convolve B with a window approximately the size of the target (e.g., 32√ó32).
- Peaks in the convolved image indicate locations where many target-like colors co-occur in the right-sized neighborhood.
- The highest peak is taken as the most likely target location.

Example (from the notes):
- Target: a blue-and-white striped shirt. Blue appears in few non-target regions, so its bins get high R(k) and produce strong responses. White appears in many objects, so it receives low weights and yields weak responses. After convolution, a clear peak appears at the shirt‚Äôs location.

Why it works well:
- Robust to background clutter: ubiquitous colors are down-weighted by the ratio.
- Robust to occlusion: even with only part of the object visible, enough target-colored pixels can produce a peak after convolution.
- Efficient: with small windows (e.g., 32√ó32), it can run in real time and is useful for tracking.

Limitations and notes:
- If other objects share the same distinctive color distribution as the target, multiple peaks can occur.
- Performance depends on sensible color quantization and stable illumination.
- Severe occlusion or prior occlusion in the model example can reduce effectiveness for some targets.

Source: Swain and Ballard, ‚ÄúColor Indexing‚Äù (see sections on color histograms, ratio histogram backprojection, and the convolution step; figures illustrating the striped shirt example and occlusion results) [./documents/Swain1.pdf]. If you want, I can walk you through implementing this step-by-step in code.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

","Histogram backprojection is a technique for locating an object in an image based on a model histogram of its appearance (e.g., color). Each pixel is assigned a probability based on how well it matches the model histogram, producing a probability map where high values indicate likely object locations.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main components of a CNN?,"Key components of a Convolutional Neural Network (CNN):

- Input layer
  - Takes image tensors (e.g., H√óW√óC).

- Convolutional layers
  - Learnable filters/kernels slide over the image to extract features (edges, textures, shapes).
  - Core hyperparameters: number of filters, kernel size, stride, and zero-padding.
  - Output is a set of feature maps. 
  - According to the course material, these filter hyperparameters are specified for the convolutional layer.

- Activation functions
  - Introduce nonlinearity after each convolution (commonly ReLU; also Leaky ReLU, etc.).
  - ReLU is highlighted in the notes as the commonly used transfer function.

- Pooling layers
  - Downsample feature maps (e.g., max or average pooling) to reduce spatial size and overfitting, and add some translation invariance.

- Normalization layers (often used)
  - Batch Normalization or Layer Normalization to stabilize and speed up training.

- Fully connected (dense) layers
  - Combine high-level features for prediction, typically near the end of the network.

- Output layer
  - Uses task-appropriate activation (e.g., Softmax for multi-class classification, Sigmoid for binary/multi-label).

- Regularization and other common components (as needed)
  - Dropout, data augmentation, residual/skip connections, and (for segmentation/upsampling tasks) transposed convolutions or upsampling layers.

Typical CNN block: [Conv ‚Üí ReLU ‚Üí (BatchNorm) ‚Üí Pool] repeated several times ‚Üí [Fully Connected] ‚Üí [Output (e.g., Softmax)].

Sources: Lec-9-CNN.pdf; Practicals with Solutions - CNN.pdf.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","A Convolutional Neural Network (CNN) is composed of several layers, including the input layer, convolutional layer, pooling layer, activation function, fully connected layer, and output layer.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
What is the purpose of the pooling layer in a CNN?,"In a CNN, the pooling layer downsamples the spatial dimensions of feature maps. Its main purposes are:
- Reduce the size of feature maps to lower computation and memory costs
- Help control overfitting by reducing the number of activations passed to later layers
- Provide some translation invariance by summarizing local neighborhoods
- Increase the effective receptive field for subsequent layers

Common choices:
- Max pooling: keeps the strongest response in each window (e.g., 2√ó2 with stride 2)
- Average pooling: averages values in the window

Example: for 28√ó28 MNIST features, a 2√ó2, stride-2 max-pool reduces each feature map to 14√ó14. In our course materials, pooling is typically placed after a convolution + activation block to ‚Äúreduce the size of feature map outputs‚Äù (see CNN practicals/lecture notes). Sources: Lec-9-CNN.pdf; Practicals with Solutions - CNN.pdf. If you need, I can show how this affects tensor shapes through a toy forward pass.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Laboratory Sheet 9 - CNN.pdf"" target=""_blank"">Laboratory Sheet 9   CNN</a>

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","The pooling layer occurs after a convolutional layer and its purpose is to reduce the dimensionality of the resulting feature map. Common pooling operations include max pooling, average pooling, and global average pooling.","out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f¬∞ University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f¬∞ University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What is the difference between erosion and dilation in morphological image processing?,"Erosion and dilation are the two fundamental binary morphological operations, and they have opposite effects on the foreground (object) pixels.

- Dilation (grows the foreground)
  - Intuition: Expands object boundaries; small holes and gaps tend to shrink/close; corners round off.
  - Rule: Center the structuring element (SE) at each pixel. If any input pixel under a ‚Äú1‚Äù in the SE is foreground, the output pixel becomes foreground.
  - Typical uses: Bridge small breaks, fill small gaps, connect nearby components; can be used for edge detection by subtracting the original from the dilated image to get outer boundaries.
  - Example from the labs: Using dilation to fill gaps in broken characters (text_broken.tif). [Lab 2.pdf; Lec-5-Morphology.pdf]

- Erosion (shrinks the foreground)
  - Intuition: Contracts object boundaries; small protrusions disappear; small objects may vanish; holes grow.
  - Rule: Center the SE at each pixel. If any ‚Äú1‚Äù of the SE falls on background, the central pixel is set to background. Equivalently, the SE must fit entirely within the foreground to keep a pixel as foreground.
  - Typical uses: Separate touching objects, remove small noise components; can be used for edge detection to get inner boundaries.
  - Example from the labs: Eroding Squares.tif with different SEs shows stronger effect on small squares than large ones. [Lab 2.pdf; Lec-5-Morphology.pdf]

Key relationships and notes
- Duality: Eroding the foreground is equivalent to dilating the background with the same SE (and vice versa). [Lec-5-Morphology.pdf]
- Edge detection: 
  - Dilation then subtract original ‚Üí outer edge pixels remain.
  - Original then subtract erosion ‚Üí inner edge pixels remain. [Lec-5-Morphology.pdf]
- SE shape/size control the effect: Larger SEs produce stronger growth/shrink; elongated/asymmetric SEs affect objects directionally (e.g., connect gaps horizontally but not vertically). [Lec-5-Morphology.pdf]
- Combined operations:
  - Opening = erosion followed by dilation (removes small objects, smooths contours).
  - Closing = dilation followed by erosion (fills small holes/gaps, smooths contours). [Lec-5-Morphology.pdf]

In short: dilation grows and connects foreground regions; erosion shrinks and separates them. The choice of structuring element determines how and where these changes occur.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

","Erosion removes pixels on object boundaries, shrinking bright regions and enlarging dark ones. Dilation adds pixels to the boundaries of objects, enlarging bright regions and shrinking dark ones.","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.‚óè Read in the Squares.tif image from Moodle.‚óè Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.‚óè Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)‚óè Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¬¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.‚óè Read in the Squares.tif image from Moodle.‚óè Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.‚óè Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)‚óè Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¬¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and",
What is the Hough Transform used for?,"The Hough Transform is used to detect parametric shapes‚Äîmost commonly straight lines and circles‚Äîin images. It does this by:
- Converting edge points from the image into a parameter space (e.g., for lines, the (œÅ, Œ∏) space).
- Letting each edge point ‚Äúvote‚Äù for all shape parameters that could pass through it.
- Identifying peaks in the accumulator (votes), which correspond to shapes present in the image.

Why use it:
- Finds lines (and other shapes) even with noise, gaps, or partial occlusion.
- Detects multiple instances of a shape.
- Typically applied after edge detection (e.g., Canny) and returns line parameters (œÅ, Œ∏) for drawing the detected lines.

Examples: detecting the grid lines in a Sudoku image, road lane markings, document text baselines, and circles such as coins.

Sources: Lec-6B-Hough Transform.pdf; Lab 6.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6B-Hough Transform.pdf"" target=""_blank"">Lec 6B Hough Transform</a>

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","The Hough Transform is a feature extraction technique used to find instances of objects with a certain shape, such as lines or circles, within an image. It works by transforming points from the image space to a parameter space, where peaks correspond to the parameters of the detected shapes.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.‚óè Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a ‚Äúvoting‚Äù procedure on the points which are part of the edges in the image.‚óè Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.‚óè Apply the Hough Line Transform on the edge-detected image using HoughLines.‚óè To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (ùúå,ùúÉ) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don‚Äôt know the line parameters ‚Äì we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line‚Äôs   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.‚óè Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a ‚Äúvoting‚Äù procedure on the points which are part of the edges in the image.‚óè Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.‚óè Apply the Hough Line Transform on the edge-detected image using HoughLines.‚óè To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (ùúå,ùúÉ) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don‚Äôt know the line parameters ‚Äì we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line‚Äôs   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines",
What is the Livewire segmentation algorithm?,"Livewire (also called Intelligent Scissors) is an interactive boundary-tracing algorithm for image segmentation. The key idea is to let the user roughly indicate a region of interest (ROI) by placing ‚Äúanchor‚Äù points, while the algorithm automatically ‚Äúsnaps‚Äù the path between anchors to the strongest nearby object boundaries. It does this by formulating boundary finding as a shortest-path problem on a graph built from the image.

How it works (high level)
- Image processing: Compute edge evidence such as gradient magnitude and zero-crossings (from the Laplacian) so that strong, likely boundaries are favored.
- Convert image to a graph: Treat pixels as nodes; connect each pixel to its neighbors (e.g., 8-connectivity). Assign each edge a cost inversely related to boundary likelihood (strong edges ‚Üí low cost).
- Shortest-path search: From the last anchor point, run a shortest-path algorithm (typically Dijkstra‚Äôs) to the current cursor location. The lowest-cost path is displayed live and ‚Äúsnaps‚Äù to boundaries under the cursor.
- Interactive visualization: As you move the mouse, the path updates in real time. Clicking adds a new anchor and fixes that path segment.
- Display and event handling: The interface shows the image, the livewire path, and handles mouse clicks/moves.

Why gradient and zero-crossings?
- Gradient magnitude: prefers high-contrast transitions (strong edges).
- Zero-crossings: encourages alignment with actual edge locations (Laplacian sign changes) to avoid drifting to nearby but incorrect high gradients. Combining both improves robustness to noise and texture.

What it‚Äôs based on
- Graph theory: The problem is cast as finding minimum-cost paths on a weighted pixel graph using Dijkstra‚Äôs algorithm. The cost function typically combines gradient strength, zero-crossing evidence, and sometimes a smoothness/directionality term.

Typical workflow example
1) Click an anchor on the object boundary.
2) Move the mouse along the boundary; the livewire path snaps to the nearest strong edge.
3) Click to set another anchor. Repeat until the object is outlined.
4) Close the loop to obtain the segmented contour/ROI.

Strengths
- Precise, user-steered segmentation with less manual tracing than freehand.
- Robust to clutter compared with pure thresholding because it follows strong boundaries.
- Updates live, giving immediate feedback.

Limitations
- Performance and accuracy depend on clear edge evidence; very weak or missing boundaries can mislead the path.
- Can require multiple anchors in textured or low-contrast regions.

Notes for this module/lab
- Livewire is not available in OpenCV by default; the lab provides a Python implementation (LiveWireAlgo.py) that uses matplotlib, scikit-image, heapq, itertools, and standard math utilities.
- Implementation steps in the lab: Image processing ‚Üí graph construction ‚Üí shortest path ‚Üí interactive visualization ‚Üí display ‚Üí event handling.

References from the course materials
- E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, SIGGRAPH ‚Äò95 (demonstrates ‚Äúsnapping‚Äù behavior).
- COMP2032 Laboratory Sheet: Livewire Segmentation Algorithm.
- Practicals ‚Äì Interactive Segmentation (and solutions).

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

<a href=""http://localhost:5003/documents/Practicals - Interactive Segmentation.pdf"" target=""_blank"">Practicals   Interactive Segmentation</a>

","Livewire, also known as ""Intelligent Scissors,"" is an interactive image segmentation algorithm. It allows a user to trace an object's boundary, and the algorithm finds the minimum-cost path that follows object boundaries. It's based on graph theory and uses Dijkstra's algorithm to find the shortest path between user-selected points.","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the GrabCut algorithm?,"GrabCut is an interactive foreground/background extraction algorithm that uses iterated graph cuts to segment an object from an image with minimal user input. The typical workflow is:
- The user draws a loose rectangle (or a few brush strokes) around the object. Pixels outside the rectangle are treated as definite background; pixels inside are initially unknown.
- The algorithm builds color/texture models for foreground and background and formulates an energy with a data term (how well a pixel fits the foreground/background models) and a smoothness term (encouraging coherent regions and respecting edges/contrast).
- A graph-cut optimization labels pixels as foreground or background.
- The models are re-estimated from the new labels and the graph cut is run again‚Äîiterating until convergence.
- Users can add foreground/background scribbles to correct mistakes, and the optimization respects these constraints.
- A ‚Äúborder matting‚Äù step estimates an alpha matte along object boundaries to handle mixed/blurred edge pixels, producing smooth mattes with minimal background bleed.

Why it‚Äôs notable:
- It combines color (texture) and edge information and requires less interaction than earlier tools (often just a rectangle), yet achieves accuracy close to full two-lasso Graph Cut setups.
- Reported results show high-quality mattes and low error rates with modest user effort.

Typical use cases include photo editing and background removal.

Sources:
- Rother, Kolmogorov, and Blake, ‚ÄúGrabCut: Interactive foreground extraction using iterated graph cuts‚Äù (2004). [./documents/grab_cut.pdf]
- Lecture notes on Interactive Segmentation (GrabCut overview and examples). [./documents/Lec-8-Interactive Segmentation.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

","GrabCut is an interactive foreground extraction algorithm that uses iterated graph cuts. The user draws a rectangle around an object, and the algorithm automatically extracts the object from the background.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single ‚Äúout- side‚Äù lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images‚Äô, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single ‚Äúout- side‚Äù lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images‚Äô, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original",
What is histogram equalization?,"Histogram equalization is a contrast-enhancement technique that redistributes an image‚Äôs pixel intensities so that their histogram becomes approximately uniform. By spreading frequently occurring intensity values across the full dynamic range, it makes dark regions brighter (or bright regions darker) to reveal detail that was compressed into a narrow range.

How it works (global HE):
- Compute the image histogram and normalize it to get p(rk), the probability of each grey level rk.
- Compute the cumulative distribution function (CDF): T(rk) = Œ£j‚â§k p(rj).
- Map each input level rk to an output level sk using a lookup table: sk = round((L‚àí1)¬∑T(rk)), where L is the number of grey levels (e.g., 256).
- The result uses the full intensity range and has an approximately flat histogram.

Key points:
- Nonlinear mapping: different from linear contrast stretching; it uses the CDF, not a straight line.
- Benefits: improves global contrast, especially when the image‚Äôs intensities occupy a narrow band.
- Limitations: can over-amplify noise, create unnatural look, or shift colors if applied to RGB channels directly. For color images, equalize the luminance channel (e.g., Y in YCrCb) or use CLAHE (adaptive HE) to limit over-contrast.
- In OpenCV: use equalizeHist for 8-bit grayscale; for color, convert to YCrCb/HSV, equalize the luminance/value channel, then convert back.

Quick validity checks:
- Mapping is monotonic non-decreasing.
- Output uses most of the available intensity range.
- The normalized output histogram sums to 1.0.

References:
- Lecture 2: Histogram (Whole Image Understanding) ‚Äì theory and algorithm for histogram equalization, CDF-based mapping, example table and scaling to [0, L‚àí1]. [./documents/Lec-2-Histogram (Whole Image Understanding).pdf]
- Practicals ‚Äì Histogram Equalisation steps and example. [./documents/Practicals - Histogram (Whole Image Understanding).pdf]
- Lab 2 ‚Äì Using equalizeHist and comparing with contrast stretching. [./documents/Lab 2.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

<a href=""http://localhost:5003/documents/Practicals - Histogram (Whole Image Understanding).pdf"" target=""_blank"">Practicals   Histogram (Whole Image Understanding)</a>

",Histogram equalization is a technique used to improve the contrast of an image by redistributing the pixel intensities to achieve a more uniform distribution. This is done by computing the cumulative density function (CDF) and using it as a look-up table to map input pixel values to output pixel values.,"the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee ‚ÄîACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 ‚Äì 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let‚Äôs BREAK this down‚Ä¶So, in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their","the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee ‚ÄîACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 ‚Äì 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let‚Äôs BREAK this down‚Ä¶So, in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their",
What is the difference between sampling and quantization in digital image formation?,"Sampling vs. Quantization in digital image formation:

- What they do
  - Sampling: Discretises space. We pick sample points on the continuous scene to form a grid of pixels. It sets how many pixels the image has.
  - Quantization: Discretises intensity (and color). We map continuous brightness/color values at each sampled pixel to a finite set of levels (e.g., 256 greys).

- What they control
  - Sampling ‚Üí spatial resolution (how fine the detail you can represent).
  - Quantization ‚Üí radiometric/grey-level (or color) resolution (how finely you distinguish brightness/color differences).

- Typical issues if done too coarsely
  - Under-sampling ‚Üí aliasing (jagged edges, moir√©), loss of fine detail; governed by Nyquist: sample at least twice the highest spatial frequency.
  - Coarse quantization ‚Üí banding/contouring and loss of subtle intensity or color gradations.

- Units/parameters
  - Sampling: pixel pitch, pixels per inch (PPI), image width√óheight.
  - Quantization: bit depth (e.g., 8-bit greyscale = 256 levels; 8 bits per channel RGB = 24-bit color).

- Quick example
  - A 4000√ó3000 image (sampling) stored in 8-bit greyscale (quantization) has high spatial resolution but only 256 grey levels. If you reduce sampling to 500√ó375, edges may alias; if you reduce quantization to 3 bits (8 levels), you‚Äôll see visible banding, even at 4000√ó3000.

Cited context: Sampling digitises spatial coordinates and determines spatial resolution; quantisation digitises the light intensity function and determines grey-level/color (radiometric) resolution; under-sampling below the Nyquist rate causes aliasing. [Lec-1B-Digital Images and Point Processes.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lec-11-Finale and Revision.pdf"" target=""_blank"">Lec 11 Finale and Revision</a>

","Sampling is the digitization of spatial coordinates, determining the spatial resolution of the image (number of pixels). Quantization is the digitization of the light intensity function, determining the number of grey levels or colors that can be represented at each pixel.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE‚Ä¶Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE‚Ä¶Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:¬´< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‚Äò@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression‚Ä¢ Map vector values (R,G,B) onto scalar values‚Ä¢ Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f‚ÄôBuilding a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f‚ÄôBuilding a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE‚Ä¶Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE‚Ä¶Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:¬´< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‚Äò@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression‚Ä¢ Map vector values (R,G,B) onto scalar values‚Ä¢ Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f‚ÄôBuilding a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f‚ÄôBuilding a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2",
What is aliasing in the context of image sampling?,"Aliasing is what happens when an image is sampled or resampled at too low a rate, so different spatial patterns become indistinguishable after sampling. In other words, the sampled image and the true (continuous) image produce the same samples even though they differ, so the reconstruction by the human visual system shows false structure.

Key points:
- Cause: Undersampling relative to the scene‚Äôs highest spatial frequency (violating the Nyquist rate: need ‚â• 2 samples per highest frequency).
- Visual artifacts: Jagged/‚Äústaircase‚Äù edges, moir√© patterns, false ripples or banding, and texture that ‚Äúbeats‚Äù or reverses when downscaling.
- When it happens: During image acquisition with insufficient sensor resolution, or during resizing/downsampling without proper prefiltering.
- How to prevent: Apply anti-aliasing (low-pass) filtering to smooth high frequencies before sampling/resampling; choose a sufficiently high sampling rate; use proper resampling filters when downscaling.

Reference: Lec-1B-Digital Images and Point Processes.pdf (slides on aliasing, Nyquist rate, anti-aliasing).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",Aliasing occurs when an image is under-sampled (sampled at a rate below the Nyquist rate). This results in artifacts where two different signals (images) become indistinguishable when sampled.,"fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing ‚Ä¢ Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original ‚Ä¢ Smooth out high frequency signals before sampling so its impossible to ‚Äúsee‚Äù the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? ‚Ä¢ When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels‚Ä¢ Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can‚Äôt increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition ‚Äúseamlessly‚Äù in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass Ô¨Åltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter Ô¨Åtting of a Butterworth low-pass Ô¨Ålter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2","fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing ‚Ä¢ Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original ‚Ä¢ Smooth out high frequency signals before sampling so its impossible to ‚Äúsee‚Äù the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? ‚Ä¢ When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels‚Ä¢ Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can‚Äôt increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition ‚Äúseamlessly‚Äù in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass Ô¨Åltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter Ô¨Åtting of a Butterworth low-pass Ô¨Ålter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2",
How can noise be reduced in an image?,"Great question. In this module we treat image noise mainly as additive, zero-mean noise and reduce it using spatial filtering. Here are the main approaches you can use:

1) Average multiple images of the same scene
- If you have several frames of the same static scene, compute the pixel-wise mean.
- Because the noise is random with mean ~0, averaging cancels it out while preserving the true signal.
- This is the most effective way to reduce random (e.g., Gaussian) noise when multiple images are available.
- Source: Linear Filters lecture notes (Gaussian noise and multi-image averaging)

2) Single-image spatial filtering (linear)
- Mean (box/average) filter:
  - Convolve the image with a uniform kernel (e.g., 3√ó3, 5√ó5, 7√ó7).
  - Reduces noise but also blurs edges and fine details.
  - In OpenCV: blur(img, ksize=(3,3)), etc.
- Gaussian filter:
  - Convolve with a Gaussian kernel; better preserves structure than a box filter for Gaussian noise.
  - Choose œÉ (standard deviation) and a kernel size large enough to cover the Gaussian support (rule of thumb: kernel size about 6œÉ+1, odd).
  - In OpenCV: GaussianBlur(img, ksize, sigmaX=œÉ). In Lab 3 you‚Äôre asked to try œÉ = 0.5, 1.0, 1.5 and adjust the mask size accordingly.
- Sources: Linear Filters lecture notes; Lab 3 (Linear Filtering via convolution, mean and Gaussian filtering)

3) Single-image spatial filtering (non-linear)
- Median filter (especially effective for salt-and-pepper noise):
  - Replaces each pixel with the median of its neighborhood, removing isolated white/black specks while preserving edges better than mean filtering.
  - In OpenCV: medianBlur(img, ksize) with odd ksize (e.g., 3, 5, 7).
- Source: Covered under non-linear filtering in the module (see Lab 3 context)

Practical tips
- Start small: Try 3√ó3 kernels and increase gradually (5√ó5, 7√ó7) to balance noise removal against blur.
- Match the filter to the noise:
  - Gaussian noise: Gaussian filter (or averaging if multiple frames).
  - Salt-and-pepper noise: median filter.
- Boundary handling: Libraries handle borders for you, but be aware different modes (reflect, replicate) can affect edges slightly.
- Color images: Denoise in each channel or convert to a suitable color space; median on each channel can work well for salt-and-pepper.

Minimal OpenCV examples
- Mean filter:
  - den = cv2.blur(img, (5,5))
- Gaussian filter (œÉ=1.0, size 7√ó7):
  - den = cv2.GaussianBlur(img, (7,7), 1.0)
- Median filter:
  - den = cv2.medianBlur(img, 5)
- Multi-image averaging (N frames img_list):
  - den = np.mean(np.stack(img_list, axis=0).astype(np.float32), axis=0).astype(img.dtype)

If you can tell me the noise type (e.g., Gaussian vs salt-and-pepper) and whether you have one image or multiple frames, I can recommend exact parameters.

References
- Linear Filters lecture notes (Gaussian noise model, convolution, mean and Gaussian filtering): ./documents/Lec-3A-Linear Filters.pdf
- Lab 3: Linear & Non-Linear Filtering (mean, Gaussian, kernel sizes/œÉ guidance): ./documents/Lab 3.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3A-Linear Filters.pdf"" target=""_blank"">Lec 3A Linear Filters</a>

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

","Noise can be reduced using various filtering techniques. For additive noise like Gaussian noise, mean or Gaussian filtering can be effective. For ""salt and pepper"" noise, a median filter is often used.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I‚Äô) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‚Äò‚Äò1‚Äô* means the object has been correctly located, a ‚Äú2‚Äù indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise‚Ä¢ Noise is randomly added to each value‚Ä¢ Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern ‚Äì multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)√óP(x-1,y-1)  + F(0,-1)√óP(x,y-1)  + F(+1,-1)√óP(x+1,y-1)  + F(-1,0)√óP(x-1,y)  + ‚Ä¶  + F(+1,+1)√óP(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise‚Ä¢‚Ä¢Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI‚Ä¢ On average they give the right value‚Ä¢ They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (Œº) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2œÄœÉ  e  (x  --  2œÉ  Œº)  2 Œº28Gaussian NoiseThe level of noise is related to the Gaussian parameter, œÉœÉ = 1œÉ = 10œÉ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I‚Äô) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‚Äò‚Äò1‚Äô* means the object has been correctly located, a ‚Äú2‚Äù indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise‚Ä¢ Noise is randomly added to each value‚Ä¢ Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern ‚Äì multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)√óP(x-1,y-1)  + F(0,-1)√óP(x,y-1)  + F(+1,-1)√óP(x+1,y-1)  + F(-1,0)√óP(x-1,y)  + ‚Ä¶  + F(+1,+1)√óP(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise‚Ä¢‚Ä¢Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI‚Ä¢ On average they give the right value‚Ä¢ They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (Œº) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2œÄœÉ  e  (x  --  2œÉ  Œº)  2 Œº28Gaussian NoiseThe level of noise is related to the Gaussian parameter, œÉœÉ = 1œÉ = 10œÉ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.",
What is the difference between linear and non-linear filters?,"Great question. Here‚Äôs a concise comparison based on our module materials:

- What linear filters are:
  - Operation: Compute a weighted sum of pixel values in a neighborhood (convolution).
  - Properties: Linear and shift-invariant (obey superposition).
  - Typical examples: Mean (box) filter, Gaussian filter.
  - Effect: Smooths/blur the image and tends to attenuate edges and fine details.
  - When useful: Reducing Gaussian-like noise; Gaussian filtering gives gentler, more natural smoothing than a simple mean for the same footprint.
  - Reference: Lab 3 (Linear Filtering: mean and Gaussian) and Lecture 3B notes.

- What non-linear filters are:
  - Operation: Apply a non-linear function to the neighborhood (not a convolution).
  - Properties: Do not obey superposition; often data/adaptation dependent.
  - Typical examples: Median filter, anisotropic diffusion, bilateral filter.
  - Effect: Tend to preserve or even enhance edges while smoothing within regions.
  - When useful: 
    - Median: Very effective for salt-and-pepper (impulsive) noise.
    - Bilateral: Edge-preserving smoothing (reduces noise while keeping sharp boundaries).
    - Anisotropic diffusion: Iterative, edge-aware smoothing controlled by parameters.
  - Reference: Lab 3 (median, anisotropic diffusion, bilateral) and Lecture 3B notes.

- What you‚Äôll observe in practice (same-sized neighborhood):
  - Mean vs Gaussian: Both blur, but Gaussian weights center pixels more, so it preserves structure slightly better than a mean filter of the same size.
  - Median vs mean/Gaussian on salt-and-pepper noise: Median removes isolated black/white specks while keeping edges sharper; mean/Gaussian leave speckle remnants and blur edges.
  - Bilateral/anisotropic diffusion vs Gaussian: Similar denoising inside regions but with sharper edges maintained.

If you‚Äôd like, I can show short OpenCV snippets for blur, GaussianBlur, medianBlur, bilateralFilter, and anisotropic diffusion with suggested parameters from the lab.

Sources:
- COMP2032 Laboratory Sheet 3: Linear & Non-Linear Filtering (Lab 3.pdf)
- COMP-2032 Lecture 3B Non-Linear Filters (Lec-3B-Non-Linear Filters.pdf)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Linear filters, like mean and Gaussian filters, compute a linear function of a set of pixel values. Non-linear filters, such as the median filter, do not follow this linear relationship and tend to preserve or even enhance sharp image changes.","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.‚óè Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.‚óè Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 ‚Äì 0.1), K (0 ‚Äì 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.‚óè Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 ‚Äì 100) and sigma (50 ‚Äì 250). You may keep both sigma values the same for simplicity‚Äôs sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  ‚Ä¢ Median filtering  ‚Ä¢ Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‚Äòesau ad oe ‚Ç¨ THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i‚Ä¢ A false saturation gives a white spot in the image (salt)‚Ä¢ A failed response gives a black spot in the image (pepper)‚Ä¢ Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.‚óè Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.‚óè Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.‚óè Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.‚óè Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.‚óè Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 ‚Äì 0.1), K (0 ‚Äì 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.‚óè Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 ‚Äì 100) and sigma (50 ‚Äì 250). You may keep both sigma values the same for simplicity‚Äôs sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  ‚Ä¢ Median filtering  ‚Ä¢ Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‚Äòesau ad oe ‚Ç¨ THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i‚Ä¢ A false saturation gives a white spot in the image (salt)‚Ä¢ A failed response gives a black spot in the image (pepper)‚Ä¢ Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the",
Explain the concept of superpixels.,"Superpixels are groups of neighboring pixels that are similar in appearance (e.g., color/texture) and are treated as a single, atomic region. They provide a mid-level image representation that reduces complexity: instead of processing millions of individual pixels, many vision algorithms can work on a few hundred or thousand superpixels while still respecting important image boundaries.

Why use superpixels
- Fewer elements to process, so many tasks run faster.
- They tend to align with object boundaries better than a regular grid.
- They are less sensitive to pixel-level noise.

Desirable properties
- Compact and regular shapes.
- Good adherence to true image edges.
- Ability to choose the approximate number/size of superpixels.
- Low computational and memory cost.

How they‚Äôre generated (overview of methods)
- SLIC (Simple Linear Iterative Clustering): clusters pixels in a combined five-dimensional space of color and image coordinates to produce compact, nearly uniform superpixels. It is simple, runs in O(N) time, and typically needs only one main parameter‚Äîthe desired number of superpixels. Quality is commonly measured by boundary recall and under-segmentation error, and SLIC achieves strong results at low computational cost. [SLIC_Superpixels.pdf]
- Graph-based (e.g., Felzenszwalb-Huttenlocher): agglomerative clustering on a pixel graph, fast in practice (O(N log N)) but without explicit control over the number or compactness of superpixels. [SLIC_Superpixels.pdf]
- Other approaches (e.g., normalized cuts, seam/lattice methods) can produce superpixels but may be computationally heavier or depend on good boundary maps. [SLIC_Superpixels.pdf]

Where they‚Äôre used
- Object category recognition and medical image segmentation, where SLIC superpixels have shown good quality and efficiency. [SLIC_Superpixels.pdf]
- Depth estimation, body model estimation, and skeletonization in various pipelines that benefit from region-level processing. [SLIC_Superpixels.pdf]

Quick intuition
If an image has 1,000,000 pixels, segmenting it into ~1,000 superpixels lets later stages operate on 1,000 regions instead of 1,000,000 pixels, typically preserving object boundaries well enough for recognition, tracking, or segmentation tasks.

If you‚Äôd like, I can walk through the SLIC algorithm step by step or compare these methods in more detail.

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","Superpixels are groups of pixels that are similar in color and proximity. They provide a convenient primitive from which to compute local image features, capturing redundancy in the image and reducing the complexity of subsequent image processing tasks.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oÔ¨Äer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eÔ¨Écacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eÔ¨Éciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiÔ¨Åcation model for segmentation. ICCV (2003) 10‚Äì172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oÔ¨Äer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by Ô¨Ånding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oÔ¨Äer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eÔ¨Écacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eÔ¨Éciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiÔ¨Åcation model for segmentation. ICCV (2003) 10‚Äì172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oÔ¨Äer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by Ô¨Ånding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient",
What is the SLIC algorithm for generating superpixels?,"SLIC (Simple Linear Iterative Clustering) is a superpixel algorithm that performs k-means‚Äìlike clustering in a 5D space formed by color and image coordinates. It efficiently groups pixels into compact, nearly uniform regions (superpixels) while letting you directly control how many superpixels you want.

Key ideas:
- Feature space: Each pixel is represented by [L, a, b, x, y], usually using CIELAB color for perceptual uniformity plus its 2D image coordinates.
- Distance metric: Combines color similarity and spatial proximity:
  - D = sqrt( dc^2 + (m/S)^2 * ds^2 )
  - dc = Euclidean distance in Lab color
  - ds = Euclidean distance in image coordinates (pixels)
  - S = grid interval ‚âà sqrt(N/K), where N is number of pixels and K is desired number of superpixels
  - m = compactness parameter (balances adherence to image edges vs. spatial regularity)
- Localized search: Each cluster center only considers pixels in a 2S √ó 2S window around it, which makes the algorithm O(N) and fast in practice.

Algorithm (overview):
1. Choose K (number of superpixels). Compute S = sqrt(N/K).
2. Initialize cluster centers on a regular grid with step S. Optionally shift each center to the lowest-gradient position in a 3√ó3 neighborhood to avoid seeding on edges.
3. Iterate until convergence (few iterations usually suffice):
   - For each center, examine pixels in its 2S √ó 2S neighborhood and assign them to the nearest center using D.
   - Recompute each center as the mean of the assigned pixels in Labxy space.
4. Enforce connectivity: Relabel small stray regions to ensure each superpixel is a single connected component.

Properties and advantages:
- Outputs compact, approximately equal-sized superpixels that respect image boundaries.
- Simple interface: primarily set K (and optionally m for compactness).
- Efficient: O(N) time, with bounded, local distance evaluations; often faster than many alternatives while achieving comparable or better boundary recall and under-segmentation error.

Reference: Achanta et al., ‚ÄúSLIC Superpixels‚Äù (EPFL).

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","SLIC (Simple Linear Iterative Clustering) is an algorithm that clusters pixels in a combined five-dimensional color and image plane space to generate compact, nearly uniform superpixels. It is known for its efficiency and simplicity, with a single parameter specifying the number of superpixels.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciÔ¨Åc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oÔ¨Äer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481√ó321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¬®usstrunkSchool of Computer and Communication Sciences (IC) ¬¥Ecole Polytechnique F¬¥edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined Ô¨Åve-dimensional color and image plane space to eÔ¨É- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use ‚Äì a lone parameter speciÔ¨Åes the number of superpixels ‚Äì and the eÔ¨Éciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneÔ¨Åts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciÔ¨Åc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oÔ¨Äer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481√ó321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required",
What is the purpose of the activation function in a CNN?,"The activation function in a CNN introduces non-linearity after each convolution (and sometimes after fully connected layers). This is crucial because:
- It lets the network learn complex, non-linear patterns in images; without it, stacked linear operations would collapse to a single linear transform.
- It shapes the feature maps (e.g., ReLU makes activations sparse and helps gradients flow, speeding training).
- In the output layer, specific activations map scores to probabilities for classification: sigmoid for binary/multi‚Äëlabel and softmax for multi‚Äëclass problems.

Examples:
- Hidden layers: ReLU (and variants like Leaky ReLU).
- Output layer: Sigmoid or Softmax for classification.

References: Lec-9-CNN.pdf; Practicals with Solutions - CNN.pdf.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","The activation function, also known as the transfer function, determines the output of a neural network. It introduces non-linearity into the model, allowing it to learn more complex patterns. Common activation functions include ReLU, Sigmoid, and Softmax.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between overfitting and underfitting in the context of CNNs?,"Short answer
- Underfitting: the model is not learning enough. It is too simple or insufficiently trained to capture the patterns in the data, so it performs poorly on both the training and validation sets.
- Overfitting: the model is learning too much. It memorizes training-specific details (including noise), so it performs very well on the training set but poorly on unseen validation/test data.

How they look in practice with CNNs
- Underfitting symptoms:
  - High training loss and high validation loss (both bad).
  - Training and validation accuracies are both low.
- Overfitting symptoms:
  - Training loss keeps decreasing, training accuracy very high.
  - Validation loss stops improving or increases; validation accuracy lags or drops.

Typical causes
- Underfitting:
  - Model capacity too low (too shallow/few filters), too much regularization, too few epochs, learning rate too high, overly aggressive downsampling.
- Overfitting:
  - Model capacity too high for the dataset size, limited/imbalanced data, weak regularization, no/limited augmentation, label noise.

How to mitigate
- Reduce underfitting:
  - Increase model capacity (more filters/layers), train longer, reduce regularization, tune learning rate, use better architectures for the task, increase input resolution if appropriate.
- Reduce overfitting (as covered in the CNN lecture materials):
  - Data augmentation
  - Dropout
  - Batch normalization
  - Pooling layers
  - Early stopping
  - Noise injection
  - Weight decay (L2) and/or transfer learning with partial freezing

If you want, I can look at your training/validation curves and suggest whether you‚Äôre underfitting or overfitting and what to try next.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on new, unseen data. Underfitting happens when the model is not complex enough to capture the underlying patterns in the data and performs poorly on both training and test data.","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection ‚Äî normalizations os Data augmentation ‚Äî16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What are some common CNN architectures?,"Here are widely used CNN architectures, grouped by typical task:

Image classification (backbones)
- LeNet-5: early CNN for digit recognition.
- AlexNet: popularized deep CNNs; ReLU, dropout.
- VGG-16/19: very deep stacks of 3√ó3 convs.
- GoogLeNet/Inception: multi-branch ‚ÄúInception‚Äù modules.
- ResNet (e.g., ResNet-50/101): residual (skip) connections to train very deep nets.
- DenseNet: dense skip connections between layers.
- EfficientNet: compound scaling of depth/width/resolution.
- MobileNet (v1‚Äìv3), ShuffleNet, SqueezeNet: lightweight models for mobile/embedded.

Object detection (built on CNN backbones)
- R-CNN, Fast R-CNN, Faster R-CNN: region-based detectors (all are CNN-based per our notes).
- SSD (Single Shot Detector): one-stage detector.
- RetinaNet: one-stage with focal loss to handle class imbalance.

Image segmentation
- FCN (Fully Convolutional Networks): replace FC layers with convs for dense prediction.
- U-Net: encoder‚Äìdecoder with skip connections (widely used in medical imaging).
- DeepLab (v3+): dilated convs and ASPP for multi-scale context.

If you tell me your task (e.g., classification on mobile, high-accuracy detection, medical segmentation), I can suggest a suitable architecture and training tips.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - CNN.pdf"" target=""_blank"">Practicals   CNN</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

","Some well-known CNN architectures include LeNet, AlexNet, VGG, ResNet, GoogLeNet, and MobileNet. There are also region-based CNNs (R-CNN, Fast R-CNN, Faster R-CNN) used for object detection.","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:",
What is the watershed algorithm for segmentation?,"The watershed algorithm is a segmentation method that treats an image as a topographic surface and ‚Äúfloods‚Äù it from its local minima to form catchment basins separated by watershed lines (ridges). It is especially effective for delineating touching objects and adhering to strong boundaries.

Key ideas
- Topography: Low intensities (or low gradient values) are valleys; high values are hills.
- Catchment basins: Regions grown from local minima.
- Watershed lines: Boundaries where floods from different basins meet.

Two common formulations
1) Watershed on the gradient image (classical)
- Use the gradient magnitude of the image so that strong edges are high (hills).
- Flood from local minima (using a priority queue ordered by gradient).
- Assign pixels to the basin they are reached from; if a pixel is reached by two basins, mark it as a watershed (boundary).
- Often over-segments if the image is noisy; smoothing or minima suppression helps.

2) Marker-controlled watershed
- Provide markers (seeds) for foreground objects and background (e.g., by thresholding, morphology, or distance transform for separating touching objects).
- Run watershed constrained by these markers to greatly reduce over-segmentation.

Typical workflow
- Preprocess: denoise/smooth; compute gradient magnitude (or compute a distance transform for binary objects).
- Find seeds: local minima or user/automatic markers.
- Flooding: priority-queue expansion from seeds; build label image and watershed lines.
- Postprocess: merge tiny regions if needed.

Strengths and limitations
- Pros: Good adherence to boundaries; can precisely separate touching objects (with markers).
- Cons: Sensitive to noise; classical version can produce many small regions without proper preprocessing/markers.

Notes linked to the course materials
- Watershed is described as a gradient-ascent/flooding method producing lines that separate catchment basins; a fast queue-based implementation was proposed by Vincent and Soille (1991).
- The practical exercise in the module demonstrates pixel ordering and labeling rules for forming basins and watershed lines on a small intensity grid.

Recommended reading in your materials:
- SLIC Superpixels paper intro (categorizes watershed and summarizes its mechanics and the Vincent‚ÄìSoille queue-based variant).
- Practicals ‚Äì Segmentation and Practicals with Solutions ‚Äì Segmentation (contain the watershed exercise and a sample solution).

Sources
- SLIC_Superpixels.pdf
- Practicals - Segmentation.pdf
- Practicals with Solutions - Segmentation.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Segmentation.pdf"" target=""_blank"">Practicals with Solutions   Segmentation</a>

<a href=""http://localhost:5003/documents/Practicals - Segmentation.pdf"" target=""_blank"">Practicals   Segmentation</a>

","The watershed algorithm is a region-based segmentation method. It treats the gradient magnitude image as a topographic surface, flooding basins from local minima until watershed lines (boundaries) are formed.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eÔ¨Écient level-set based geometric Ô¨Çow. The geometric Ô¨Çow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the Ô¨Åve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are ‚Äì views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I¬∞v0 ‚ÄúACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -¬∞boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eÔ¨Écient level-set based geometric Ô¨Çow. The geometric Ô¨Çow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the Ô¨Åve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are ‚Äì views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I¬∞v0 ‚ÄúACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -¬∞boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!",
What is the purpose of a structuring element in morphological operations?,"A structuring element (SE) is the small binary ‚Äúprobe‚Äù that defines how morphological operations examine and modify an image. It specifies the neighborhood‚Äôs shape, size, and anchor point used to add or remove pixels.

Key roles:
- Defines the geometric rule of the operation:
  - Dilation: expands the foreground according to the SE‚Äôs shape, closing gaps and growing boundaries in the SE‚Äôs directions.
  - Erosion: shrinks the foreground, keeping only pixels where the SE ‚Äúfits.‚Äù
- Controls the amount and direction of change: larger SEs cause stronger effects; different shapes (disk/ellipse, rectangle, cross) affect corners, thin structures, and connectivity differently.
- Acts like a binary mask (identifying rather than weighting, unlike convolution masks); need not be square and the origin need not be centered.
- Encodes assumptions about connectivity and shape (e.g., cross ‚âà 4-connectivity, disk ‚âà isotropic smoothing).

Example: Using an elliptic SE for dilation will round corners and close small holes uniformly; a cross-shaped SE expands mainly along horizontal/vertical directions.

Sources: Lec-5-Morphology.pdf; Lab 5.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

",A structuring element is a small kernel used in morphological operations like erosion and dilation. It defines the shape of the neighborhood of a pixel that is considered when applying the operation.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony",
What is the difference between opening and closing in morphological image processing?,"Opening and closing are two fundamental morphological filters that use a structuring element (SE) but apply erosion and dilation in opposite orders, leading to different effects.

- Opening (erosion followed by dilation):
  - Effect: Removes small bright (foreground) objects and thin protrusions; breaks narrow connections; smooths object boundaries by shaving off small bright details.
  - Typical use: Remove ‚Äúsalt‚Äù noise, thin text or specks brighter than the background; separate weakly connected bright blobs.
  - Binary intuition: Deletes small white spots; shrinks then regrows, so small bright features disappear.
  - Grayscale intuition: Acts like a local min then max filter; suppresses small bright peaks.

- Closing (dilation followed by erosion):
  - Effect: Fills small dark holes/gaps in bright objects; bridges narrow dark channels; smooths boundaries by filling small dark indentations.
  - Typical use: Remove ‚Äúpepper‚Äù noise (small dark specks), fill gaps in bright regions, thicken/bridge breaks.
  - Binary intuition: Fills small black holes in white regions; expands then shrinks, so small dark intrusions vanish.
  - Grayscale intuition: Acts like a local max then min filter; suppresses small dark valleys.

Key points:
- They are not inverses. Opening then closing ‚â† closing then opening; they solve different problems.
- Both depend critically on the structuring element‚Äôs shape and size. Choose an SE just larger than the unwanted feature (e.g., larger than text stroke width to remove text; larger than noise specks to suppress them without harming main structures).
- In practice (e.g., OpenCV): use morphologyEx(img, MORPH_OPEN, SE) or morphologyEx(img, MORPH_CLOSE, SE).

Example:
- To remove small dark text over a light (white) region, apply closing on the white foreground with an SE wider than the text strokes; this fills the dark text gaps. To remove small bright specks on a dark background, use opening with an SE slightly larger than the specks.

Based on module practical notes on morphology (erosion, dilation, opening, closing, and structuring elements) [Practicals - Morphology.pdf; Practicals with Solutions - Morphology.pdf]. If you‚Äôd like, I can suggest a suitable SE size for your specific image.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

","Opening is an erosion followed by a dilation, which can remove small objects and smooth object contours. Closing is a dilation followed by an erosion, which can fill small holes and connect nearby objects.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure ¬ª 1 thecne? the proewss: + teew flor each cut Ete toature ‚Äò ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‚Äòaction process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left",
How can color histograms be used for image retrieval?,"Color histograms provide a compact, robust ‚Äúsignature‚Äù of an image‚Äôs color content, which you can use to retrieve similar images from a database.

How it works
- Build signatures:
  - Choose a color space and quantization (e.g., 3D bins in opponent-color space or HSV).
  - For each database image, compute a normalized color histogram (counts per bin divided by total pixels).
  - For a query image (or a selected region), compute its histogram the same way. [./documents/Swain1.pdf]
- Compare histograms with Histogram Intersection:
  - Score(q, d) = sum over bins i of min(qi, di), optionally normalized by sum(qi).
  - Rank database images by this score; higher means more similar.
  - Benefits: does not require perfect segmentation, is robust to occlusion, image and histogram resolution changes. Works well for indexing large databases. [./documents/Swain1.pdf]
- Speed at scale:
  - Use Incremental Intersection to index very large databases efficiently by accumulating partial scores only over bins present in the query. [./documents/Swain1.pdf]
- Optional localization for re-ranking:
  - Use Histogram Backprojection to find where in an image the query‚Äôs color distribution occurs; useful to localize the target or refine retrieval by checking spatial support. [./documents/Swain1.pdf]

Why histograms are good for retrieval
- Invariant to translation and in-plane rotation; change slowly with out-of-plane rotation, occlusion, and distance. Different objects often have markedly different histograms. [./documents/Swain1.pdf]

Practical recipe
1) Preprocessing: pick color space (e.g., opponent colors from Swain & Ballard, or HSV), decide binning (coarser bins = faster, more robust; finer bins = more discriminative).
2) Database: precompute and store normalized histograms.
3) Query: compute query histogram.
4) Scoring: compute histogram intersection scores and rank.
5) Optional: apply backprojection on top candidates to verify spatial consistency.
6) Illumination handling: apply color constancy or illumination-invariant color spaces to improve robustness under varying light. [./documents/Swain1.pdf]

Notes and tips
- Normalize histograms to mitigate image size differences.
- Background colors can dominate; if you have a rough mask of the object/region of interest, compute a masked histogram.
- A small set of histograms can represent different views of the same 3D object for better recall. [./documents/Swain1.pdf]

If you‚Äôd like, I can sketch pseudocode for histogram intersection-based retrieval or advise on bin choices for your dataset.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

","Color histograms can be used to represent the color distribution of an image. By comparing the color histogram of a query image with the histograms of images in a database using a similarity measure like histogram intersection, similar images can be retrieved.","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is",
What is histogram intersection?,"Histogram intersection is a way to measure how much two histograms overlap. Given two n-bin histograms I and M, the (unnormalized) intersection is:
Sum over bins of min(Ii, Mi)

A common normalized match score (between 0 and 1) divides by the total mass of the model histogram M:
Score(I, M) = [sum_i min(Ii, Mi)] / [sum_i Mi]

Intuition:
- It measures ‚Äúhow much of the model could be present in the image.‚Äù
- Extra pixels in the image that don‚Äôt appear in the model do not reduce the score (they just don‚Äôt contribute), which makes it robust to background clutter and partial occlusion.
- The score only increases when image pixels share colors (or other features represented by bins) present in the model, up to the amount present in the model.

Example:
- H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)
- H2 = (0, 40, 0, 0, 0, 6, 0, 110, 0)
Intersection = min(10,0)+‚Ä¶+min(10,6)+‚Ä¶ = 6
If H2 is the model, normalized score = 6 / sum(H2).

Why it‚Äôs used:
- Effective for recognition with color histograms; robust to viewpoint changes, occlusion, image/histogram resolution changes. Lighting variation typically requires a color constancy step beforehand.
- Efficient variants (Incremental Intersection) match only the largest bins first for fast indexing without losing accuracy.

References:
- Swain and Ballard (1991); course slides on Histogram Intersection
- Sources: ./documents/Swain1.pdf; ./documents/Lec-2-Histogram (Whole Image Understanding).pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram intersection is a method for comparing two histograms. It calculates the sum of the minimum values for each corresponding bin in the two histograms. This measure is robust to occlusion and changes in image resolution.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‚Äò(and ‚Äò-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  √•  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R ‚Äì G - BY = 2 * B ‚Äì R ‚Äì G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly‚Äî distractions in the background of the object,‚Äî viewing the object from a variety of viewpoints,‚Äî occlusion,‚Äî varying image resolution,‚Äî varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‚Äò(and ‚Äò-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  √•  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R ‚Äì G - BY = 2 * B ‚Äì R ‚Äì G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly‚Äî distractions in the background of the object,‚Äî viewing the object from a variety of viewpoints,‚Äî occlusion,‚Äî varying image resolution,‚Äî varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to",
What are the different types of redundancy in image compression?,"The three main types of redundancy exploited in image compression are:

- Coding redundancy: Using more bits than necessary to represent symbol values. Variable-length codes (e.g., Huffman coding) assign shorter codes to more frequent gray levels to reduce average bits per pixel.

- Spatial redundancy (interpixel redundancy): Neighboring pixels are often highly correlated or identical in smooth regions. Methods like predictive coding, run-length coding (for repeated values), and transforms (e.g., DCT in JPEG) exploit this correlation.

- Psychovisual redundancy: The human visual system is less sensitive to certain information (e.g., high spatial frequencies or precise chroma detail). Schemes like GIF‚Äôs limited color palette and JPEG‚Äôs quantization (and chroma subsampling) discard or coarsen less perceptually important data.

Sources: 
- Lecture 10: Image Compression (COMP-2032)
- Practicals and Practicals with Solutions ‚Äì Image Compression

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Image Compression.pdf"" target=""_blank"">Practicals with Solutions   Image Compression</a>

","There are three main types of redundancy in image compression: coding redundancy, spatial redundancy, and psychovisual redundancy.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=‚Äî =0,2,...,L‚Äî1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1√•() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=‚Äî =0,2,...,L‚Äî1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1√•() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1",
What is Huffman coding?,"Huffman coding is a lossless entropy-coding method that assigns variable-length, prefix-free bit codes to symbols based on their probabilities. More frequent symbols get shorter codes; rarer symbols get longer ones. It removes coding redundancy and is a core building block in image compression pipelines (e.g., the entropy stage of baseline JPEG) without degrading image quality.

How it works (static Huffman):
- Estimate symbol probabilities (e.g., from an image histogram).
- Build a binary tree by repeatedly merging the two least-probable nodes.
- Assign 0/1 to branches; each symbol‚Äôs codeword is the root-to-leaf bit sequence.
- The resulting code is prefix-free and optimal among all prefix codes for the given probabilities.

Why it‚Äôs useful in image processing:
- Reduces average bits per pixel (or per transform coefficient) with no loss of information.
- Simple, fast, and near the theoretical lower bound given by the source entropy.

Example (toy):
- Probabilities: A:0.4, B:0.3, C:0.2, D:0.1.
- One valid Huffman code: A=0, B=10, C=110, D=111.
- Average length = 0.4¬∑1 + 0.3¬∑2 + 0.2¬∑3 + 0.1¬∑3 = 1.9 bits/symbol (< fixed 2 bits).

Notes:
- Optimal for integer-length prefix codes; arithmetic coding can be slightly closer to entropy.
- Requires symbol statistics (done via a pass over the data or using adaptive variants).

If you like, I can walk you through deriving the Huffman code for the course‚Äôs example histogram from the practicals.

References:
- Practicals ‚Äì Image Compression (COMP-2032): Types of redundancy and Huffman coding.
- Practicals with Solutions ‚Äì Image Compression: Worked Huffman coding example.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Image Compression.pdf"" target=""_blank"">Practicals with Solutions   Image Compression</a>

","Huffman coding is a lossless data compression algorithm. It assigns variable-length codes to input characters, with the lengths of the assigned codes being based on the frequencies of corresponding characters. More frequent characters get shorter codes.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code ‚Äì Let‚Äôs BREAK it DOWN‚Ä¶Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
How does JPEG compression work?,"Here‚Äôs the JPEG pipeline and why each step is used.

What JPEG tries to do
- Reduce coding redundancy with entropy coding (e.g., Huffman).
- Reduce spatial/psychovisual redundancy by transforming and quantising so that visually less important info is stored coarsely or discarded.
- Control the trade-off between file size and quality via quantisation.

Baseline JPEG (sequential) steps
1) Color preprocessing (typical)
   - Convert RGB to YCbCr (luminance Y and chroma Cb, Cr): exploits that the eye is more sensitive to brightness than color.
   - Optionally subsample chroma (e.g., 4:2:0) to save bits with little visible loss.
   Note: While standard in JPEG, the core slides focus on the transform/quantise/encode steps below.

2) Block the image
   - Split each channel into 8√ó8 pixel blocks.

3) Transform: 2D DCT per 8√ó8 block
   - Produces one DC coefficient (top-left) and 63 AC coefficients.
   - Compacts most energy into low frequencies (upper-left area), preparing for quantisation.

4) Quantisation (key lossy step)
   - Divide each DCT coefficient by a quantisation matrix and round.
   - Low frequencies (visually important) quantised finely; high frequencies more coarsely.
   - Stronger quantisation ‚Üí more zeros ‚Üí smaller files but visible artefacts (e.g., blocking, ringing). Slides: ‚ÄúIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visible.‚Äù

5) Zig-zag scan
   - Reorder the 8√ó8 quantised DCT block from low to high frequency in a zig-zag path (DC then AC).
   - This tends to produce long runs of trailing zeros in the AC stream.

6) Entropy coding of coefficients
   - DC coefficients: coded as differences between adjacent blocks (DPCM) to reduce variance.
   - AC coefficients: run-length encode sequences of zeros and the size/category of the next non-zero.
   - Finally, apply Huffman coding to DC and AC symbol streams. The ordering ‚Äúproduces long sequences of zeros‚Äù which compress well with run-length + Huffman coding.

Decoding
- Reverse the steps: Huffman decode ‚Üí rebuild DC (inverse DPCM) and AC from run-lengths ‚Üí inverse zig-zag to 8√ó8 ‚Üí dequantise ‚Üí inverse DCT ‚Üí merge blocks ‚Üí convert back to RGB (if needed).

Quality vs size
- The ‚Äúquality‚Äù slider mainly scales the quantisation tables.
- Higher compression (lower quality) ‚Üí more aggressive quantisation ‚Üí more zeros ‚Üí smaller files but more visible block boundaries and loss of fine detail.
- Objective fidelity can be measured with Erms and SNR, but subjective human judgment is often more meaningful.

Key ideas linked to course topics
- Redundancy types: coding (Huffman), spatial (DCT decorrelation), psychovisual (quantisation that discards visually less important info).
- JPEG is a complete compression scheme combining transform, quantisation, run-length, and Huffman coding; zig-zag scanning is central to forming zero runs for ACs.

If you‚Äôd like, I can walk through a tiny 8√ó8 example showing zig-zag ordering and run-length/Huffman coding of the AC stream.

Sources: Lec-10 ‚Äì Image Compression; Finale and Revision; Practicals ‚Äì Image Compression (University of Nottingham, COMP-2032). For further clarifications, you can also contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or TA Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

<a href=""http://localhost:5003/documents/Practicals - Image Compression.pdf"" target=""_blank"">Practicals   Image Compression</a>

<a href=""http://localhost:5003/documents/Lec-11-Finale and Revision.pdf"" target=""_blank"">Lec 11 Finale and Revision</a>

","JPEG compression is a lossy compression method that involves dividing an image into blocks, applying a Discrete Cosine Transform (DCT) to each block, quantizing the DCT coefficients, and then using Huffman coding to compress the quantized coefficients.","Compression‚Äî26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 ‚Äî3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.‚Äî.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -‚Äî . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf‚Äù te . = om ¬´File size: } 49K es 100 dpi high JPEG compression ~ ‚Äî ye ‚Äò ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compressionü§î48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well‚Ä¶the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE‚Ä¶CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing ‚Äì theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f‚Äô(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = √¶ √ß √ß √® 1 MN M - 1 N - 1 √•√• x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 √∂ √∑ √∑ √∏ 2/1 SNR = M - 1 N - 1 √•√• ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - √•√• ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" ùëá ùëü = ( ùëù"" ùë§ ùëëùë§ !ùëëùë† ùëëùëü = ùëëùëá(ùëü) ùëëùëü = ùëë ùëëùëü "" ( ! ùëù"" ùë§ ùëëùë§ = ùëù""(ùëü)ùëù# ùë† = ùëù"" ùëü ùëëùëü ùëëùë† = ùëù"" ùëü 1 ùëù"" ùëü = 1 0 ‚â§ ùë† ‚â§ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram","Compression‚Äî26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 ‚Äî3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.‚Äî.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -‚Äî . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf‚Äù te . = om ¬´File size: } 49K es 100 dpi high JPEG compression ~ ‚Äî ye ‚Äò ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compressionü§î48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well‚Ä¶the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE‚Ä¶CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing ‚Äì theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f‚Äô(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = √¶ √ß √ß √® 1 MN M - 1 N - 1 √•√• x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 √∂ √∑ √∑ √∏ 2/1 SNR = M - 1 N - 1 √•√• ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - √•√• ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression‚Ä¢ Types of redundancy: coding, spatial, psychovisual‚Ä¢ Structure of compression systems‚Ä¢ Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?‚Ä¢ Compression affects image quality‚Ä¢ Need to understand/choose methods/parameters‚Ä¢ New methods are based on core concepts20 aHuffman Code ‚Äì For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" ùëá ùëü = ( ùëù"" ùë§ ùëëùë§ !ùëëùë† ùëëùëü = ùëëùëá(ùëü) ùëëùëü = ùëë ùëëùëü "" ( ! ùëù"" ùë§ ùëëùë§ = ùëù""(ùëü)ùëù# ùë† = ùëù"" ùëü ùëëùëü ùëëùë† = ùëù"" ùëü 1 ùëù"" ùëü = 1 0 ‚â§ ùë† ‚â§ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram",
What is the difference between RGB and HSV color spaces?,"Key differences:

- What they represent
  - RGB: Three additive primaries (Red, Green, Blue). Colors are formed by mixing light intensities of R, G, and B. Good for display and hardware representation.
  - HSV: Separates ‚Äúwhat the color is‚Äù from ‚Äúhow bright it is.‚Äù Hue = general color (angle on a color wheel), Saturation = how pure/strong the color is, Value = brightness.

- Sensitivity to illumination
  - RGB: All three channels change with lighting, so color-based processing can be sensitive to shadows and highlights.
  - HSV: Color (Hue, Saturation) is separated from brightness (Value), making it less sensitive to illumination changes. This often helps in color-based segmentation or detection.

- Practical use
  - RGB: Natural for image acquisition and display; arithmetic on channels mixes color and intensity.
  - HSV: Useful when you want to detect/segment by color regardless of lighting (e.g., focusing on Hue/Saturation and ignoring Value). Often you don‚Äôt need all three components‚Äîe.g., skin detection can rely mainly on H and S; plant analysis might even use just a ‚Äúgreenness‚Äù measure like G ‚Äì (R + B)/2.

Example intuition: A red object in bright light and in shadow will have very different RGB values, but similar Hue (and often similar Saturation). In HSV you can threshold on Hue to find it more robustly while tolerating changes in Value.

References:
- Lec-1B-Digital Images and Point Processes (HSV separates color from intensity; definitions of Hue, Saturation, Value; RGB as additive primaries)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","RGB (Red, Green, Blue) is an additive color model based on light, commonly used in displays. HSV (Hue, Saturation, Value) is a color model based on human perception of color. Hue represents the color, saturation represents the intensity of the color, and value represents the brightness. HSV separates color information from intensity, making it less sensitive to illumination changes.","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie‚Ä¢ RGB is OK, but there are other general &application-specific colour spaces‚Ä¶‚Ä¢If you want to work with plants, you might use|just G, or ‚Äògreenness‚Äô:mGG ‚Äì (R+B)/2HSV is based on colour rather   than light  ‚Ä¢ Hue - what general colour is it  ‚Ä¢ Saturation ‚Äì how strongly coloured is it  ‚Ä¢ Value ‚Äì how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject‚Äôs surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie‚Ä¢ RGB is OK, but there are other general &application-specific colour spaces‚Ä¶‚Ä¢If you want to work with plants, you might use|just G, or ‚Äògreenness‚Äô:mGG ‚Äì (R+B)/2HSV is based on colour rather   than light  ‚Ä¢ Hue - what general colour is it  ‚Ä¢ Saturation ‚Äì how strongly coloured is it  ‚Ä¢ Value ‚Äì how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject‚Äôs surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is gamma correction?,"Gamma correction is a point-wise, non-linear intensity transform used to adjust image brightness and to compensate for the nonlinear response of display devices.

Key ideas:
- Displays don‚Äôt emit light linearly with input voltage; luminance L is roughly L ‚âà V^Œ≥d, with device gamma Œ≥d around 2.2‚Äì2.5. To make what you see match the original image intensities, you pre-compensate the image by applying an inverse power.
- The standard formula (for 8-bit images) is:
  O = 255 √ó (I/255)^Œ≥
  where I is the input pixel value, O is the output, and Œ≥ is the chosen gamma.
- Effects:
  - Œ≥ < 1: brightens (boosts dark tones, reveals shadow detail)
  - Œ≥ = 1: no change (linear)
  - Œ≥ > 1: darkens (compresses dark tones)

In display compensation, if the display has Œ≥d ‚âà 2.5, you apply Œ≥ = 1/2.5 to the image so that after the display‚Äôs response, the perceived luminance is linear.

Implementation tips:
- Convert I to floating point in [0,1], apply the power, then scale back to 0‚Äì255 and clip.
- Example gammas often used for illustration: 0.5 (brighten), 1.0 (no change), 1.5 (darken).

Sources:
- Lec-1B-Digital Images and Point Processes.pdf (Gamma response and g(x,y) = f(x,y)^(1/Œ≥d))
- Lab 2.pdf (Gamma mapping formula and interpretation of Œ≥ < 1, = 1, > 1)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",Gamma correction is a non-linear operation used to adjust the brightness and contrast of an image. It is often used to compensate for the non-linear response of display devices.,"a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ is the gamma value.ùõæùëÇ = ùêº 255 √ó 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if‚Äô University of Nottingham ‚Äî~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values ùêº to create ùëÇ (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:‚óè Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.‚óè Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.‚Ä¢ When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform‚Ä¢ You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think¬∞.RAMP=25L ‚âà V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction‚Ä¢ We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each","a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ is the gamma value.ùõæùëÇ = ùêº 255 √ó 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if‚Äô University of Nottingham ‚Äî~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values ùêº to create ùëÇ (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:‚óè Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.‚óè Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.‚Ä¢ When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform‚Ä¢ You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think¬∞.RAMP=25L ‚âà V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction‚Ä¢ We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each",
What is anisotropic diffusion?,"Anisotropic diffusion is an edge-preserving smoothing process that reduces noise by averaging only among pixels that are already similar, while avoiding smoothing across edges.

How it works
- For each pixel p, we compute a weighted average of its neighbors q:
  p' = sum_q s(p,q) I(q) / sum_q s(p,q)
- The similarity (or conductance) s(p,q) lies in [0,1]:
  - s ‚âà 1 if p and q are similar (e.g., similar intensity)
  - s ‚âà 0 if they differ (e.g., across an edge)
- This makes diffusion ‚Äúanisotropic‚Äù (not the same in all directions): it spreads within regions but is inhibited across boundaries, preserving edges while denoising. In contrast, mean/Gaussian (isotropic) filters smooth indiscriminately and blur edges.

Relationship to bilateral filtering
- Bilateral filtering is closely related: it combines a spatial weight (nearby pixels) with a range weight (similar intensity). It‚Äôs a practical, discrete form of edge-preserving smoothing akin to anisotropic diffusion. [Lec-3B-Non-Linear Filters.pdf]

Why use it
- To remove noise while keeping important structures (edges, boundaries) sharp‚Äîuseful in natural images and medical/remote-sensing data. [Lec-3B-Non-Linear Filters.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

","Anisotropic diffusion is an edge-preserving smoothing technique. Unlike isotropic filters like Gaussian blur that smooth uniformly in all directions, anisotropic diffusion encourages smoothing within regions of similar intensity while inhibiting it across strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is bilateral filtering?,"Bilateral filtering is a non-linear, edge-preserving smoothing filter. It denoises an image by averaging nearby pixels, but it only gives high weight to pixels that are both spatially close and similar in intensity to the center pixel. This preserves edges while reducing noise.

How it works (for a pixel p):
- Output(p) = (1/Wp) Œ£ over q in a local window S of [Gs(||p‚àíq||) √ó Gr(|I(p)‚àíI(q)|) √ó I(q)]
- Gs: spatial Gaussian (weights nearby pixels more)
- Gr: range/intensity Gaussian (weights pixels with similar intensity more)
- Wp: normalization (sum of all weights)

Key points:
- If the intensity similarity term Gr is always 1, the filter reduces to a standard mean/Gaussian blur.
- Because Gr down-weights pixels across an intensity jump, edges are preserved.
- Parameters:
  - sigma_s (spatial): controls the size of the neighborhood; larger values smooth over larger areas.
  - sigma_r (range): controls how much intensity difference is tolerated; smaller values preserve edges more strongly but may leave more noise.

Use cases:
- Denoising while keeping edges sharp (e.g., as a pre-processing step before edge detection or segmentation).

Notes:
- Related conceptually to anisotropic diffusion (both aim to smooth within regions while preserving boundaries).
- Can be computationally heavier than simple Gaussian smoothing.

References: Lec-3B-Non-Linear Filters.pdf; Practicals - Linear and Non-Linear Filters.pdf; Lab 3.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

<a href=""http://localhost:5003/documents/Practicals - Linear and Non-Linear Filters.pdf"" target=""_blank"">Practicals   Linear and Non Linear Filters</a>

<a href=""http://localhost:5003/documents/Lab 3.pdf"" target=""_blank"">Lab 3</a>

","Bilateral filtering is a non-linear, edge-preserving smoothing filter. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels. The weights depend not only on the spatial distance of pixels but also on the difference in their intensity values.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding‚Ä¶Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x‚Äî)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ¬∞ Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=√• ¬¥q~‚Äî 8(P.9)s(p,Sums pixel values in a region√•s(p,q)‚ÄîCounts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = √• G s ( || qp - q √é S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q √•  ][  IGB  =  p  q  S  √é  1  √•  ][  =  p  W  S  q  √é  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding‚Ä¶Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x‚Äî)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ¬∞ Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3",
What is the Canny edge detector?,"The Canny edge detector is a multi-stage algorithm for finding thin, well-localized edges in images, designed by John Canny to be ‚Äúoptimal‚Äù under three criteria: good detection (find real edges, avoid false ones), good localization (edges placed close to true positions), and minimal response (each edge reported once).

How it works:
1) Noise reduction: Smooth the image with a Gaussian filter to suppress noise.
2) Gradient computation: Compute intensity gradients (often with Sobel filters) in x and y; get gradient magnitude and direction.
3) Non-maximum suppression: Thin edges by keeping only local maxima of the gradient along the gradient direction.
4) Hysteresis thresholding: Use two thresholds (low and high). Pixels above the high threshold are edges; below the low threshold are non-edges; those in between are kept only if connected to strong edges, which helps link true edges and reject noise.

Key parameters and their effects:
- Gaussian sigma/kernel size: larger values reduce noise but can blur/diminish fine edges.
- High and low thresholds: higher thresholds yield fewer, stronger edges; lower thresholds include more detail but risk noise.
- Gradient operator/aperture size (e.g., Sobel kernel size): affects gradient accuracy and sensitivity.

Why it‚Äôs popular:
- Produces thin, continuous edges
- Robust to noise due to Gaussian smoothing and hysteresis
- Approximates the first derivative of a Gaussian, aligning with the theoretical optimal form

Sources: Lec-6A-Derivative and Edges.pdf; Lab 6.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","The Canny edge detector is a multi-stage edge detection algorithm. It involves noise reduction using a Gaussian filter, finding intensity gradients, non-maximum suppression to thin edges, and hysteresis thresholding to connect weak edges to strong ones.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian",
What is unsharp masking?,"Unsharp masking is an edge-enhancement technique that sharpens an image by adding back a high-frequency ‚Äúdetail‚Äù component extracted from the image.

How it works
- Smooth the original image (typically with a Gaussian blur) to remove high-frequency detail.
- Form an unsharp mask by subtracting the smoothed image from the original: M = I ‚àí (GœÉ * I).
- Add some fraction of this mask back to the original to obtain the sharpened image: Isharp = I + k ¬∑ M, where k controls the strength (‚Äúamount‚Äù), and œÉ controls the blur scale (the size of details enhanced).

Key points
- Enhances edges noticeably by relying on the smoothing effect to create a difference between original and blurred images.
- Can also amplify noise (since noise is high-frequency), and overuse can produce halos.
- Choice of œÉ: smaller œÉ sharpens fine details; larger œÉ accentuates broader transitions.

Sources: Lec-6A-Derivative and Edges.pdf; Practicals with Solutions - Derivative and Edges.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Derivative and Edges.pdf"" target=""_blank"">Practicals with Solutions   Derivative and Edges</a>

","Unsharp masking is an image sharpening technique. It works by subtracting a blurred version of the image from the original image to create a mask of the image's high-frequency components, which is then added back to the original image to sharpen it.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -‚Äî‚Äî‚Äî__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -‚Äî‚Äî__, I eee C ooo ‚Äî_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask ‚Äî En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make‚Äô ¬∞ the edge appear more obvious -‚Äî‚Ñ¢‚Äî_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  ‚Ä¢  Even if they are noise  ‚Ä¢  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f‚ÄôDerivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I‚Ä¢ Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKA œÉ = 1.5BœÉ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -‚Äî‚Äî‚Äî__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -‚Äî‚Äî__, I eee C ooo ‚Äî_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask ‚Äî En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make‚Äô ¬∞ the edge appear more obvious -‚Äî‚Ñ¢‚Äî_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  ‚Ä¢  Even if they are noise  ‚Ä¢  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f‚ÄôDerivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I‚Ä¢ Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest œÉ value?ACK: Prof. Tony Pridmore, UNUKA œÉ = 1.5BœÉ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is a structuring element in morphology?,"A structuring element (SE) in mathematical morphology is a small binary pattern (a set of pixel coordinates) that defines the neighborhood and shape used to probe an image during operations like dilation, erosion, opening, and closing.

Key points:
- It is like a binary mask: 1s mark the pixels to test; it has an origin (often the center) that is placed over each image pixel.
- Shape and size control the effect:
  - Common shapes: square/rectangle, disk/ellipse, cross, line (elongated).
  - Larger SEs produce stronger smoothing, thicker dilations, and thinner erosions.
  - Asymmetric/elongated SEs cause directional effects (e.g., grow/shrink more along one axis).
- Similar effects can often be achieved by repeatedly applying a smaller, similarly shaped SE.
- In practice (e.g., OpenCV), you can create SEs with getStructuringElement (RECT, ELLIPSE, CROSS) or with NumPy ones for rectangles.

Example: A 3√ó3 square SE will dilate by one pixel in all 8 directions; a 3√ó3 cross SE affects only the 4-connected neighbors.

Sources: Lec-5-Morphology.pdf; Lab 5.pdf. If you‚Äôd like, I can show quick code snippets for creating these SEs in OpenCV or NumPy.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Lab 5.pdf"" target=""_blank"">Lab 5</a>

","A structuring element is a small matrix of pixels, typically with a defined shape (e.g., square, disk, cross), used as a probe in morphological operations. It defines the neighborhood of a pixel to be processed.","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B‚Ä¢ The boundaries between foreground and background are often smoothed in the process‚Ä¢ The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I‚Ä¢ Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:‚óè Create an elliptic structuring element of sizes 3, 5 and 7.‚óè Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy‚Äôs ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.‚óè Read in the Square-Hole.jpg image from Moodle.‚óè Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).‚óè Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too ‚Äì giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A¬ÆBACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology‚Ä¢ A patch of the binary image to be processed ‚Ä¢ A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‚Äò- ' '- ‚Äò- ‚Äò-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B‚Ä¢ The boundaries between foreground and background are often smoothed in the process‚Ä¢ The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I‚Ä¢ Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 ‚Ä¢ The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm ‚Ä¢ If any of the ‚Äò1‚Äô",
How can you use a binary image as a mask?,"A binary image (0 = background, 255 = foreground) can act as a mask to select which pixels an operation is applied to. In our course materials, this is exactly how we ‚Äúkeep‚Äù or ‚Äúreplace‚Äù pixels only in selected regions, and Python/OpenCV make it straightforward.

Typical uses and patterns:

- Extract/keep only the masked region
  - OpenCV: roi = cv2.bitwise_and(img, img, mask=mask)
  - NumPy: out = np.zeros_like(img); out[mask > 0] = img[mask > 0]

- Remove/zero out the masked region
  - inv = cv2.bitwise_not(mask)
  - out = cv2.bitwise_and(img, img, mask=inv)

- Blend/compose two images using a mask
  - NumPy: out = np.where(mask[..., None] > 0, imgA, imgB)  # for color images
  - OpenCV:
    - partA = cv2.bitwise_and(imgA, imgA, mask=mask)
    - partB = cv2.bitwise_and(imgB, imgB, mask=cv2.bitwise_not(mask))
    - out = cv2.bitwise_or(partA, partB)

- Apply an operation only inside the mask
  - Example (blur only masked pixels):
    - blurred = cv2.GaussianBlur(img, (5, 5), 0)
    - out = img.copy(); out[mask > 0] = blurred[mask > 0]
  - Compute a statistic over the masked region:
    - mean_val = cv2.mean(gray, mask=mask)  # mean of gray where mask > 0

Practical tips from the labs:
- Make sure the mask is single-channel uint8 with values 0 and 255, and the same width/height as the target image.
- For color images with NumPy arithmetic, broadcast the mask: img * (mask/255)[:, :, None].
- You can obtain masks by thresholding (e.g., cv2.threshold with THRESH_BINARY, Otsu, or adaptive methods), and clean them with morphology (opening/closing) before use.

This aligns with the practical notes: ‚ÄúBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this,‚Äù and the examples where thresholds over different regions are combined using masks. If you want, tell me which library you‚Äôre using (OpenCV or scikit-image/NumPy), and I‚Äôll tailor the code to your setup.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - Thresholding and Binary Images.pdf"" target=""_blank"">Practicals with Solutions   Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image can be used as a mask to apply an operation to a specific region of another image. Pixels in the binary mask with a value of 1 (or True) indicate the region where the operation should be applied, while pixels with a value of 0 (or False) are ignored.","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing‚Äô a ume. For each subse‚Ç¨; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing‚Äô a ume. For each subse‚Ç¨; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!",
What is the Nyquist rate?,"The Nyquist rate is the minimum sampling rate required to avoid aliasing: sample at least twice the highest frequency present in the signal. 

- 1D signals: f_s ‚â• 2 f_max, where f_max is the highest frequency; the Nyquist frequency is f_s/2.
- Images (2D): you need at least two samples per smallest resolvable period along each spatial direction. Equivalently, pixel spacing must be ‚â§ half the smallest feature period. For example, if the finest detail repeats every p mm, you need pixel spacing ‚â§ p/2 (i.e., ‚â• 2/p samples per mm).

Sampling below the Nyquist rate leads to aliasing (e.g., moir√© patterns, jagged edges). When downsampling images, apply a low-pass (anti-alias) filter so the remaining bandwidth fits below the new Nyquist limit.

Source: Lecture 1B ‚Äì Digital Images and Point Processes (COMP-2032), ‚ÄúSampling‚Ä¶ Nyquist Rate‚Ä¶ Aliasing.‚Äù

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

",The Nyquist rate is the minimum sampling rate required to avoid aliasing. It is twice the frequency of the highest frequency component in the signal (or image) being sampled.,"rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing‚Ä¢ Aliasing occurs when two signals (images) become indistinguishable when sampled ‚Ä¢ In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?‚Ä¢ Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&‚Ä¢ Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing‚Ä¢ Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744‚Ä¢ When downsampling, need to compute a summary pixel value fromie ‚Äòeach Se k. |Pick one, mean, weighted mean‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?‚Ä¢ Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed ‚Ä¢ Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between a point process and a spatial filter?,"Short answer:
- Point process = per-pixel mapping. Each output pixel depends only on the corresponding input pixel (and fixed parameters), not on its neighbors.
- Spatial filter = neighborhood operation. Each output pixel depends on a set of nearby input pixels (linear or non-linear combinations).

Details:
- Point process
  - Definition: g(x,y) depends only on f(x,y) and maybe some parameters.
  - Typical uses: brightness/contrast changes, gamma/intensity transforms, colour space conversion (e.g., RGB ‚Üí HSV).
  - Example: g(x,y) = a f(x,y) + b, where a controls contrast (gain) and b controls brightness (bias). Colour space transformation is also a point process.
  - References: Lec-1B-Digital Images and Point Processes.pdf

- Spatial filter (neighborhood/area filter)
  - Definition: g(x,y) depends on pixel values in a spatial neighborhood around (x,y). Can be linear (e.g., convolution with a kernel) or non-linear.
  - Typical uses: noise reduction/smoothing, edge-preserving smoothing, etc.
  - Examples:
    - Mean/Gaussian smoothing: make each pixel more like its neighbors (can be seen as diffusion processes).
    - Median filter: good for removing small speckle/impulse noise.
    - Anisotropic diffusion: weighted averaging that favors similar neighbors using a similarity function s(p,q) in [0,1], making pixels more like neighbors they already resemble.
  - References: Lec-3B-Non-Linear Filters.pdf

Rule of thumb:
- Use a point process for global, per-pixel tone/colour adjustments.
- Use a spatial filter when local context (neighboring pixels) matters, e.g., denoising or edge-aware smoothing.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lec-3B-Non-Linear Filters.pdf"" target=""_blank"">Lec 3B Non Linear Filters</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Thresholding and Binary Images.pdf"" target=""_blank"">Practicals with Solutions   Thresholding and Binary Images</a>

","A point process operates on each pixel independently, where the output pixel value depends only on the corresponding input pixel value. A spatial filter operates on a local neighborhood of pixels, where the output pixel value depends on the values of the input pixel and its neighbors.","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in‚Äô the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‚Äòaction process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in‚Äô the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‚Äòtapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN‚Ä¢ We don‚Äôt always need all 3 values  ‚Ä¢ Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in‚Äô the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‚Äòaction process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in‚Äô the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‚Äòtapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA ‚Ä¢ Mean and Gaussian filters make each pixel more like its neighbours ‚Ä¢ Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) ‚Ä¢ s(p,q) has values in the range from 0 to 1 ‚Ä¢ If the pixels p and q are similar then s(p,q) is close to 1 ‚Ä¢ If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  √• ¬¥  q  s(p,  q)  √•  s(p,  q) 11[TheThe Similarity Function‚Ä¢ The smoothing function, s(p,q) needs to be found‚Ä¢If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‚ÄòOther functions often used include: | Pee|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
How can you convert an RGB image to grayscale?,"Short answer:
- Simple average: I = (R + G + B) / 3
- Perceptual (weighted) luminance: I = 0.30 R + 0.59 G + 0.11 B
- Library function: OpenCV‚Äôs cvtColor(..., COLOR_BGR2GRAY) uses weights ‚âà 0.299, 0.587, 0.114

Notes:
- Human vision is more sensitive to green, so the weighted method usually looks better than a simple average.
- In OpenCV, images load as BGR, not RGB. Mind channel order.
- Convert to float before arithmetic to avoid uint8 overflow, then convert back to uint8. (This is highlighted in the lab notes.)

Example (NumPy and OpenCV):
- Vectorized average (RGB array):
  R = img_rgb[:, :, 0].astype(np.float32)
  G = img_rgb[:, :, 1].astype(np.float32)
  B = img_rgb[:, :, 2].astype(np.float32)
  gray_avg = np.rint((R + G + B) / 3.0).astype(np.uint8)

- Vectorized weighted (RGB array, lecture weights):
  gray_w = np.rint(0.30*R + 0.59*G + 0.11*B).astype(np.uint8)

- OpenCV built-in (BGR input):
  img_bgr = cv2.imread('image.png')  # uint8, BGR
  gray_cv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

If you need, I can also show a simple loop version (as in the lab) or how to compare your result to cvtColor.

References:
- Lecture: I = 0.30R + 0.59G + 0.11B; motivation for grayscale conversion [Lec-1B-Digital Images and Point Processes.pdf]
- Lab task and tips (uint8 overflow, cvtColor comparison) [Lab 1B.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

<a href=""http://localhost:5003/documents/Lab 1B.pdf"" target=""_blank"">Lab 1B</a>

","An RGB image can be converted to grayscale by taking a weighted average of the red, green, and blue channels. A common formula is:
Grayscale = 0.30*R + 0.59*G + 0.11*B, which takes into account the human eye's different sensitivities to different colors.","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.‚óè Display its red, green and blue components as three separate grey-level images.‚óè Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)‚óè Perform the above again but using NumPy‚Äôs array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)‚óè Produce and display an image of the atrium showing the ‚Äògreenness‚Äô value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.‚óè Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel ‚Ä¢ Makes processing easier ‚Ä¢ Reduces the amount of information ‚Ä¢ Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*‚Ä¢ Most common starting point‚Ä¢ Retinal cells are sensitive tothree primary colours R, G, B‚Ä¢ Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.‚óè Display its red, green and blue components as three separate grey-level images.‚óè Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)‚óè Perform the above again but using NumPy‚Äôs array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)‚óè Produce and display an image of the atrium showing the ‚Äògreenness‚Äô value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.‚óè Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image ùêº to new values in ùëÇ. The formula is shown in (1), where ùõæ|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel ‚Ä¢ Makes processing easier ‚Ä¢ Reduces the amount of information ‚Ä¢ Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so‚Ä¶ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*‚Ä¢ Most common starting point‚Ä¢ Retinal cells are sensitive tothree primary colours R, G, B‚Ä¢ Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let‚Äôs Walk DOWN Memory LaneB  255  255  255",
What is the purpose of histogram backprojection in object tracking?,"Histogram backprojection is used to locate (and then track across frames) an object by projecting its color histogram onto a new image to create a likelihood map showing where the object‚Äôs colors occur. The highest response (after optionally convolving with a window of the object‚Äôs size) gives the object‚Äôs position; repeating this per frame yields tracking.

Key points:
- It solves the ‚Äúwhere is the object?‚Äù problem efficiently, even in crowded scenes.
- Using ratio backprojection, colors distinctive to the target get strong responses while common background colors are down-weighted (e.g., the blue stripes of a shirt respond strongly; white responds weakly).
- It is robust to partial occlusion and fast enough for real-time tracking.

Reference: Swain and Ballard, Color Indexing (see ./documents/Swain1.pdf).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram backprojection produces a probability map of object locations based on a model histogram. This probability map is commonly used with tracking algorithms such as mean-shift to estimate and update the object's position across frames.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‚Äògained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‚Äògained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between first and second derivative methods for edge detection?,"Short answer:
- What they detect:
  - First derivative methods look for peaks in the gradient magnitude (rate of change).
  - Second derivative methods look for zero-crossings of the second derivative (curvature), often via the Laplacian.

- Pros/cons:
  - First derivative:
    - Pros: Strong response at true edges; good localization with non-maximum suppression (e.g., Canny).
    - Cons: Sensitive to noise; requires careful thresholding and edge linking.
  - Second derivative:
    - Pros: Zero-crossings are well-defined and easy to detect.
    - Cons: Very sensitive to noise; tends to round off corners; producing smooth, connected contours can be difficult.

- Typical operators:
  - First derivative: Roberts, Prewitt, Sobel; Canny (Gaussian smoothing + gradients + non-max suppression + hysteresis).
  - Second derivative: Laplacian; Laplacian of Gaussian (LoG) to reduce noise before zero-crossing detection.

- Practice:
  - First-derivative methods are more common in real applications, largely due to Canny‚Äôs performance and robustness.

Intuition with a 1D step edge:
- First derivative shows a large positive/negative peak at the edge location.
- Second derivative changes sign at the edge center; detect the zero-crossing between the positive and negative lobes.

Sources: Lec-6A-Derivative and Edges.pdf; Lab 6.pdf (COMP2032). If you want, I can show a quick Sobel vs Laplacian comparison on one of the lab images and discuss parameter effects.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

","First derivative methods (e.g., Sobel, Roberts) look for peaks in the image gradient to find edges. Second derivative methods (e.g., Laplacian of Gaussian) look for zero-crossings in the second derivative of the image to find edges.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative ‚Ä¢ Strong response at edges, but - Well-defined, easy to detect also respond to noise ‚Ä¢ Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in",
What are the main steps in the Canny edge detection algorithm?,"The Canny edge detector has four main stages:

1) Noise reduction
- Smooth the image with a Gaussian filter to suppress noise before gradient computation. Parameter: Gaussian œÉ (or kernel size).
2) Gradient computation
- Compute horizontal and vertical derivatives (commonly using Sobel filters), then gradient magnitude and direction from Gx and Gy.
3) Non-maximum suppression
- Thin edges by keeping only local maxima of the gradient magnitude along the gradient direction.
4) Hysteresis thresholding (edge tracking)
- Use two thresholds: high and low. Pixels above high are strong edges, below low are non-edges, and those in between are weak edges kept only if connected to strong edges; others are discarded.

Common tunables: Gaussian œÉ (smoothing), low/high thresholds for hysteresis, and derivative kernel size (e.g., Sobel aperture). These control sensitivity and edge thinness/continuity.

References: Lab 6 (Ang & Chandesa); Lecture 6A: Derivative and Edges.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","The Canny edge detection algorithm involves: 1) Gaussian smoothing to reduce noise, 2) finding the intensity gradient, 3) non-maximum suppression to thin edges, and 4) hysteresis thresholding to link edges.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the ‚Äúobjects‚Äù in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¬¢ EX 0 if ||X@) ‚Äî XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 ¬´ po } ~ . ‚Äò {Flat segment 7‚Äò, ; 2 ‚Äò. i 4 ‚Äò ; S| ‚Äúe rok \ a *, H *s b--sia-¬¢ ‚Äòw---¬ª-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] ¬´|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¬∂  ¬∂  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the ‚Äúobjects‚Äù in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¬¢ EX 0 if ||X@) ‚Äî XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing",
What is the purpose of non-maximum suppression in the Canny edge detector?,"Non-maximum suppression in the Canny detector thins edges by keeping only the strongest gradient responses along the edge direction and suppressing the rest. Concretely, at each pixel it compares the gradient magnitude to its two neighbors along the gradient direction; if it isn‚Äôt a local maximum, it‚Äôs set to zero. This produces well-localized, roughly 1-pixel-wide edges and removes thick or blurred edge responses before hysteresis thresholding. [Lec-6A-Derivative and Edges.pdf; Lab 6.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Non-maximum suppression is used to thin the edges found by the gradient calculation. For each pixel, it checks if the gradient magnitude is the largest in the direction of the gradient. If not, the pixel is suppressed (set to zero).","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is hysteresis thresholding?,"Hysteresis thresholding is a two-threshold strategy for deciding which edge pixels to keep in an edge map (used in the Canny detector).

Key idea:
- Use two thresholds: a high threshold to find confident (strong) edge pixels, and a lower threshold to keep weaker pixels only if they are connected to a strong edge.
- This preserves continuous edges and suppresses isolated noise responses.

Typical steps:
1) Apply non-maximum suppression to thin the gradient magnitude image.
2) Mark pixels with magnitude ‚â• high threshold as strong edges.
3) Starting from these strong edges, follow connected pixels; keep any pixel with magnitude ‚â• low threshold that connects to a strong edge.
4) Discard all remaining pixels.

Effects and assumptions:
- Keeps weak edge segments that bridge gaps between strong edge responses (‚Äúfills in most of the gaps‚Äù).
- Assumes edges form continuous curves.
- User still chooses the two thresholds, but selection is less sensitive than a single threshold.

This is considered the industry-standard thresholding method within the Canny edge detector.
Source: Lec-6A-Derivative and Edges.pdf (COMP2032)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Hysteresis thresholding is a technique used in the Canny edge detector to connect weak edges to strong edges. It uses two thresholds, a high and a low. Pixels with a gradient magnitude above the high threshold are considered strong edges, and pixels between the two thresholds are considered weak edges. Weak edges are only kept if they are connected to strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiÔ¨Åed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eÔ¨ÉciencyFor images of size 480 √ó 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edgesü§î54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren‚Äôt really independent, they make up lines  ‚Ä¢ The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren‚Äôt really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn‚Äôt survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' 'ü•≥ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that ¬´ is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D ‚Äî W)D2z Zz = arg.mingrz‚Äî0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi‚Äîo Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of",
What is the difference between the Sobel and Roberts edge detectors?,"Both Sobel and Roberts are first-derivative (gradient-based) edge detectors that classify edges by thresholding the gradient magnitude, but they differ in kernels, noise sensitivity, and typical use.

Key differences
- Kernel size and orientation
  - Roberts: uses 2√ó2 diagonal difference kernels (estimates gradients along diagonals). Very small footprint.
  - Sobel: uses 3√ó3 kernels to estimate gx and gy (can be extended to larger sizes).
- Noise sensitivity and smoothing
  - Roberts: minimal spatial support and no inherent smoothing ‚Üí very sensitive to noise; strong response only to very sharp edges.
  - Sobel: includes an averaging effect in the 3√ó3 mask ‚Üí more robust to noise; larger Sobel operators are even more stable in noise.
- Edge appearance
  - Roberts: very thin, sharp edges; can be fragmented in noisy images.
  - Sobel: smoother, more continuous edges; can appear slightly thicker unless post-processed.
- Practical use and availability
  - Roberts: less common nowadays.
  - Sobel: still widely used and available in most libraries; supports parameter choices (e.g., kernel size) to trade off detail vs. noise.
- Thresholding
  - Both rely on a user-supplied threshold to decide which gradient magnitudes are edges.

When to use which
- Clean, high-contrast images, or when extreme speed and very thin edges are desired: Roberts.
- Noisy images or general-purpose edge detection with more stable results: Sobel (consider larger kernels if noise is significant).

Notes for practice (from lab guidance)
- Convert images to grayscale before applying either method.
- Good parameter choices (e.g., threshold, Sobel kernel size) are often more important than the specific operator.

Sources: Lec-6A-Derivative and Edges.pdf; Lab 6.pdf (School of Computer Science, COMP2032).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 6.pdf"" target=""_blank"">Lab 6</a>

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","The Sobel and Roberts edge detectors are both first-derivative edge detectors, but they use different kernels to approximate the gradient. The Sobel operator uses 3x3 kernels, while the Roberts operator uses 2x2 kernels.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute ‚Äì 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¬¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V¬∞L f(x. ¬ª) * G(x. y)] = V¬∞G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <‚Äî‚Äî<‚Äî‚Äî37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.‚óè Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute ‚Äì 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¬¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V¬∞L f(x. ¬ª) * G(x. y)] = V¬∞G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <‚Äî‚Äî<‚Äî‚Äî37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.‚óè Read in the Opera house.jpeg image from Moodle.‚óè By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a color histogram?,"A color histogram is a summary of how colors are distributed in an image. It is built by discretizing a chosen color space (e.g., RGB, HSV, or opponent colors) into bins and counting how many pixels fall into each bin. In RGB, this is typically a 3D histogram where each bin corresponds to a range of red, green, and blue values.

How it‚Äôs constructed:
- Choose a color space and the number of bins per channel (e.g., 8√ó8√ó8 = 512 bins).
- For each pixel, find its bin based on its color values and increment that bin‚Äôs count.
- Optionally normalize the histogram so the counts sum to 1 (useful for comparing images of different sizes).

Key properties:
- Invariant to translation and rotation about the viewing axis, and changes only slowly with scale, viewpoint, and partial occlusion.
- Does not encode spatial arrangement‚Äîonly how often colors occur.
- The choice of color space and binning resolution affects performance, but many practical systems work well with moderate quantization (e.g., 8√ó8√ó8 bins).

Common uses:
- Object recognition via histogram comparison (e.g., histogram intersection).
- Localization/tracking via histogram backprojection (highlighting image regions whose colors match a model).
- Image retrieval and coarse segmentation based on color content.

Example:
- Using an 8√ó8√ó8 RGB histogram, all pixels with R in [32‚Äì63], G in [224‚Äì255], B in [0‚Äì31] would contribute to the same bin; the bin‚Äôs count reflects how common that ‚Äúreddish with little blue‚Äù color is in the image.

References:
- Swain and Ballard, ‚ÄúColor Indexing‚Äù (introduces color histograms, histogram intersection, and backprojection). [./documents/Swain1.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",A color histogram is a representation of the distribution of colors in an image. It counts the number of pixels for each color in a discrete color space.,"is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,","is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,",
How can you use a histogram to improve image contrast?,"You improve image contrast by analyzing the histogram and then redistributing pixel intensities so they occupy more of the available range. Two standard approaches from the course are:

1) Histogram equalisation (non‚Äëlinear, aims for a flat histogram)
- Idea: Map pixel values so the output histogram is (nearly) uniform, boosting contrast especially in low-contrast regions.
- How: Compute the histogram and its cumulative distribution function (CDF), then map each input level r to s = round((L‚àí1)¬∑CDF(r)), where L is the number of grey levels.
- In practice (OpenCV): use cv2.equalizeHist on a grayscale image.
- Typical effect: Narrow/peaked histograms (low contrast) become more spread out. It is parameter-free and more general than linear stretching.

2) Contrast stretching (linear normalization)
- Idea: Linearly map the current intensity range [min, max] to the full display range, usually [0, 255].
- Formula: I_out = (I ‚àí min) √ó 255 / (max ‚àí min).
- In practice (OpenCV): cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).
- Typical effect: Expands the histogram to fill the full range but preserves the original relative spacing (linear mapping).

Practical workflow (as in the lab):
- Convert to grayscale.
- Plot the histogram (e.g., 256 bins). A low-contrast image shows a histogram squeezed into a small intensity range, while a high-contrast image is spread out.
- Apply:
  - Histogram equalisation: equ = cv2.equalizeHist(gray)
  - Contrast stretching: stretch = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX)
- Compare images and their histograms.

Optional adjustments and their histogram effects:
- Gain (alpha): scales intensities, stretches/compresses histogram around 0.
- Bias (beta): shifts intensities, moves histogram left/right.
- Gamma: non-linear remapping I_out = 255¬∑(I/255)^Œ≥; Œ≥<1 brightens (pulls histogram right), Œ≥>1 darkens (pulls left). Inspect how these change the histogram and how equalisation responds afterward.

Notes:
- Normalised histogram bins sum to 1; each bin gives the probability of that grey level, which is useful for understanding and guiding contrast enhancement.
- Equalisation vs stretching: equalisation is non-linear and tries to flatten the histogram automatically; stretching is linear and depends on the data range you map.

If you want, I can provide a minimal Python/OpenCV snippet to compute and visualise these steps.

Sources:
- Lecture 2: Histogram (Whole Image Understanding)
- Lab 2: Histogram Equalisation and Contrast Stretching

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",Histogram equalization can be used to improve image contrast by redistributing the pixel intensities to create a more uniform histogram.,"the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image ‚Ä¢ To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values ‚Ä¢ More general than linear or piecewise contrast stretching - No parameters to specify, ‚Äúapply a transform that makes the output histogram FLAT‚Äù Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability","the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @¬©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image ‚Ä¢ To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values ‚Ä¢ More general than linear or piecewise contrast stretching - No parameters to specify, ‚Äúapply a transform that makes the output histogram FLAT‚Äù Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‚ÄòI'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability",
What is a binary image?,"A binary image is an image in which each pixel has only two possible values, typically representing foreground vs. background. Commonly these values are:
- 0 and 1 (logical/boolean), or
- 0 and 255 in 8-bit images (0 = black, 255 = white).

Binary images are often produced by thresholding a grayscale (or color) image, and are widely used for tasks like segmentation, edge detection, morphology (erosion/dilation), and creating masks/ROIs. Note that binary images may still need noise removal or enhancement after thresholding.

Source: Lecture 4 ‚Äî Thresholding & Binary Images; Lab 4 ‚Äî Thresholding & Binary Images.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image is an image where each pixel can have only one of two values, typically 0 or 1 (or black and white).","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e¬© Nottingham ‚Äúi UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‚ÄòThe PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding‚Ä¢ A dark object on a light background in a grey-level image ‚Ä¢ Choose a threshold value, T ‚Ä¢ Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform",
How can you create a binary image from a grayscale image?,"You create a binary image by thresholding the grayscale image so each pixel becomes either 0 (black) or 255 (white). Common approaches:

1) Global (fixed) threshold
- Pick a threshold T.
- If pixel ‚â• T ‚Üí 255 (foreground), else 0 (background).

OpenCV (Python):
- img must be grayscale (uint8).
- _, binary = cv2.threshold(img, T, 255, cv2.THRESH_BINARY)

2) Otsu‚Äôs automatic threshold
- Lets the algorithm choose T from the image histogram.
- Useful when the histogram is bimodal.

OpenCV (Python):
- _, binary_otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

3) Adaptive thresholding
- Computes a local threshold for each pixel (good for uneven lighting).
- Methods: mean or Gaussian.

OpenCV (Python):
- binary_adapt = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                       cv2.THRESH_BINARY, blockSize=11, C=2)
- or use cv2.ADAPTIVE_THRESH_GAUSSIAN_C

Practical tips
- Denoise first (e.g., Gaussian blur) to get cleaner masks.
- Inspect the histogram to pick a reasonable global T, and compare with Otsu‚Äôs result to see which is better for your image.

Reference: Lab 4: Thresholding & Binary Images (COMP2032). If you need more detail on when to choose each method, I can elaborate.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image can be created from a grayscale image using thresholding. A threshold value is chosen, and all pixels with an intensity value above the threshold are set to one value (e.g., 1), while all pixels below the threshold are set to the other value (e.g., 0).","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:‚óè Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.‚óè Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let‚Äôs retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):‚Ä¶image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let‚Äôs read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f¬∞ University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, ‚Ç¨ {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is ‚Äúcoherent‚Äù, reflecting a tendency to solidity of objects. This is captured by a ‚ÄúGibbs‚Äù energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = ‚Äîlogh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
What are connected components in a binary image?,"Connected components are the ‚Äúobjects‚Äù in a binary image: each is a maximal set of foreground pixels that are mutually connected under a chosen connectivity rule.

Key points:
- Foreground/background: Typically black pixels are treated as objects (foreground) and white as background.
- Connectivity choice:
  - 4-connected: pixels are connected through up, down, left, right.
  - 8-connected: also counts the four diagonals as connected.
  The choice affects whether diagonally touching pixels belong to the same component.
- Labeling: Connected-component labeling converts the binary image into a label image, assigning a unique integer label to every distinct component so all pixels in the same object share the same label.
- Typical algorithm (sequential, 4-neighbour example):
  - First pass: scan top-left to bottom-right; look at already-visited neighbours (top and left). If they have labels, reuse one (record equivalences if they differ); if not, create a new label.
  - Second pass: resolve label equivalences so each component has a single consistent label.
- Why it‚Äôs useful: Once labeled, you can compute properties per component (e.g., area, centroid, bounding box) and process objects individually.

If you like, I can show a tiny example or code snippet to demonstrate 4- vs 8-connectivity.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

","Connected components are sets of connected pixels in a binary image that share the same value (usually the ""on"" value). They represent distinct objects or regions in the image.","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 ‚Äì 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) ‚Äì 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‚Äò ‚Äîs Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 ‚Äì 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) ‚Äì 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‚Äò ‚Äîs Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython‚Äôs array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -",
How can you use connected components analysis?,"Connected components analysis (CCA) lets you turn a binary image into a label image so each distinct object gets a unique ID. You can then measure and manipulate each object separately. In the module, we cover the idea, the two-pass algorithm, and typical uses like masking and morphology pre/post-processing.

What it‚Äôs for
- Count objects, measure area/centroid/bounding box.
- Keep the largest object, remove small noise.
- Create masks/ROIs to apply further processing only to selected components.

Key ideas from the course
- Choose connectivity (4- or 8-neighbourhood). The lecture‚Äôs sequential algorithm shows 4-neighbours, checking only already-scanned pixels (top and left).
- Two-pass algorithm:
  1) First pass: scan top-left to bottom-right. For a foreground pixel:
     - If top and left are background ‚Üí assign a new label.
     - If one neighbour is labeled ‚Üí copy that label.
     - If both are labeled and different ‚Üí assign one and record an equivalence between labels.
  2) Second pass: resolve label equivalences so all pixels of the same object share one canonical label.
- Note: Connected component algorithms can be a bottleneck; keep a separate output array for labels.
- Binary images can be used as masks to select which pixels operations apply to.
(Sources: Lec-5-Morphology.pdf; Practicals - Morphology.pdf; Practicals with Solutions - Morphology.pdf)

Typical workflow
1) Preprocess: convert to grayscale, threshold to binary (ensure ‚Äúforeground‚Äù is the object pixels). Optionally clean using morphology (opening/closing) to remove noise or fill holes.
2) Label: run CCA to get a label image.
3) Measure/filter: compute area, bounding box, centroid; remove small components, keep the largest, etc.
4) Mask or extract: build a mask from chosen labels and apply to the original image.

Minimal Python example (OpenCV)
- Threshold to binary (foreground=255), then label and use stats:
  - import cv2, numpy as np
  - img = cv2.imread('image.png', cv2.IMREAD_GRAYSCALE)
  - _, bin = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
  - num, labels, stats, centroids = cv2.connectedComponentsWithStats(bin, connectivity=8)
  - # stats columns: [x, y, width, height, area]; centroids: (cx, cy)

Examples of use
- Count objects:
  - object_count = num - 1  # exclude background label 0
- Remove small blobs:
  - min_area = 100
  - keep = np.zeros_like(bin)
  - for label in range(1, num):
      - if stats[label, cv2.CC_STAT_AREA] >= min_area:
        - keep[labels == label] = 255
- Keep the largest component:
  - largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
  - mask = (labels == largest).astype(np.uint8) * 255
- Extract object on black background:
  - color = cv2.imread('image.png', cv2.IMREAD_COLOR)
  - result = cv2.bitwise_and(color, color, mask=mask)
- Use as ROI/mask for further processing (e.g., blur only inside mask).

Morphology with CCA
- Before CCA: opening (erode then dilate) removes small noise; closing (dilate then erode) fills small gaps.
- After CCA: use component masks to refine or measure per-object.
(Sources: Lec-5-Morphology.pdf; Practicals - Morphology.pdf; Practicals with Solutions - Morphology.pdf)

If you want, share an example image and your goal (e.g., ‚Äúkeep the largest object‚Äù), and I can suggest precise preprocessing and filtering steps. If you need more detail on the two-pass algorithm implementation from first principles, I can walk you through that too.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals - Morphology.pdf"" target=""_blank"">Practicals   Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Morphology.pdf"" target=""_blank"">Practicals with Solutions   Morphology</a>

","Connected components analysis is used to identify and label the distinct objects in a binary image. Once labeled, properties of each component, such as its area, perimeter, and shape, can be calculated.","basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.‚óè Use rotate to rotate the image by 90 degrees. Display the result.‚óè Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.‚óè imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated","basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.‚óè Use rotate to rotate the image by 90 degrees. Display the result.‚óè Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.‚óè imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background ‚Üí assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of images is a ated",
What is the difference between 4-connectivity and 8-connectivity?,"Short answer:
- 4-connectivity: A pixel is connected only to its N, S, E, W neighbours.
  Neighbour set of (i, j): {(i‚àí1,j), (i+1,j), (i,j‚àí1), (i,j+1)}.
- 8-connectivity: Adds the four diagonal neighbours (NE, NW, SE, SW).
  Neighbour set adds {(i‚àí1,j‚àí1), (i‚àí1,j+1), (i+1,j‚àí1), (i+1,j+1)}.

Consequences:
- Connected components: Diagonally touching pixels are separate with 4-connectivity but merged with 8-connectivity.
  Example: Two foreground pixels at (0,0) and (1,1) form two components with 4-connectivity, one component with 8-connectivity.
- Choice affects segmentation/labeling: 4-connectivity can prevent merges via thin diagonal contacts; 8-connectivity gives fewer, larger components and treats diagonals as contiguous.
- To avoid topological ambiguities (object and background both appearing connected), some tasks use mixed connectivity (e.g., 8 for foreground, 4 for background).

Source: Segmentation lecture notes (connected components examples) [Lec-7-Segmentation.pdf].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-7-Segmentation.pdf"" target=""_blank"">Lec 7 Segmentation</a>

","In a 2D grid, 4-connectivity considers only the top, bottom, left, and right neighbors of a pixel. 8-connectivity considers these four neighbors as well as the four diagonal neighbors.","< k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than ‚Äúie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an‚Äî-2am‚Äî¬´¬£ +1)-‚Äîk+3an‚Äî- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) ‚Äî assoc(A, A) assoc(A, V) 4 assoc(B, V) ‚Äî assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2‚Äî Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","< k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than ‚Äúie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an‚Äî-2am‚Äî¬´¬£ +1)-‚Äîk+3an‚Äî- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) ‚Äî assoc(A, A) assoc(A, V) 4 assoc(B, V) ‚Äî assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2‚Äî Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
What is a Region of Interest (ROI)?,"A Region of Interest (ROI) is a selected subset of an image on which you focus analysis or processing, ignoring the rest. It lets you apply operations only where they are needed.

Common ways to represent an ROI:
- Bounding box or polygon (geometric selection)
- Binary mask the same size as the image, with 1/True for pixels inside the ROI and 0/False outside

Why ROIs are useful:
- Apply different processing in different areas (e.g., different thresholds for light vs. dark regions)
- Compute statistics only within a region
- Restrict filtering or morphology to relevant areas
- Speed up processing by ignoring background

Example from the course materials:
- Define ROIs by colour to split an image into two masks, then compute and apply separate thresholds per mask (e.g., thresh(pg[mask == 0]) and thresh(pg[mask == 1])) so each region gets an appropriate binarization.

Sources:
- Lec-5-Morphology.pdf (ROIs and Masks; ‚ÄúROIs defined by colour‚Äù and selective processing)
- Practicals with Solutions - Thresholding and Binary Images.pdf (separate thresholds per mask region)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Thresholding and Binary Images.pdf"" target=""_blank"">Practicals with Solutions   Thresholding and Binary Images</a>

",A Region of Interest (ROI) is a selected area of an image that is targeted for a specific purpose or analysis.,"pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‚Äòpendent high-levet requires that, first, the MIVURU representat : ‚Äòaction process be contnes aramelers corresponding ¬£ domain, is Of Images is a paradigm alec sroals. A feahire space ig ‚ÄúdA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon ¬ß domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‚Äò the feature of interest im into a point in thes parameter. After the entiaction process be et rame‚Äòers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each‚Ä¶8 & &‚Ç¨ 8 8 8 8 8B 7250 150o ¬ß@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds,","pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‚Äòpendent high-levet requires that, first, the MIVURU representat : ‚Äòaction process be contnes aramelers corresponding ¬£ domain, is Of Images is a paradigm alec sroals. A feahire space ig ‚ÄúdA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon ¬ß domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee ‚Äî | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphologyü§î32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‚Äò the feature of interest im into a point in thes parameter. After the entiaction process be et rame‚Äòers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‚Äòpendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each‚Ä¶8 & &‚Ç¨ 8 8 8 8 8B 7250 150o ¬ß@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions‚Äô thresholds,",
How is a graph used in the Livewire segmentation algorithm?,"Short answer: Livewire (Intelligent Scissors) turns the image into a graph and finds minimum-cost paths that ‚Äúsnap‚Äù to edges.

How the graph is built and used:
- Nodes: Each pixel becomes a node (typically on a 4- or 8-connected grid).
- Edges: Each node is connected to its neighboring pixels; these connections are the graph edges.
- Edge weights (costs): Computed from edge-likeliness features so that strong, likely boundaries are cheap to follow. Common cues (from Mortensen & Barrett, 1995) include:
  - High image gradient magnitude ‚Üí lower cost
  - Presence of a Laplacian zero-crossing (edge) ‚Üí lower cost
  - Sometimes gradient direction alignment and edge continuity are included
- Shortest path: When you click a seed (anchor) point, the algorithm runs a shortest-path search (e.g., Dijkstra using a priority queue/heap) from that seed to all other pixels, producing a predecessor map.
- Live interaction: As you move the cursor (the ‚Äúfree‚Äù point), the current shortest path from the cursor back to the seed is displayed in real time (the ‚Äúlive wire‚Äù), snapping to object boundaries because those routes have the lowest cumulative cost.
- Final boundary: When you place another anchor, the current shortest path is fixed as part of the segmentation, and the process repeats to trace the whole ROI.

Why a graph? Converting the image to a graph lets boundary finding be solved as a global optimal path problem with efficient algorithms, and supports interactive, responsive updates as the user moves the cursor.

Sources:
- COMP2032 Laboratory Sheet: Livewire (notes the steps ‚ÄúConvert Image to Graph‚Äù and ‚ÄúShortest Path Algorithm‚Äù and that it uses heapq for Dijkstra-like search)
- Mortensen & Barrett, ‚ÄúIntelligent Scissors for Image Composition,‚Äù SIGGRAPH ‚Äô95 (describes the graph formulation and cost function)

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

<a href=""http://localhost:5003/documents/Practicals - Interactive Segmentation.pdf"" target=""_blank"">Practicals   Interactive Segmentation</a>

<a href=""http://localhost:5003/documents/Practicals with Solutions - Interactive Segmentation.pdf"" target=""_blank"">Practicals with Solutions   Interactive Segmentation</a>

","In the Livewire algorithm, the image is converted into a graph where each pixel is a node. The edge weights between adjacent pixels are determined by the image's edge information, such as intensity differences.","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH ¬∞95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about‚Ä¶1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by‚Ä¶ A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the role of Dijkstra's algorithm in Livewire?,"Dijkstra‚Äôs algorithm is the engine that makes Livewire ‚Äúsnap‚Äù to object boundaries.

- After converting the image to a graph (pixels = nodes; edges connect neighboring pixels), each edge is given a cost based on edge evidence (e.g., gradient magnitude‚Äîstrong edges get low cost).
- When you set a seed point, Livewire runs Dijkstra‚Äôs algorithm from that seed to compute the shortest-path tree over the image graph.
- As you move the cursor (the free point), the current boundary segment is obtained by following the predecessor pointers from the free point back to the seed along the shortest-path tree. This updates instantly, producing the live ‚Äúwire.‚Äù
- When you accept a segment and place a new seed, Dijkstra is run again from the new seed.

In short, Dijkstra‚Äôs algorithm finds the globally minimal-cost path between the seed and any cursor position under the chosen edge-cost function, ensuring the boundary adheres to strong image edges. (See lab notes and the Intelligent Scissors paper for details.) [COMP2032 Laboratory Sheet 8b - Livewire.pdf; mort-sigg95.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf"" target=""_blank"">COMP2032 Laboratory Sheet 8b   Livewire</a>

<a href=""http://localhost:5003/documents/mort-sigg95.pdf"" target=""_blank"">mort sigg95</a>

",Dijkstra's algorithm is used in the Livewire algorithm to find the shortest path between a user-selected start point and end point on the image graph. This shortest path corresponds to the object boundary.,"f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.‚Ä¢ Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if‚Äô University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:‚Ä¢ The image is converted into a graph where each pixel represents a node.‚Ä¢ The edge weights between adjacent pixels are determined based on the image's edge information.‚Ä¢ Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:‚Ä¢ The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively ‚Äúsnap‚Äù and ‚Äúwrap‚Äù the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. SpeciÔ¨Åcally, note the live-wire segments corresponding to user-speciÔ¨Åed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-Ô¨Çy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input","f¬∞ University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:‚Ä¢ Matplotlib: Used for creating plots and visualisations.‚Ä¢ Skimage: Used for colour space conversion and manipulation in images.‚Ä¢ Math: Used for computing the absolute value of a floating-point number.‚Ä¢ Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.‚Ä¢ Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if‚Äô University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:‚Ä¢ The image is converted into a graph where each pixel represents a node.‚Ä¢ The edge weights between adjacent pixels are determined based on the image's edge information.‚Ä¢ Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:‚Ä¢ The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively ‚Äúsnap‚Äù and ‚Äúwrap‚Äù the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. SpeciÔ¨Åcally, note the live-wire segments corresponding to user-speciÔ¨Åed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-Ô¨Çy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input",
What are the main steps in the GrabCut algorithm?,"Here are the main steps in the GrabCut algorithm:

- User initialization
  - Draw a loose rectangle around the object (or paint rough foreground/background strokes).
  - Pixels outside the rectangle are marked sure background; pixels inside are marked ‚Äúprobable‚Äù foreground. With strokes, some pixels can be marked sure foreground/background. [Laboratory Sheet 8 - Interactive Segmentation.pdf; Lec-8-Interactive Segmentation.pdf; grab_cut.pdf]

- Color modeling (GMMs)
  - Model foreground and background color distributions with Gaussian Mixture Models (typically K components per class).
  - Initialize from the current labeling (e.g., k-means inside/outside) and assign each pixel to a GMM component. [grab_cut.pdf]

- Energy construction
  - Define an energy with:
    - Data term: -log likelihood of each pixel color under FG/BG GMMs (drives pixels to the class that explains their color best).
    - Smoothness term: penalizes neighboring pixels taking different labels, weighted by their color contrast (encourages coherent regions and aligns to edges). [grab_cut.pdf; Lec-8-Interactive Segmentation.pdf]

- Graph cut optimization (min-cut/max-flow)
  - Build a graph with t-links (data terms) and n-links (smoothness terms), and compute the minimum s‚Äìt cut to obtain a new FG/BG labeling. (Implemented with standard max-flow/min-cut algorithms.) [grab_cut.pdf; An_experimental_comparison_of_min-cut_max-_flow_algorithms_for_energy_minimization_in_vision.pdf]

- Iterative refinement
  - Re-estimate the FG/BG GMM parameters from the new labeling, rebuild the graph, and run graph cut again.
  - Repeat until convergence or for a fixed number of iterations. This iterative loop is the key extension over basic graph cut. [grab_cut.pdf]

- Optional border matting
  - Estimate a soft alpha matte around the boundary to avoid haloing and produce smooth, bleed-free composites. [grab_cut.pdf; Lec-8-Interactive Segmentation.pdf]

- Interactive updates (if needed)
  - If the result is not satisfactory, the user adds foreground/background strokes; the mask is updated and the optimization loop is rerun. (In OpenCV: GC_INIT_WITH_RECT or GC_INIT_WITH_MASK; bgdModel/fgdModel are updated across iterations.) [Laboratory Sheet 8 - Interactive Segmentation.pdf; Lec-8-Interactive Segmentation.pdf]

If you‚Äôd like, I can also show minimal OpenCV code that follows these steps.

**Cited Materials:**

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

<a href=""http://localhost:5003/documents/Laboratory Sheet 8 - Interactive Segmentation.pdf"" target=""_blank"">Laboratory Sheet 8   Interactive Segmentation</a>

","The GrabCut algorithm involves an iterative process of: 1) assigning GMM components to pixels, 2) learning GMM parameters from the data, 3) estimating the segmentation using a min-cut algorithm, and repeating until convergence.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let‚Äôs try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‚Äòinput‚Äô to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if‚Äô University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|¬Æ ‚Äî\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for ‚Äúborder matting‚Äù has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let‚Äôs try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‚Äòinput‚Äô to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if‚Äô University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:",
What is the purpose of the Transformer in the Vision Transformer (ViT) model?,"In ViT, the Transformer is the core feature extractor. Its purpose is to:

- Take the image split into fixed-size patches (each linearly projected to an embedding), add positional encodings, and prepend a learnable [CLS] token.
- Use stacked multi-head self-attention and feed-forward layers to model global relationships between all patches, capturing long-range dependencies and context.
- Produce a compact, task-ready representation; the final [CLS] token output is used for image classification (or other heads for different tasks).

In short: the Transformer replaces convolutions by learning global, contextual features from patch embeddings via self-attention, then outputs a representation for prediction.",The Transformer in a Vision Transformer (ViT) model is used to capture both local and global relationships within an image by treating the image as a sequence of patches.,"words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNNü§î22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNNü§î22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the main difference between CNNs and ViTs?,"Main differences:

- Core operation
  - CNNs: use learned convolution filters applied locally over the 2D image grid (weight sharing, local receptive fields), plus pooling to build hierarchical, translation-equivariant features.
  - ViTs: split the image into fixed-size patches, linearly project each patch to an embedding, add positional embeddings, then process the sequence with a Transformer encoder using global self-attention (often with a [CLS] token for classification).

- Inductive bias and data needs
  - CNNs have strong image-specific inductive biases (locality, translation equivariance), so they work well with limited data.
  - ViTs have weaker inductive bias and rely on self-attention to model global relationships; they typically benefit from large-scale pretraining or heavy augmentation.

- Receptive field
  - CNNs grow receptive fields gradually across layers.
  - ViTs can model long-range/global dependencies from the earliest layers via attention.

- Computational pattern
  - CNN cost scales with kernel size and feature map resolution.
  - Vanilla ViT self-attention scales roughly quadratically with the number of patches (though many variants mitigate this).

In short: CNNs are convolution-based with strong locality/translation priors; ViTs are attention-based on patch embeddings with global context modeling. Neither is universally ‚Äúbetter‚Äù‚Äîperformance depends on data size, compute, and task.","CNNs use convolutional layers to extract features from an image, which are inherently local operations. ViTs, on the other hand, divide the image into patches and use a Transformer architecture to learn relationships between these patches, allowing them to capture more global context.","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha‚ÄîS Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8‚Äî_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!-Thank you for trying!|the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:‚óè Create and display the histogram.‚óè Use equalizeHist to perform histogram equalisation and display the histogram of the result.‚óè Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:‚óè Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).‚óè Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by",
What is the purpose of zero-padding in convolutional layers?,"Zero-padding means adding rows/columns of zeros around the image before applying a convolution.

Main purposes:
- Preserve/control output size: with padding, you can keep the feature map the same spatial size as the input (‚Äúsame‚Äù convolution) instead of shrinking each layer.
- Let filters process border pixels: ensures the filter ‚Äúfits‚Äù everywhere, so edge information isn‚Äôt discarded.
- Enable deeper networks without rapid spatial shrinkage and keep sizes compatible with chosen strides/pooling.

Quick example: a 5√ó5 image convolved with a 3√ó3 filter
- Without padding ‚Üí output is 3√ó3
- With padding of 1 pixel on all sides ‚Üí output stays 5√ó5

Sources: Lec-9-CNN.pdf; Practicals - CNN.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

",Zero-padding is the process of adding zeros around the border of an input image before applying a convolutional filter. This is done to control the spatial size of the output feature map and to ensure that the filter can be applied to pixels at the edge of the image.,"Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep","Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‚Äò17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16¬∞ | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep",
What is stride in a convolutional layer?,"Stride is the number of pixels the convolutional filter moves (slides) after each operation over the input. 

Key effects:
- Stride 1 moves the filter one pixel at a time, producing a larger feature map with more spatial detail.
- Larger strides (e.g., 2) ‚Äújump‚Äù further, reducing the feature map size (acts like downsampling) and lowering computation.
- Together with kernel size and zero-padding, stride determines the output dimensions.

Example: With a 5√ó5 input, 3√ó3 kernel, no padding:
- Stride 1 ‚Üí output size 3√ó3
- Stride 2 ‚Üí output size 2√ó2

Source: Lec-9-CNN.pdf (slides: ‚ÄúConvolutional Layer‚Äù ‚Äì Stride: ‚Äú# of pixels the filtering window moves after each operation‚Äù).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","Stride is the number of pixels by which the filter moves across the image at each step. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time, resulting in a smaller output feature map.
QA about Material Content","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights","Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & ‚Äòtapped into a point in th parameter. After the entitB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entireB: a small gaussian (ùùà = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries ‚Äì ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) ‚Äì Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights",
What are the deliverables for the coursework?,"Per the Module Assessment Sheet (COMP2032, Introduction to Image Processing), the group coursework deliverables are:

- Python code: all .py files, submitted as a single ZIP archive
- A conference paper: 6 pages, maximum 2000 words, in PDF format (note: extra pages will incur a deduction)

Source: Module Assessment Sheet for 2024‚Äì2025 (Assessment Sheet COMP2032 Coursework 2025.pdf)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

","The coursework requires a group submission of Python code files (as a ZIP folder), a 6-page conference paper (2000 words max, PDF format), and result images (as a ZIP folder). There is also an individual submission of a peer assessment form.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the deadline for the coursework submission?,"Friday, 25 April 2025, by 11:59 pm. 
Source: Assessment Sheet COMP2032 Coursework 2025.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The submission date for the coursework is Friday, 25 April 2025, by 11:59 pm.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the penalty for late submission?,"I don‚Äôt see any late-submission policy in the materials I have. It‚Äôs likely in the full assessment brief or on the module‚Äôs Moodle page.

Please contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office: Room BB71, Block B, Malaysia Campus
- Teaching Assistant: Mr. Irfan Yaqub: hcxiy1@nottingham.edu.my

If you can share the exact brief or confirm which assessment this is for (coursework code/paper), I can double-check the details.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",Work submitted after the deadline is subject to a penalty of 5 marks for each late working day.,"reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3","reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3",
What is the topic of the coursework?,"The coursework centers on building a Python image-processing application and writing a 2000‚Äëword conference‚Äëstyle paper that includes a literature review, an explanation of your method, and an evaluation of results. It is assessed as 100% coursework, with an in‚Äëlab test component included.

Sources: Lec-11-Finale and Revision.pdf; Lec-1A-Introduction.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

<a href=""http://localhost:5003/documents/Lec-11-Finale and Revision.pdf"" target=""_blank"">Lec 11 Finale and Revision</a>

",The coursework involves developing a software solution in Python for the semantic segmentation of flowers. This includes creating an image processing pipeline to separate flowers from the background and then classifying them.,"practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main parts of the coursework?,"Here are the main parts of the COMP2032 coursework, based on the assessment info provided:

Group deliverables:
- Python application: Implement your image processing pipeline in Python.
- Conference paper: 6 pages, max 2000 words, PDF. Use the provided template COMP2032-CW-GroupXXX.docx (rename XXX to your group number). Do not change formatting; marks will be deducted if you do. There is a 5% penalty for every additional page for the entire group.
- Result images: Submit output/result images as a ZIP (include an ‚ÄúOutput‚Äù folder and an ‚ÄúImage Processing Pipeline‚Äù folder as specified).
- Provided resources: You will be given two datasets and a Transformer base to use in your work.

Individual requirement:
- Peer assessment form: Each student must submit this separately via the ‚ÄúPeer Assessment‚Äù link on Moodle. Not submitting incurs a 10% penalty on your overall group mark.

Additional note:
- The module overview also mentions an in-lab test as part of the assessment structure alongside the programming and paper components.

If you want, I can turn this into a short submission checklist for your group.

Sources: Assessment Sheet COMP2032 Coursework 2025.pdf; Lec-1A-Introduction.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The coursework has two main parts: a SEGMENTATION PART where you design and implement an image processing pipeline to segment flowers from images in Dataset 1, and a SEMANTIC PART where you classify the entire flower dataset (Dataset 2) using a provided Transformer-based model.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
What are the requirements for the output images in the segmentation part?,"Here‚Äôs what your segmentation output images must satisfy:

Coursework output requirements
- One output per input image.
- File format: .jpg.
- Organisation: save in an output folder with three sub-folders (e.g., easy, medium, hard).
- Naming: keep the same base filename as the input image.
- Appearance: black background with only the segmented flower visible (no other content).
- Optional: you may save intermediate pipeline images in an image-processing-pipeline folder, with sub-folders named after each input image.
(Source: Assessment Sheet COMP2032 Coursework 2025)

For using the Segmentation Evaluation Model
- Accepted formats: .jpg or .png.
- Consistency: segmented outputs and ground-truth masks must use the same format and matching filenames.
- Size: 256√ó256 pixels (otherwise they will be auto-resized, which may introduce distortions).
- Folder structure:
  - segmentation_submission/
    - segmented/ image_001.png, image_002.png, ‚Ä¶
    - ground_truth/ image_001.png, image_002.png, ‚Ä¶
- Metrics reported: Cosine Similarity and pixel-wise Accuracy.
(Source: Guidelines for Using the Segmentation Evaluation Model)

If you want, I can provide a tiny example folder tree with a couple of filenames to check your setup.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

<a href=""http://localhost:5003/documents/Guidelines for Using the Segmentation Evaluation Model.pdf"" target=""_blank"">Guidelines for Using the Segmentation Evaluation Model</a>

","The output images must have a black background with only the segmented flower. They should be saved as .jpg files in an ""output"" folder, categorized into sub-folders named ""easy,"" ""medium,"" and ""hard,"" corresponding to the input images.","f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided","f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided",
What evaluation metrics should be used for the segmentation part?,"For the coursework‚Äôs segmentation evaluation, two metrics are used:

- Cosine Similarity: Measures high-level similarity between your segmented image and the ground-truth mask. Score ranges from 0 to 1 (higher is better).
- Pixel-wise Accuracy: Proportion of pixels that match between your segmentation and the ground-truth mask.

Source: Guidelines for Using the Segmentation Evaluation Model (COMP2032).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Guidelines for Using the Segmentation Evaluation Model.pdf"" target=""_blank"">Guidelines for Using the Segmentation Evaluation Model</a>

<a href=""http://localhost:5003/documents/Metric for Image Segmentation.pdf"" target=""_blank"">Metric for Image Segmentation</a>

",The evaluation of the segmentation pipeline can be both qualitative and quantitative. The coursework suggests using mean IoU (Intersection over Union) as a metric.,"f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| ‚Äî MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results‚Ä¶.BeWhat we can do with metrics?‚Ä¢ Automatic evaluation‚Ä¢ Comparison between differenti* / ,*approaches‚Ä¢ Evaluation from multiple perspectives‚Ä¢ ‚Ä¶Sim√©oni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry √© √© 70 ‚Äòslgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the","f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| ‚Äî MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results‚Ä¶.BeWhat we can do with metrics?‚Ä¢ Automatic evaluation‚Ä¢ Comparison between differenti* / ,*approaches‚Ä¢ Evaluation from multiple perspectives‚Ä¢ ‚Ä¶Sim√©oni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry √© √© 70 ‚Äòslgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the",
What is the purpose of Dataset 1?,"I‚Äôm not sure which resource ‚ÄúDataset 1‚Äù refers to from the materials I have. It isn‚Äôt explicitly named in the snippets of the Segmentation slides or the Lab Sheet 9 (CNN) notes I can see.

Could you tell me:
- Which document or lab/assignment it‚Äôs from (e.g., ‚ÄúLab Sheet X‚Äù, ‚ÄúLec-Y slides‚Äù), and
- Any filename or brief description shown next to ‚ÄúDataset 1‚Äù?

If you mean:
- Lab Sheet 9 ‚Äì CNN: the dataset used there is MNIST, whose purpose is to provide training/testing images of handwritten digits for building a CNN classifier.
- The Segmentation lecture examples: those small labeled grids are toy datasets used to illustrate segmentation/labeling concepts (e.g., connected components/region growing).

If this is about a different ‚ÄúDataset 1‚Äù in the module materials, please share a screenshot or exact wording and I‚Äôll help right away. Alternatively, you can confirm with:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71 Block B, Malaysia Campus
- TA: Mr Irfan Yaqub: hcxiy1@nottingham.edu.my","Dataset 1 contains images of three species of flowers, categorized into ""easy,"" ""medium,"" and ""hard"" sub-folders, along with their corresponding ground truths. This dataset is to be used for developing and evaluating the image processing pipeline for the segmentation part of the coursework.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What is the purpose of Dataset 2?,"Dataset 2 is for the semantic segmentation part of the coursework. It contains 847 images with corresponding ground-truth masks and should be used to develop and evaluate your semantic segmentation pipeline (e.g., using mIoU). [Assessment Sheet COMP2032 Coursework 2025]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","Dataset 2, consisting of 847 images with ground truths, is to be used for the semantic part of the coursework, which involves classifying the entire flower dataset.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What are the requirements for the conference paper?,"Based on the coursework brief, the conference paper must meet the following requirements:

Format and length
- Max 6 pages AND max 2000 words
- Submit as a PDF
- Exceeding the page limit incurs a 5% deduction per additional page for the entire group
- Use the provided template: COMP2032-CW-GroupXXX.docx (replace XXX with your group number, e.g., 013)
- Do not change the template‚Äôs formatting or headers (marks will be deducted if you do)
- Replace the provided table and all red/blue placeholder text with your own content

What to include in the paper
1) Methods and techniques
   - Describe the chosen method(s) and specific image processing technique(s) you employed
2) Rationale
   - Explain why you chose those techniques; include pseudocode if applicable
3) Results
   - Present results from your practical implementation
4) Critical evaluation of methods
   - Discuss strengths and weaknesses based on your results
   - Explicitly refer to features seen in your results
   - Support your discussion using relevant segmentation metrics (as per the segmentation evaluation model guidelines)
5) Classification evaluation
   - Critically evaluate your overall pipeline via pixel-wise classification
   - Use cosine similarity and accuracy metrics (generated from your semantic component) to support your claims

Related submission items
- Result images must be submitted separately as a ZIP (with ‚ÄúOutput‚Äù and ‚ÄúImage Processing Pipeline‚Äù folders)
- Each student must also submit an individual peer assessment form on Moodle (10% penalty if not submitted)

If you need the exact wording or template, check the Assessment Sheet and the provided template file on Moodle. If anything is unclear, you can confirm details with Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or the TA, Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The conference paper should be a maximum of 6 pages (2000 words), detailing the chosen methods, justifying their selection, presenting the results, and critically evaluating the implemented pipeline and classification results. It must be submitted in PDF format using the provided template.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
Are we allowed to modify the provided Transformer and Model_input Python files?,"I don‚Äôt have any instruction in the provided materials that mentions rules for modifying Transformer.py or Model_input.py, so I can‚Äôt confirm either way.

What you can do now:
- Check the assignment brief/README and the header comments in those files for phrases like ‚ÄúDo not modify‚Äù or ‚ÄúOnly edit between TODO markers.‚Äù
- If you share the exact wording from the brief here, I can interpret it for you.
- When in doubt, email:
  - Dr. Tissa Chandesa (module convenor): Tissa.Chandesa@nottingham.edu.my
  - TA: Mr Irfan Yaqub: hcxiy1@nottingham.edu.my

General safe practice (if modifications are allowed):
- Keep file names, class/function names, and function signatures unchanged (the autograder often depends on these).
- Only edit within clearly marked TODO blocks.
- If you add helpers, don‚Äôt break the existing import paths or I/O formats expected by the evaluation script. 
- Avoid changing the evaluation pipeline or data loading behavior unless explicitly permitted.

You can use the following button to send an email: <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Clarification on modifying Transformer.py and Model_input.py&body=Dear Dr. Chandesa,

I hope you are well. I‚Äôm working on the Introduction to Image Processing coursework and would like to confirm whether we are allowed to modify the provided files Transformer.py and Model_input.py.

Specifically:
- Are we permitted to change internal implementations in these files, or should we only complete marked TODO sections?
- Must we keep file names and public function/class names and signatures unchanged (for compatibility with the autograder/testing scripts)?
- Are there any sections explicitly marked ‚ÄúDo not modify‚Äù that we should strictly leave as-is?

Context:
- Assignment/Lab name: [please fill]
- Deadline: [please fill]

My details:
- Name:
- Student ID:
- Tutorial group (if applicable):

Many thanks for the clarification.

Best regards,
[Your Name]"">Draft email to Dr. Chandesa</a>","No, you are not allowed to modify any of the provided parameters in the transformer and Model_input files.","test images which are not in the training dataset. First, let‚Äôs create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","test images which are not in the training dataset. First, let‚Äôs create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):‚Ä¶def __len__(self):‚Ä¶In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let‚Äôs add a parameter to self.__init__() to indicate which part of the dataset. It has two values: ‚Äútrain‚Äù or ‚Äútest‚Äù depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let‚Äôs set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the peer assessment form for?,"It‚Äôs an individual submission where you evaluate contributions within your group for the coursework. You must submit it via the ‚ÄúPeer Assessment‚Äù link on Moodle; if you don‚Äôt submit it, you‚Äôll be penalised 10% of your overall group mark (Assessment Sheet COMP2032 Coursework 2025.pdf).

The assessment sheet doesn‚Äôt state the exact criteria or how the peer input affects individual marks beyond the penalty for non-submission. For specifics, please check the Moodle instructions or contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71, Block B, Malaysia Campus
- Mr. Irfan Yaqub (TA): hcxiy1@nottingham.edu.my

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",Each group member must individually submit a peer assessment form. Failure to do so will result in a 10% penalty on the overall group mark.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
What are the assessment criteria for the coursework?,"Here‚Äôs the coursework breakdown and marking criteria from the assessment sheet:

Deliverables
- Python application (implementation of your pipeline)
- Conference-style paper (~2000 words) including literature review, explanation of methods, and evaluation of results

Marking criteria and weights
- Programming component: 30%
  - Your code must run; if it cannot be executed, this 30% component will receive 0%.
- Conference paper: 70%
  - Justification of chosen algorithms based on your literature review: 15%
  - Critical analysis and discussion of your findings: 25%
  - Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), leading to a conclusion on the classification result: 30%

If you need the official rubric or have any doubts about the expectations for each section, please contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71 Block B, Malaysia Campus
- Teaching Assistant: Mr Irfan Yaqub, hcxiy1@nottingham.edu.my

Source: Assessment Sheet COMP2032 Coursework 2025.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

","The Python code is worth 30%, and the conference paper is worth 70%. The paper is broken down into justification of algorithms (15%), critical analysis of findings (25%), and discussion of strengths and weaknesses (30%).","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¬º f √æ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k √æ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the format for the in-lab test?,"I don‚Äôt have the detailed format of the in-lab test in the materials I can see. The intro slides mention there will be an ‚Äúin-lab test‚Äù as part of assessment, but they don‚Äôt specify things like duration, number of questions, resources allowed, or grading scheme. The coursework assessment sheet focuses on the group project/paper and doesn‚Äôt describe the in-lab test format.

For the exact format, please check the module‚Äôs Moodle page or contact:
- Dr. Tissa Chandesa (Module Convenor) ‚Äî Tissa.Chandesa@nottingham.edu.my, Office: Room BB71, Block B, Malaysia Campus
- Mr. Irfan Yaqub (TA) ‚Äî hcxiy1@nottingham.edu.my

If you can share any announcement text you‚Äôve seen, I can help interpret it and suggest how to prepare. 

References:
- Lec-1A-Introduction slides (assessment overview)
- Assessment Sheet COMP2032 Coursework 2025 (coursework deliverables)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",The in-lab test is an individual assessment worth 50% of the final grade. It focuses on theories covered in the lectures and practical exercises.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f¬∞ University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format‚Ä¢ Accepted formats: .jpg, .png‚Ä¢ Ensure both segmented images and ground truth masks use the same format. ‚Ä¢ Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256√ó256 pixels (or will be resized automatically).‚Ä¢ If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ ‚îÇ‚îÄ‚îÄ /segmented/ # Student's segmented outputs ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png ‚îÇ‚îÄ‚îÄ /ground_truth/ # Provided ground truth masks ‚îÇ ‚îú‚îÄ‚îÄ image_001.png ‚îÇ ‚îú‚îÄ‚îÄ image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
When is the in-lab test?,"The in-lab test is on 29 April 2025, 2:30 pm‚Äì3:30 pm, in TCR 1. It‚Äôs an individual test focusing on theories from lectures and practicals.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The in-lab test is scheduled for 29 April 2025, from 2:30 pm to 3:30 pm in TCR 1.

QA about Out-of-the-Domain Questions","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, ‚Ç¨ may run away at the earliest 150",
What is the capital of France?,"I‚Äôm here to help with topics related to the Introduction to Image Processing module. If you have a question on image processing (e.g., histogram intersection, morphology, watershed segmentation), I‚Äôd be happy to help.

For general knowledge questions like country capitals, I‚Äôm not able to assist here. If you have an image-processing question, please go ahead!","I'm sorry, I can only answer questions related to image processing.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t ‚Äú enough representatt ‚Äòaction process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane¬ª Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t ‚Äú enough representatt ‚Äòaction process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane¬ª Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you explain the theory of relativity?,"I‚Äôm here to help with topics from Introduction to Image Processing. The theory of relativity is outside this module‚Äôs scope, so I can‚Äôt provide a proper explanation based on our course materials.

If you meant a related image-processing concept (e.g., edge detection with first/second derivatives, Canny vs. Sobel/Roberts, histogram backprojection/intersection, or watershed segmentation), let me know and I‚Äôll help.

For course-related guidance, you can contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71 Block B, Malaysia Campus
- Teaching Assistant: Mr. Irfan Yaqub, hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What are the main causes of climate change?,"Thanks for your question. I‚Äôm here to help with topics related to the Introduction to Image Processing module, so I can‚Äôt provide a detailed answer on the causes of climate change.

If you‚Äôre interested in how image processing is used to study climate change (e.g., satellite image analysis for glacier retreat, land cover change detection, sea-ice segmentation, NDVI vegetation monitoring, or CNN-based semantic segmentation of remote sensing images), I‚Äôd be happy to help.

For course-related guidance, you can contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office: Room BB71, Block B, Malaysia Campus
- Teaching Assistant: Mr Irfan Yaqub, hcxiy1@nottingham.edu.my

Let me know if you‚Äôd like image-processing methods relevant to climate studies.","I'm sorry, I can only answer questions related to image processing.","connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
"Who wrote the play ""Hamlet""?","I‚Äôm here to help with topics in the Introduction to Image Processing module. I can‚Äôt answer general literature questions like this. 

If you have an image processing question, feel free to ask. For module-related queries, you can contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my
- Mr. Irfan Yaqub (TA): hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )√º if q‚Äôs cost changed, make q point back to p√ò Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (l√©*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ‚Äù 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts ‚Äî__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )√º if q‚Äôs cost changed, make q point back to p√ò Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (l√©*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ‚Äù 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts ‚Äî__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!",
What is the chemical formula for water?,"Sorry, I can only assist with topics related to the Introduction to Image Processing module. If your question is about water in an image-processing context (e.g., detecting water bodies in images), please clarify and I‚Äôll be happy to help.

For other questions, please contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71, Block B, Malaysia Campus
- TA: Mr. Irfan Yaqub: hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are",
Can you recommend a good restaurant in Nottingham?,"I‚Äôm here to help with Image Processing topics for the Introduction to Image Processing module. I can‚Äôt give recommendations on restaurants. If you have questions on segmentation, filtering, CNN basics, or anything from the course materials, feel free to ask! If it‚Äôs about course logistics, you can contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my
- TA: Mr Irfan Yaqub: hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of‚Ä¶A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and ‚Äî1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 ‚Äî 2k + 2k?)(a(x) + 7) + 2(1 ‚Äî 2k) B(x)2a(x)_+ki ‚Äîkh)MMki ‚Äîkh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 ¬£82)(a(e) +7) +211 ‚Äî 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L‚Äîb*)6(@) , 2ba(x) by bM bM bM 6M (1+ ‚Äô)(¬´?(D ‚Äî W)a +17(D ‚Äî W)1) nTD1 b (1 ‚Äî B)17(D ‚Äî W)z 7 b1/D1 2b27(D‚ÄîW)z 2b17(D‚Äî W)1 ‚Äúopi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1‚Äî z)'(D‚ÄîW)(1+2) b17D1 T (+2) - 60 ‚Äî2)] @- W)[(1 +2) ‚Äî 41-2) ; b1¬∞D1Let D be an N x N diagonal matrix with d","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of‚Ä¶A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and ‚Äî1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 ‚Äî 2k + 2k?)(a(x) + 7) + 2(1 ‚Äî 2k) B(x)2a(x)_+ki ‚Äîkh)MMki ‚Äîkh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 ¬£82)(a(e) +7) +211 ‚Äî 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L‚Äîb*)6(@) , 2ba(x) by bM bM bM 6M (1+ ‚Äô)(¬´?(D ‚Äî W)a +17(D ‚Äî W)1) nTD1 b (1 ‚Äî B)17(D ‚Äî W)z 7 b1/D1 2b27(D‚ÄîW)z 2b17(D‚Äî W)1 ‚Äúopi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1‚Äî z)'(D‚ÄîW)(1+2) b17D1 T (+2) - 60 ‚Äî2)] @- W)[(1 +2) ‚Äî 41-2) ; b1¬∞D1Let D be an N x N diagonal matrix with d",
What is the current weather forecast?,"I‚Äôm here to help with topics related to the Introduction to Image Processing module. I can‚Äôt provide weather forecasts. If you have questions on image processing concepts, assignments, or techniques (e.g., filtering, histograms, CNNs), feel free to ask!","I'm sorry, I can only answer questions related to image processing.","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network‚Äôs output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamar√≠a, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the meaning of life?,"I‚Äôm here specifically to help with Image Processing topics for the Introduction to Image Processing module. I can‚Äôt answer philosophical questions like the meaning of life.

If you‚Äôd like, ask me about:
- Histogram intersection and backprojection
- Superpixels (e.g., SLIC) and CRF/graph-cut segmentation
- Min-cut/max-flow algorithms for energy minimization
- Thresholding (global, adaptive/mean/Gaussian)
- Evaluation metrics (e.g., VOC/IoU)

If you meant something image-related by ‚Äúmeaning of life‚Äù (e.g., semantic scene understanding), please clarify and I‚Äôll help.","I'm sorry, I can only answer questions related to image processing.","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term œà assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiÔ¨Åer trained using the SIFT descriptors. The pairwise term œÜ assigns to each pair of superpixels a potential to have similar or diÔ¨Äering labels based on the diÔ¨Äerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¬¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxÔ¨Çow algorithm to produce a Ô¨Ånal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048√ó1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term œà assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiÔ¨Åer trained using the SIFT descriptors. The pairwise term œÜ assigns to each pair of superpixels a potential to have similar or diÔ¨Äering labels based on the diÔ¨Äerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¬¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxÔ¨Çow algorithm to produce a Ô¨Ånal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048√ó1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How does a car engine work?,"I‚Äôm here to help with topics related to the Introduction to Image Processing module. I can‚Äôt provide support on car engines/mechanical engineering.

If you meant an ‚Äúengine‚Äù in the context of image processing (e.g., segmentation pipelines, CNN inference engines, watershed/normalized cuts, morphology, connected components), let me know and I‚Äôll be happy to explain.

For course-related image processing queries, you can also contact:
- Dr. Tissa Chandesa (module convenor): Tissa.Chandesa@nottingham.edu.my
- TA: Mr. Irfan Yaqub: hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will ‚Äúpop‚Äù off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to ‚Äúpop‚Äù off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‚Äò domain. . ated goals. A feature space‚Äô ‚Äòd through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‚Äòaction process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input",
What are the rules of chess?,"I‚Äôm here to help with Image Processing topics for the Introduction to Image Processing module. The rules of chess are outside this scope.

If your question relates to image processing and chess (e.g., detecting a chessboard or recognizing pieces in an image/video), I can help. For example, we could:
- Detect the chessboard using corner detection or a checkerboard pattern finder.
- Segment pieces using color models (e.g., histogram backprojection) and refine with morphology.
- Classify pieces with features or template matching, potentially using histogram intersection for color-based matching.

If you meant something else within image processing, please clarify. For course-related queries, you can contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71, Block B, UNM Malaysia Campus
- TA: Mr. Irfan Yaqub: hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you tell me a joke?,"Sure! Here‚Äôs an image processing joke:

Why did the image go to therapy?
Because it had too many unresolved edges and couldn‚Äôt commit to a stable threshold.

Want another one?","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) ‚Äúatta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D ‚ÄîW)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 ‚Äúas 0.176; 4a Wa=dAx ‚Äú Aah -0.2) ‚Äúa as, ‚Äòaa eae VOT 7&9 20~SCOOSC¬´O ‚Äú44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(¬´) = 1 ‚Äî d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!",
What is the history of the internet?,"I‚Äôm here to help with Image Processing topics for the Introduction to Image Processing module. The history of the internet is outside my scope.

If you have a question on image processing‚Äîe.g., histogram backprojection, histogram intersection matching, thresholding, binary images, or segmentation‚Äîfeel free to ask. For non-course topics, you might consult general internet history resources. If your question relates to this module‚Äôs content or assignments, you can also contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my
- Mr. Irfan Yaqub (TA): hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.","tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding ‚Ç¨ Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ¬∞ ‚Äòe : ie ciewk pg toate om ‚Äòous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair ¬ª ¬´vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the¬Æ parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding ‚Ç¨ Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ¬∞ ‚Äòe : ie ciewk pg toate om ‚Äòous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‚Äòaction process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¬¢ a time. For each subset,‚Äù the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
How do I learn to play the guitar?,"I‚Äôm sorry, but I can only assist with topics related to the Introduction to Image Processing module. If you have questions on things like histogram equalisation, CNNs for image classification, or interactive segmentation (e.g., GrabCut), I‚Äôm happy to help.

If you need guidance within the module:
- Module convenor: Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), Office: Room BB71, Block B, Malaysia Campus
- Teaching Assistant: Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my)

Would you like help with a specific image processing topic?","I'm sorry, I can only answer questions related to image processing.","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let‚Äôs create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let‚Äôs initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f¬∞ University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:‚óè In the first interaction, the user can only draw a rectangle on the image.‚óè The user is encouraged to press Key 1 to start drawing the rectangle.‚óè The user can start segmentation after drawing rectangles or curves by pressing ‚ÄúEnter‚Äù.‚óè After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.‚óè After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.‚óè The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let‚Äôs rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let‚Äôs create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let‚Äôs initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f¬∞ University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:‚óè In the first interaction, the user can only draw a rectangle on the image.‚óè The user is encouraged to press Key 1 to start drawing the rectangle.‚óè The user can start segmentation after drawing rectangles or curves by pressing ‚ÄúEnter‚Äù.‚óè After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.‚óè After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.‚óè The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5",
What are the best programming languages to learn in 2025?,"I can only advise within the scope of image processing and this module (COMP2032). If you meant ‚Äúbest languages overall,‚Äù I‚Äôm not able to comment broadly. For image processing in 2025, here‚Äôs what I recommend:

- Python ‚Äî Top priority. It‚Äôs the language used for the COMP2032 coursework and conference-paper project, and it dominates modern image processing/computer vision.
  - Why: Huge ecosystem (NumPy, OpenCV, scikit-image, Pillow), deep learning frameworks (PyTorch, TensorFlow), fast prototyping, lots of tutorials/datasets.
  - Course note: The assessment specifically requires a Python application. 
- C++ ‚Äî For performance and real-time systems.
  - Why: OpenCV‚Äôs fastest components, good for embedded/edge and production deployment; can interoperate with Python.
- MATLAB ‚Äî Excellent for learning and quick algorithm prototyping.
  - Why: Image Processing Toolbox is very strong; widely used in academia. Caveat: licensing and less common in production.
- CUDA (with C++ or via PyTorch/TensorFlow) ‚Äî For GPU acceleration.
  - Why: Needed when speeding up heavy image processing or deep learning pipelines.
- Nice-to-have extras (depending on goals):
  - JavaScript/WebAssembly/WebGPU for in-browser image tools/visualization.
  - Julia for high-performance numerical work (smaller ecosystem than Python/C++ but growing).

If you‚Äôre taking COMP2032, start with Python and OpenCV/scikit-image, then add PyTorch for learning-based methods. Consider C++ when you need speed or plan to deploy.

If you need module-specific guidance on tooling, you can also contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my
- Teaching Assistant: Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","I'm sorry, I can only answer questions related to image processing.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
Can you help me with my math homework?,"I‚Äôm here to help with topics related to the Introduction to Image Processing module. I can‚Äôt assist with general math homework, but I can help with math as it applies to image processing, such as:
- Convolution/correlation and linear filters
- Fourier transforms and frequency-domain filtering
- Thresholding (e.g., Otsu), histograms, and probability
- Edge detection (Sobel/Canny), gradients
- Morphology (dilation/erosion)
- Clustering/segmentation (k-means, normalized cuts, watershed)
- Evaluation metrics (IoU, precision/recall)

If your math question is related to any of these, please share the exact problem and I‚Äôll help step by step.

For module-related queries, you can also contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office: Room BB71 Block B, Malaysia Campus
- Mr. Irfan Yaqub (TA): hcxiy1@nottingham.edu.my","I'm sorry, I can only answer questions related to image processing.

QA about Course-Related Questions","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > ‚ÄúS2¬∞ = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n‚Äî 4am ‚Äî 2x 2am 2am Goda c+d,‚Äô2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >‚Äî5 ea} dame 2 ‚Äî(B-2am)‚Äô where B= 2M(n+1)‚Äî dac ‚Äî 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C ‚Äî B‚Äô) + 4a B‚Äî 4a? ~ 2 1 (2 ‚Äî B?) + 4a B- 4a? ((2 ‚Äî B‚Äô) + 4a B‚Äî4a?)n < (4c k ‚Äî 4k?)n + 4a n(e ‚Äî 2k ‚Äî a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k ‚Äî 4k?)n + 4(c ‚Äî 2k) ‚Äî4da+1<@ or if dc k? + 4c ‚Äî (4k? + 8k + da ‚Äî 1) <since n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we onlysince n < k. Since 4k¬Æ + 8k + 4a ‚Äî1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative ‚Äì for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline‚Äôs performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 ‚Äî (a n(2k, ‚Äî1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula‚Äîthe cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the ¬´; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
Who is the module convenor for COMP2032?,"Dr. Tissa Chandesa.

Email: Tissa.Chandesa@nottingham.edu.my
Office: Room BB71, Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",The module convenor for COMPT2032 is Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my).,"MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
Where can I find the lecture slides?,"I‚Äôm not sure where your cohort‚Äôs slides are hosted from here. Typically, they‚Äôre posted on the module‚Äôs VLE (e.g., Moodle) under COMP2032/Introduction to Image Processing in a section like ‚ÄúLecture Notes‚Äù or ‚ÄúSlides.‚Äù

If you can‚Äôt find them there, please contact:
- Module convenor: Dr. Tissa Chandesa ‚Äî Tissa.Chandesa@nottingham.edu.my
  Office: Room BB71, Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor
- Teaching Assistant: Mr Irfan Yaqub ‚Äî hcxiy1@nottingham.edu.my

If you‚Äôd like, tell me which week/topic you need and I can check whether I have that specific slide deck title to help you reference it when you email them.


You can use the following button to email the convenor:

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Unable to find lecture slides for Introduction to Image Processing (COMP2032)&body=Dear Dr. Chandesa,

I hope you are well. I‚Äôm trying to access the lecture slides for Introduction to Image Processing (COMP2032) but I can‚Äôt find them on Moodle. Could you please let me know where they are posted, or check if there might be a permissions issue on my account?

Details:
‚Ä¢ Specific lecture(s) I‚Äôm looking for: [e.g., Week X ‚Äì Topic]
‚Ä¢ My name and student ID: [Your Name], [Your ID]
‚Ä¢ Programme/Year: [e.g., BSc Computer Science, Year 2]

If appropriate, I‚Äôm happy to CC Mr Irfan Yaqub (hcxiy1@nottingham.edu.my).

Many thanks for your help.

Kind regards,
[Your Name]"">Email Dr Chandesa</a>",You can find the lecture slides on Moodle‚Äôs course page.,"slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16‚óè The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17‚óè Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:‚Ä¢ No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0‚Ä¢ Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1‚Ä¢ Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5‚Ä¢ Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1‚Ä¢ 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.‚Ä¢ Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16‚óè The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17‚óè Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‚Äò1. The Whiteboard Problem ‚Äî revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!",
What are the office hours for the teaching assistants?,"I don‚Äôt have the TA office hours in the materials I can access. For the most up-to-date info, please:

- Email the Teaching Assistant: Mr Irfan Yaqub (hcxiy1@nottingham.edu.my)
- If needed, contact the module convenor: Dr Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), Office: Room BB71, Block B, Malaysia Campus

You can also check the module‚Äôs Moodle page or recent announcements‚Äîoffice hours are often posted there. If you‚Äôd like, I can help draft a short email request.

It isn‚Äôt specified in the materials I have. You can contact the TA directly to confirm office hours.

You can use the following button to send an email:

 <a class=""email-btn"" href=""mailto:hcxiy1@nottingham.edu.my?subject=Request for TA office hours for Introduction to Image Processing (COMP2032)&body=Dear Mr Yaqub,

I hope you are well. I am a student in Introduction to Image Processing (COMP2032) and would like to confirm the teaching assistants‚Äô office hours.

Could you please let me know:
- Your regular weekly office hours and location (or online link)
- Whether appointments are required and how to book
- Any changes for this week‚Äôs availability

Many thanks,
[Your full name]
[Student ID]
[Group number, if applicable]"">Email Mr Irfan Yaqub</a>","I'm sorry, I don‚Äôt have access to the office hours for the teaching assistants.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: ‚ÄúIf, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: ‚Äú[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¬¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: ‚Äú[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE¬Æ O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How do I submit my coursework?,"Here‚Äôs what to submit, where, and by when for COMP2032 (Introduction to Image Processing):

When and where
- Deadline: Friday, 25 April 2025, by 11:59pm
- Submit via Moodle:
  - Group submission area: upload the group deliverables
  - Individual submission: complete the ‚ÄúPeer Assessment‚Äù link on Moodle
- Late policy: 5 marks (out of 100) deducted per late working day
- Feedback: marks and written feedback will be returned via Moodle in the week commencing 2 June 2025
[Source: Assessment Sheet COMP2032 Coursework 2025]

Group submission (one per group)
- Python code: all .py files, zipped into a single ZIP
- Conference paper: PDF, max 6 pages and 2000 words
  - Use the provided template ‚ÄúCOMP2032-CW-GroupXXX.docx‚Äù (replace XXX with your group number)
  - Do not change formatting/headers; replace all placeholder text
  - Exceeding 6 pages: 5% deduction for every additional page for the entire group
- Result Images: submit as a ZIP folder (include any required Output and Image Processing Pipeline folders as specified)
[Source: Assessment details in the retrieved context and template notes]

Individual submission (each student)
- Complete and submit the Peer Assessment form via the ‚ÄúPeer Assessment‚Äù link on Moodle
  - Not submitting the peer assessment incurs a 10% penalty on your overall group mark
[Source: Assessment Sheet COMP2032 Coursework 2025]

Quick checklist before uploading
- Code runs and is packaged in a single ZIP (include any needed data/output folders as requested)
- Paper exported to PDF, 6 pages max, 2000 words max, correct template and formatting
- Result images zipped separately (if specified for your group)
- Submit to the correct Moodle links (group area + individual peer assessment)
- Keep the Moodle submission receipts

If anything is unclear (e.g., exact Moodle links or folder structure for outputs), please contact:
- Module Convenor: Dr. Tissa Chandesa, Tissa.Chandesa@nottingham.edu.my, Office BB71 Block B (Malaysia Campus)
- Teaching Assistant: Mr. Irfan Yaqub, hcxiy1@nottingham.edu.my

References:
- Assessment Sheet COMP2032 Coursework 2025
- Provided template and deliverable notes in the course materials on Moodle

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",You submit your coursework through the Moodle assignment page.,"DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¬∂  ¬∂  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both ‚Äì before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x ‚Äì 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI‚Äô‚Äô(1) = (I‚Äô(1.5) - I‚Äô(0.5))/1  I‚Äô(0.5) = (I(1) - I(0))/1   and   I‚Äô(1.5) = (I(2) - I(1))/1  \  I‚Äô‚Äô(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the Ô¨Ålter order.Equivalencing of spatial frequencies is performed by Ô¨Årst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares Ô¨Åt can be used to create a normalized Butterworth Ô¨Ålter B(u, v, d0', n') (whered0' and n' are the Ô¨Åt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by Ô¨Åtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¬¢ x y,( ) = F 1- [ ( , , B u v d 0 ¬¢ n¬¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¬∂  ¬∂  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both ‚Äì before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x ‚Äì 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI‚Äô‚Äô(1) = (I‚Äô(1.5) - I‚Äô(0.5))/1  I‚Äô(0.5) = (I(1) - I(0))/1   and   I‚Äô(1.5) = (I(2) - I(1))/1  \  I‚Äô‚Äô(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)‚àí Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the ‚ÄúPeer Assessment‚Äù link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass Ô¨Ålter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the Ô¨Ålter order.Equivalencing of spatial frequencies is performed by Ô¨Årst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares Ô¨Åt can be used to create a normalized Butterworth Ô¨Ålter B(u, v, d0', n') (whered0' and n' are the Ô¨Åt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by Ô¨Åtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¬¢ x y,( ) = F 1- [ ( , , B u v d 0 ¬¢ n¬¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z ‚Äî Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+‚Ñ¢M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k ‚Äî YL, min‚Äô, Mi) 1‚Äî HU, M) = kand so1 ‚Äî HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=‚Äî>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in ¬£ are n-tuples(C1, C2, ---, ‚Ç¨,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What is the assessment criteria for the coursework?,"Here‚Äôs what the coursework will be marked on, based on the assessment sheet:

Deliverables
- Python code: submit .py files as a ZIP
- Conference paper: 6 pages, max 2000 words, PDF (additional pages will incur a deduction)
- Release: Fri 7 Mar 2025
- Submission: Fri 25 Apr 2025 by 11:59pm
- Late policy: 5 marks deducted per late working day
- Feedback: w/c 2 June 2025 via Moodle

Marking criteria and weighting
- Programming (30%)
  - Your Python application must run; if the code cannot be executed, 0% is awarded for this component.
- Conference paper (70%)
  - Justification of chosen algorithms based on a literature review: 15%
  - Critical analysis and discussion of your findings: 25%
  - Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), leading to a conclusion on the classification result: 30%

If you need the full, detailed rubric beyond the above, please contact:
- Module convenor: Dr. Tissa Chandesa ‚Äî Tissa.Chandesa@nottingham.edu.my, Office BB71, Block B (Malaysia Campus)
- Teaching assistant: Mr. Irfan Yaqub ‚Äî hcxiy1@nottingham.edu.my

Source: Assessment Sheet COMP2032 Coursework 2025 (School of Computer Science, Introduction to Image Processing / COMP2032).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

","The assessment criteria is as follows:
+Python code(s): 30% [unable to run codes will result in 0% being awarded]
+Conference Paper:
‚àí Justification of chosen algorithms based on performed literature review: 15%
‚àí Critical analysis and discussion of your findings: 25%
‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%.
+In-Lab Test Individual (50%) - Focuses on theories covered in the lectures and practical exercises","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 ‚Äì Assessment (100% CW)III,II|‚Ä¢ Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  ‚Ä¢ In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  ‚Ä¢ Image Processing  ‚Ä¢ Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction",
Can I get an extension for the coursework?,"I don‚Äôt have the authority to grant extensions. From the assessment sheet, the coursework is due Friday, 25 April 2025 by 11:59 pm, with a late penalty of 5 marks per working day. The document doesn‚Äôt specify an extension process.

Please contact the module convenor to request an extension:
- Dr. Tissa Chandesa ‚Äî Email: Tissa.Chandesa@nottingham.edu.my
  Office: Room BB71, Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor
- Teaching Assistant: Mr. Irfan Yaqub ‚Äî Email: hcxiy1@nottingham.edu.my

If you‚Äôd like, I can help you draft a concise email request‚Äîjust tell me your group number, the reason, and how long an extension you‚Äôre seeking.

Source: Assessment Sheet COMP2032 Coursework 2025 (due date and late policy).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","I‚Äôm not able to help you with an extension. You should contact the module convenor (Dr. Tissa Chandesa) and request a coursework extension, providing a reason for it.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n ¬ª |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n ‚Äî 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 ‚Äî 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2‚Äù quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is‚Äîusing equation (4)Proof. The points in E for whichn Sq=1 i=0form ann ‚Äî 1 dimensional subset of E, which we will call P. We can find the n ‚Äî 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)‚Äù (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‚ÄòV,,/Vp of the volume|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n ¬ª |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n ‚Äî 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 ‚Äî 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2‚Äù quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is‚Äîusing equation (4)Proof. The points in E for whichn Sq=1 i=0form ann ‚Äî 1 dimensional subset of E, which we will call P. We can find the n ‚Äî 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)‚Äù (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‚ÄòV,,/Vp of the volume|tion,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-‚Ñ¢, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the room number for the lectures?,"I‚Äôm not sure‚Äîthis information isn‚Äôt in the materials I have. Please check the module‚Äôs Moodle page or the university timetable.

You can also contact:
- Module convenor: Dr. Tissa Chandesa, Tissa.Chandesa@nottingham.edu.my, Office: Room BB71, Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor
- Teaching Assistant: Mr. Irfan Yaqub, hcxiy1@nottingham.edu.my

They can confirm the lecture room and any updates.

You can contact the module convenor to confirm the lecture room: <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Lecture room for Introduction to Image Processing&body=Dear Dr. Chandesa,

I am a student in Introduction to Image Processing and would like to confirm the room number and building for our lectures this semester. Could you please let me know the lecture venue (and day/time if applicable)?

Name:
Student ID:
Programme/Group (if relevant):

Thank you very much.

Best regards,
[Your Name]"">Email Dr. Chandesa</a>","The lectures are in F3A04 (Block F3, Level A, Room 04) on Wednesdays (09:00 to 11:00). Labs are in BB80 (Block B, Level B, Room 80) on Tuesdays (14:00 to 16:00)","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
How many people are in the group for the coursework?,"I‚Äôm not seeing the group size specified in the retrieved assessment snippets I have. So I can‚Äôt confirm how many students per group from the current context.

Please check the ‚ÄúAssessment Sheet COMP2032 Coursework 2025‚Äù on Moodle or your module handbook. If it‚Äôs not there, you can confirm with:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71 Block B (Malaysia Campus)
- Teaching Assistant: Mr. Irfan Yaqub, hcxiy1@nottingham.edu.my

If you can share the specific coursework brief or a page from the assessment sheet, I can look for the group size in it for you.


You can use the following button to email the module convenor:

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=COMP2032 Introduction to Image Processing ‚Äì Coursework group size clarification&body=Dear Dr Chandesa,

I hope you are well. Could you please confirm the required group size for the Introduction to Image Processing coursework (COMP2032)?

If possible, could you also advise on:
- Whether groups must be from the same lab section
- The deadline for forming/registering groups
- The procedure or link for registering our group

I couldn‚Äôt find a definitive note in the assessment sheet/Moodle page. Thank you very much for your guidance.

Kind regards,
[Your Name]
[Student ID]
[Programme/Year]
"">Email Dr Tissa Chandesa</a>",You need to form a group of four people for the coursework.,"30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%‚àí Critical analysis and discussion of your findings: 25%‚àí Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat‚Äôs ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‚Äò‚Äî the pixel has the same color as one of the colors in the model, and‚Äî the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What programming language is used in this module?,"Python. The labs use Python with OpenCV (cv2) and Matplotlib (pyplot). 
Sources: Lab 1A (Image Processing in Python) and Lab 2 (Histograms).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 1A.pdf"" target=""_blank"">Lab 1A</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",This module provides practical experience of image processing using Python.,"import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.‚óè Aliasing using as. This modifies the name of the module during referencing. It is done when the module‚Äôs name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:‚óè Read it in using imread.‚óè Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).‚óè Write it to a new file using imwrite.shape can provide basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).‚óè Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).‚óè Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.‚óè Using import. The entire module is imported, and the module must be referenced when used.‚óè Using from‚Ä¶ import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The","import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.‚óè Aliasing using as. This modifies the name of the module during referencing. It is done when the module‚Äôs name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:‚óè Read it in using imread.‚óè Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).‚óè Write it to a new file using imwrite.shape can provide basic information about the image.‚óè Use shape on the image and print the results. How many values do you get, and what|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).‚óè Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).‚óè Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?‚óè Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if‚Äô University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.‚óè Using import. The entire module is imported, and the module must be referenced when used.‚óè Using from‚Ä¶ import. The specified functions are imported, and the module does not need to be referenced.‚óè Using from‚Ä¶ import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‚Äò‚Äòwhat‚Äù or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The",
Where can I find the datasets for the coursework?,"Short answer: check the module‚Äôs Moodle page. That‚Äôs where the datasets used in the coursework/labs are posted.

Details from the materials:
- Thresholding task: the Tulip.jpeg image is provided on the module‚Äôs Moodle page [Lab 4.pdf].
- CNN/MNIST task: the code expects a local MNIST folder with:
  - ./MNIST/mnist_train/
  - ./MNIST/mnist_test/
  Look for a downloadable MNIST archive on Moodle, or prepare this structure yourself if instructed [Laboratory Sheet 9 - CNN.pdf].

If you can‚Äôt find the files, please let me know which specific coursework/lab you mean, and I can point you more precisely. Otherwise, you can contact:
- Dr. Tissa Chandesa (Module Convenor): Tissa.Chandesa@nottingham.edu.my, Office BB71, Block B, Malaysia Campus
- TA: Mr Irfan Yaqub: hcxiy1@nottingham.edu.my

Sources:
- Lab 4 (thresholding): ./documents/Lab 4.pdf
- CNN/MNIST: ./documents/Laboratory Sheet 9 - CNN.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Laboratory Sheet 9 - CNN.pdf"" target=""_blank"">Laboratory Sheet 9   CNN</a>

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

",You can download the datasets from Moodle‚Äôs coursework page.,"3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder‚Äôs name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image‚Äôs path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","3 of 16f¬∞ University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder‚Äôs name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image‚Äôs path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let‚Äôs create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.‚óè Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.‚óè Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?‚óè Compare the results of the four methods. Which method produced the best result?‚óè Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: ‚ÄúIf, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
