input,actual_output,expected_output,retrieval_context,context,source_file
What is Thresholding?,"Thresholding is a technique used in image processing to classify pixels in an image into either foreground or background based on their intensity values. It involves setting a threshold value and comparing each pixel's value to it. If the pixel value is above or below the threshold, it is assigned a specific value, typically 0 (black) or 255 (white), resulting in a binary image.

In simple terms, thresholding helps to separate objects of interest from the rest of the image by creating a binary image where pixels are either part of the object (foreground) or not (background).

For example, consider a dark object on a light background in a grey-level image. By choosing a suitable threshold value, T, pixels with brightness less than T are considered object pixels, while those with brightness greater than or equal to T are considered background pixels.

Thresholding is a fundamental step in many image processing applications, including edge detection and segmentation.

Source: Lec-4-Thresholding and Binary Images.pdf, Lab 4.pdf

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-4-Thresholding and Binary Images.pdf"" target=""_blank"">Lec 4 Thresholding and Binary Images</a>

",Thresholding is a method of image segmentation that creates binary images. It works by setting a threshold value (T) and classifying pixels as either object or background based on whether their brightness is above or below this threshold.,"University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.",
What is the difference between Otsu and Rosin thresholding methods?,"Otsu and Rosin are two different thresholding methods used in image processing.

Otsu's thresholding method is a widely used technique that works well when the histogram of the image is bimodal, i.e., it has two distinct peaks. The method calculates the optimal threshold value by maximizing the between-class variance of the two classes (foreground and background) [./documents/Lec-4-Thresholding and Binary Images.pdf].

On the other hand, Rosin's unimodal thresholding method is used when the histogram is not bimodal, but rather unimodal, i.e., it has a single peak. This method is particularly useful when the object of interest is predominantly one color (e.g., text is mainly white with a small amount of black). Rosin's method finds the peak, draws a line from there to the top of the furthest bin, and then finds the top of the bin that is furthest from this line; that bin value is the threshold [./documents/Lec-4-Thresholding and Binary Images.pdf].

The key differences between Otsu and Rosin thresholding methods are:

1. **Histogram assumption**: Otsu assumes a bimodal histogram, while Rosin assumes a unimodal histogram.
2. **Threshold calculation**: Otsu calculates the threshold by maximizing between-class variance, whereas Rosin uses a geometric approach to find the threshold.

When to use each method? Use Otsu when you have a bimodal histogram, and use Rosin when you have a unimodal histogram.

For more information on thresholding methods, you can refer to the lecture notes on Thresholding and Binary Images [./documents/Lec-4-Thresholding and Binary Images.pdf].","Otsu's method is used for bimodal histograms, assuming two peaks that can be separated by a single threshold. Rosin's method is for unimodal histograms, where there's only one peak, and it finds the threshold by drawing a line from the peak to the furthest bin.","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f’ =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¢ .° , ,* ,• Finds the peak• Draws a line from there to the top of thefurthest bin• Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin’s unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f’ =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods• Imaging conditions and object properties can vary within a single image as well as across sets of images'e '• Histograms can be too complex for any method’s|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the","threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|UNUKThresholding & Binary Images 50 100 150 200 250 300COMP 203211f’ =Unimodal ThresholdingIMany histograms are not bimodal, there is often only one peak e.g.,,text is mainly white, with a small amount of blackI \¢ .° , ,* ,• Finds the peak• Draws a line from there to the top of thefurthest bin• Finds the top of the bin that is furthestfrom this line; that bin value is the_thresholdRosin’s unimodal methodBin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032|Thresholding & Binary Images12Histogram of edge  Unimodal ThresholdingCan be applied to any suitable image, e.g., intensity gradientsEdge of magnitudes  (inverted for display)  COMP 2032  Thresholding & Binary Images Edge of magnitudes (inverted for display)Histogram of edgemagnitudesACK: Prof. Tony Pridmore, UNUK13Unimodal Thresholding  Rosin f’ =ACK: Prof. Tony Pridmore, UNUKThresholding & Binary ImagesCOMP 203214Otsu vs RosinFrequency Bin ValueACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesFrequency Threshold ; Bin Value15irLocal Adaptive Methods• Imaging conditions and object properties can vary within a single image as well as across sets of images'e '• Histograms can be too complex for any method’s|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a structuring element in morphological operations?,"A structuring element is a binary mask used in morphological operations, similar to a filter mask in convolution. It is used to process a binary image by identifying pixels to be considered in the operation, rather than weighting them like in convolution. The shape and size of the structuring element can vary, and it is used to expand or shrink the foreground in the image.

According to the lecture notes, a structuring element is defined as ""a binary mask, c.f. filter masks but identifying rather than weighting pixels"" (Source: './documents/Lec-5-Morphology.pdf'). The size and shape of the structuring element determine the effect of the morphological operation. Larger structuring elements produce more extreme effects, and very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element.

You can create structuring elements using the `getStructuringElement` function, which allows you to specify the shape (e.g., elliptic, rectangular, cross-shaped) and size of the element (Source: './documents/Lab 5.pdf'). For example, you can create an elliptic structuring element of sizes 3, 5, and 7, or a rectangular structuring element using NumPy's `ones` function.",A structuring element is a small matrix or kernel used to probe an image in morphological operations like erosion and dilation. It defines the neighbourhood of a pixel and is used to modify the pixel's value based on the values of its neighbours.,"if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt",
Explain the process of Histogram Backprojection.,"Histogram Backprojection is a technique used to locate an object within an image based on its color histogram. The process involves several steps:

1. **Model Histogram Creation**: A color histogram of the target object (model) is created by discretizing the image colors and counting the number of times each discrete color occurs.

2. **Image Histogram Creation**: Similarly, a color histogram of the input image is created.

3. **Ratio Histogram Calculation**: The ratio histogram is calculated by dividing the model histogram by the image histogram. This step highlights the colors that are more frequent in the model than in the image.

4. **Backprojection**: The ratio histogram is then backprojected onto the image. This involves replacing each pixel in the image with the corresponding value from the ratio histogram. The resulting image is called the backprojected ratio histogram.

5. **Convolution**: The backprojected ratio histogram is then convolved with a kernel (usually a square or circular kernel). This step helps to identify the regions in the image where the colors in the model histogram are more likely to occur together.

6. **Peak Detection**: The resulting convolved image is then analyzed to detect peaks, which indicate the likely locations of the target object.

The Histogram Backprojection technique is robust to occlusion and can work well even when the objects containing the same colors occur in the image. It is also efficient and can be used for real-time object tracking.

As mentioned in the context, Histogram Backprojection ""finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target"" (Source: Swain1.pdf). 

For more detailed information, you can refer to the provided document (Swain1.pdf).","Histogram backprojection is a technique for locating an object in an image based on a model histogram of its appearance (e.g., color). Each pixel is assigned a probability based on how well it matches the model histogram, producing a probability map where high values indicate likely object locations.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main components of a CNN?,"The main components of a Convolutional Neural Network (CNN) are:

1. **Convolutional Layer**: This layer applies filters to the input image to extract features. The filter hyperparameters such as the number of filters, stride, and zero-padding are required in this layer.

2. **Activation Function**: This is used to introduce non-linearity into the model. Commonly used activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Softmax.

3. **Pooling Layer**: This layer reduces the spatial dimensions of the feature maps to reduce the number of parameters and the computation cost.

4. **Fully Connected Layer**: This layer is used for classification purposes. It takes the output from the convolutional and pooling layers and produces a probability distribution over the classes.

These components work together to enable a CNN to extract features from images and perform tasks such as image classification, object detection, and more.

Here is a simple example of how these components are used in a CNN:
```markdown
CNN Architecture:
1. Convolutional Layer -> ReLU Activation
2. Pooling Layer
3. Convolutional Layer -> ReLU Activation
4. Pooling Layer
5. Flatten
6. Fully Connected Layer -> Softmax Activation (for classification)
```
You can refer to the provided lecture notes and practical materials for more detailed information on CNNs, specifically `./documents/Lec-9-CNN.pdf` and `./documents/Practicals with Solutions - CNN.pdf`.","A Convolutional Neural Network (CNN) is composed of several layers, including the input layer, convolutional layer, pooling layer, activation function, fully connected layer, and output layer.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLUB: Sigmoid C: Softmax D: Leaky ReLU17Question 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation|Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns Ray, P. (14 Jan 2021). Convolutional Neural Network (CNN) and its application - All you need to know. https://medium.com/analytics-vidhya/convolutional-neural- network-cnn-and-its-application-all-u-need-to-know-f29c1d51b3e5MATLAB (n.d.). What is a Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network.htmlCOMP 2032CNN19University of Nottingham UK | CHINA | MALAYSIAFutureVision Transformer (ViT). | ; J., 1COMP 2032 CNN- Designed for Computer Vision with remarkable results - Uses neural network to split images into smaller patches, allowing model(s) to capture both local and global relationships within images.1Animation extracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
What is the purpose of the pooling layer in a CNN?,"The purpose of the pooling layer in a Convolutional Neural Network (CNN) is to reduce the spatial dimensions of the feature maps generated by the convolutional layers. This is done by downsampling the feature maps, effectively reducing the number of parameters and the number of computations required in the network.

According to the provided context from the lecture notes, the pooling layer is used after each CNN layer to ""reduce the size of feature map outputs by the CNN layer."" 

Some key benefits of using a pooling layer include:
1. **Reducing spatial dimensions**: Pooling reduces the height and width of the feature maps, which in turn reduces the number of parameters and computations required in the network.
2. **Retaining important information**: Pooling retains the most important information in the feature maps while discarding less important details.
3. **Improving translation invariance**: By downsampling the feature maps, pooling helps the network become more invariant to small translations in the input image.

Common types of pooling include:
- **Max pooling**: This involves taking the maximum value across each window of the feature map.
- **Average pooling**: This involves taking the average value across each window of the feature map.

By reducing the spatial dimensions of the feature maps, the pooling layer helps to:
- Reduce the risk of overfitting
- Improve the robustness of the network to small transformations
- Decrease the computational requirements of the network

You can refer to the provided documents, specifically `./documents/Lec-9-CNN.pdf` and `./documents/Practicals with Solutions - CNN.pdf`, for more detailed information on CNNs and the role of pooling layers within them.","The pooling layer occurs after a convolutional layer and its purpose is to reduce the dimensionality of the resulting feature map. Common pooling operations include max pooling, average pooling, and global average pooling.","out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f° University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input which is a 4D tensor (mini-batch size, channel, height, width), where mini-batch size is the number of images in the mini-batch, channel is the number of channels in each image and height and weight represents the height and width of the image. Since we pre-processed the MNIST images to grayscale earlier, channel will be set to 1. Both height and width will be set to 28 since each image of MNIST is 28 by 28 pixels.Next, we shall define the execution sequence of the two predefined convolutional layers. Each CNN layer is a combination of a convolutional layer and an activation function layer. A max-pooling layer is created after each CNN layer to reduce the size of feature map outputs by the CNN layer.class ToyModel(nn.Module):def forward(self, x):# conv layer 1x = self.conv1(x)# activationx = self.relu(x)# max poolingx = self.pooling(x)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 7 of 16f° University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science# conv layer 2x = self.conv2(x)x = self.relu(x)x = self.pooling(x)The output feature|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What is the difference between erosion and dilation in morphological image processing?,"In morphological image processing, erosion and dilation are two fundamental operations that are used to manipulate the shape and size of objects in an image.

**Dilation:**
Dilation is an operation that expands or grows the foreground (objects) in an image by adding pixels to the boundaries of the objects. It is used to fill in gaps, connect broken objects, or expand the size of objects. The effect of dilation is to make the objects larger and more prominent.

**Erosion:**
Erosion, on the other hand, is an operation that shrinks or reduces the foreground (objects) in an image by removing pixels from the boundaries of the objects. It is used to separate touching objects, remove small noise components, or reduce the size of objects. The effect of erosion is to make the objects smaller and less prominent.

In summary, the key differences between erosion and dilation are:

* Dilation expands the foreground, while erosion shrinks it.
* Dilation is used to fill in gaps and connect broken objects, while erosion is used to separate touching objects and remove small noise components.

These two operations are duals of each other, meaning that eroding the foreground is equivalent to dilating the background, and vice versa [./documents/Lec-5-Morphology.pdf].

To illustrate the difference, consider an example where you have a binary image with objects that are touching each other. Applying erosion to the image will separate the objects, while applying dilation will merge them together.

I hope this explanation helps clarify the difference between erosion and dilation in morphological image processing!","Erosion removes pixels on object boundaries, shrinking bright regions and enlarging dark ones. Dilation adds pixels to the boundaries of objects, enlarging bright regions and shrinking dark ones.","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.● Read in the Squares.tif image from Moodle.● Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.● Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)● Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’ pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and","sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the text_broken.tif image, perform dilation to fill in the gaps between the broken characters.3. ErosionErosion is the opposite of dilation, where the boundary of the foreground is shrunk instead, or it can also be thought of as performing dilation on the background. It is used to separate touching objects or remove small noise components.● Read in the Squares.tif image from Moodle.● Perform erosion with erode using all the structuring elements and compare the differences. Take note of the effect on both the small and large squares.4. OpeningOpening is a morphological operation which involves performing erosion on an image and then performing dilation. It helps smooth the contours of the foreground and removes small unwanted components.● Read in the Cells.png image from Moodle. (Note: The image is a colour image, you will need to convert it to binary first. Ensure that the cells are the foreground.)● Perform opening on the image with morphologyEx. Use a suitable structuring element to remove the small cells so that the image only contains the large cells.Prepared by|may recognize # date using ""00"" @s 1900 rather than the yeerr 2000. FIGURE G.S (a) Sarephle Gest of poor rowoloteon enh brates clearmciers imiagritied verw) i>) Srectunne dement (¢} Dilation of fa Ire Cb) Brokers Scgrments were pen aACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology16Edge Detection by Dilation- Dilation input image (note rounding of corners)- Subtract from original- Edges remain (pixels on the outside of the boundary)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology17Erosion - Binary image to erode 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the foreground pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’ pixels in the structuring element overlap (erode foreground) (intersect) the background then the foreground pixel is also set to backgroundACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology18Erosionooo CREE < ecocoBfeca 2 oo Mo coco] coo OBIE coo ORIRIEIEGResult of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and",
What is the Hough Transform used for?,"The Hough Transform is a technique used to detect shapes, such as lines and circles, in an image. It works by converting the image space into a parameter space and conducting a ""voting"" procedure on the points that are part of the edges in the image.

According to the lecture notes, the Hough Transform is used to:

1. Detect lines in an image by identifying the parameters (m, c) that define the line equation y = mx + c.
2. Identify shapes that can be represented by mathematical equations.

The Hough Transform is useful because it can handle noisy or incomplete data and can detect multiple instances of a shape in an image.

Source: ./documents/Lec-6B-Hough Transform.pdf","The Hough Transform is a feature extraction technique used to find instances of objects with a certain shape, such as lines or circles, within an image. It works by transforming points from the image space to a parameter space, where peaks correspond to the parameters of the detected shapes.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.● Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a “voting” procedure on the points which are part of the edges in the image.● Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.● Apply the Hough Line Transform on the edge-detected image using HoughLines.● To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (𝜌,𝜃) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ……………………………. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e© Nottingham “i UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don’t know the line parameters – we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line’s   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in the Tulip.jpeg image.● Were you able to achieve all the tasks? If not, why?Compare the results of the three operators (Sobel, LoG and Canny) and identify the differences. Which edge detection method do you think performed the best and why?2. Hough TransformHough Transform is a technique used to detect shapes (e.g., lines, circles) which can be represented by mathematical equations. This is done by converting the image space into a parameter space and conducting a “voting” procedure on the points which are part of the edges in the image.● Read the sudoku.jpg image from Moodle and convert it to binary. You may either apply thresholding or Canny edge detection on the image to achieve this. However, Canny would be better to highlight the edges which are areas of interest.● Apply the Hough Line Transform on the edge-detected image using HoughLines.● To find the optimal parameters, we must first be able to see the lines detected. The HoughLines function returns an array of (𝜌,𝜃) values, which we will use to draw the lines on the image. You may read more about this here:|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6B Hough TransformbisirLearning OutcomesIDENTIFY  1. How to find lines?  2. The Hough Transform  3. Other parameter spaces COMP 2032Hough Transform2University of Nottingham UK | CHINA | MALAYSIAFinding Lines:Template MatchingirWhy Lines?Geometric primitives make important vision tasks easierE.g., matching or classifying images- Some geometric objects are invariant under projection from the (3D) world to (2D) image - Straight lines in the world create straight lines on the image A straight line in the image is evidence for a straight line in the worlda @ szen (im (]ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform4fThe Problem- Suppose straight lines are important - Edge detection provides a set of points (xi, yi) which are likely to lie on those lines - But how many lines are there and what are their parameters?ACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough TransformyPTA TT TT TAIN LU/T TT TT INT Ix5irTemplate Matching- One solution is to take a straight line and match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|match it to all possible image positions and orientations - Compute a measure of fit to the edge data E.g., count how many edges are under each possible line Incredibly expensive!ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Hough Transformy  x 6irirTemplate Matching vs. Hough'IClassic template matching takes a line, lies   it on the image data and asks:  -  Does it fit here?  -  Here?  -  Here?  -  How about here?  -  Here then?  - ……………………………. The Hough takes each data   item (edge) and asks:  What lines could pass   through this? PeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transform7University of m @e© Nottingham “i UK | CHINA | MALAYSIAThe Hough TransformLine ParametersThe standard equation of a line is- If we know the line parameters m, c - we can vary x, compute y and draw the line - This line represents the set of (x,y) pairs that satisfy the above equationACK: Prof. Tony Pridmore, UNUKCOMP 2032Hough Transformy = mx + cy = mx + c  x yBut we don’t know the line parameters – we just have some data points (xi, yi)9Parameter SpaceBut if a data point (xi, yi) lies on a line then that line’s   parameters m, c must satisfy  yi = mxi + c  So we can represent all possible lines",
What is the Livewire segmentation algorithm?,"The Livewire segmentation algorithm, also known as ""Intelligent Scissors,"" is a technique used in image processing to segment images by avoiding traversal of object boundaries. It represents the input image as a graph and uses a shortest path algorithm to find the optimal boundary between the object of interest and the background.

The Livewire algorithm involves six significant steps:

1. Image Processing
2. Converting Image to Graph
3. Shortest Path Algorithm
4. Interactive Visualisation
5. Displaying the Image
6. Event Handling

This algorithm is famously used to segment out Regions of Interest (ROI) for further processing and is based on graph theory, making it an effective tool for image segmentation tasks.

To implement Livewire, you can use libraries such as Matplotlib, Skimage, Math, Heapq, and Itertools in Python. An unofficial implementation of Livewire is available, as it is not natively supported in OpenCV.

References:
- COMP2032 Laboratory Sheet: Livewire Segmentation Algorithm 
- E. N. Mortensen and W. A. Barrett, ""Intelligent Scissors for Image Composition,"" in ACM Computer Graphics (SIGGRAPH '95), 1995.","Livewire, also known as ""Intelligent Scissors,"" is an interactive image segmentation algorithm. It allows a user to trace an object's boundary, and the algorithm finds the minimum-cost path that follows object boundaries. It's based on graph theory and uses Dijkstra's algorithm to find the shortest path between user-selected points.","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the GrabCut algorithm?,"The GrabCut algorithm is an interactive image segmentation technique used for foreground extraction. It is an extension of the graph-cut approach, which combines both texture (color) and edge (contrast) information to achieve segmentation.

### Key Features of GrabCut

1. **Iterative Optimization**: GrabCut uses an iterative version of the graph-cut optimization algorithm to refine the segmentation.
2. **Simplified User Interaction**: The algorithm reduces the need for extensive user interaction, requiring only a rough rectangle around the object of interest.
3. **Border Matting**: GrabCut includes a robust border matting algorithm to estimate the alpha-matte around object boundaries and the colors of foreground pixels.

### How GrabCut Works

1. The user draws a rectangle around the object of interest.
2. The algorithm initializes the segmentation using the pixels outside the rectangle as background and the pixels inside as foreground.
3. The iterative graph-cut optimization refines the segmentation, using both color and edge information.
4. The border matting algorithm is applied to estimate the alpha-matte around object boundaries.

### Advantages of GrabCut

* **Accurate Segmentation**: GrabCut can achieve accurate segmentation with a relatively small amount of user interaction.
* **Robustness to Background**: The algorithm is robust to background clutter and can handle complex scenes.

### Example Use Cases

GrabCut is useful for various image editing tasks, such as:

* Object extraction for compositing or replacement
* Background removal or replacement
* Image segmentation for object recognition or tracking

For more information, you can refer to the original paper by Rother et al. (2004) [./documents/grab_cut.pdf].

**Cited Materials:**

<a href=""http://localhost:5003/documents/grab_cut.pdf"" target=""_blank"">grab cut</a>

<a href=""http://localhost:5003/documents/Lec-8-Interactive Segmentation.pdf"" target=""_blank"">Lec 8 Interactive Segmentation</a>

","GrabCut is an interactive foreground extraction algorithm that uses iterated graph cuts. The user draws a rectangle around an object, and the algorithm automatically extracts the object from the background.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single “out- side” lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images’, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|second experiment we compare GrabCut using a single “out- side” lasso (e.g. red line in fig. 2c (top)) with Graph Cut using two lassos, outer and inner (e.g. red and white lines in fig. 2c (top)). Even with missing foreground training data, GrabCut performs at almost the same level of accuracy as Graph Cut. Based on a ground truth database of 50 images’, the error rates are 1.36 +0.18% for Graph Cut compared with 2.13 + 0.19% for GrabCut .In conclusion, a new algorithm for foreground extraction has been proposed and demonstrated, which obtains foreground alpha mattes of good quality for moderately difficult images with a rather modest degree of user effort. The system combines hard segmenta- tion by iterative graph-cut optimisation with border matting to deal with blur and mixed pixels on object boundaries.Image credits. Images in fig. 5, 8 and flower image in fig. 1 are from Corel Professional Photos, copyright 2003 Microsoft Research and its licensors, all rights reserved. The llama image (fig. 1, 2) is courtesy of Matthew Brown.2see www.research.microsoft.com/vision/cambridge/segmentation/No User InteractionFigure 8: Results using GrabCut. The first row shows the original",
What is histogram equalization?,"Histogram equalization is a technique used in image processing to improve the contrast of an image. It works by redistributing the pixel values in the image to achieve a uniform distribution, thereby enhancing the visibility of details in both bright and dark areas.

To perform histogram equalization, the following steps are taken:

1. **Compute the Cumulative Distribution Function (CDF)**: The CDF of the input image is calculated. This represents the cumulative sum of the pixel values in the image.
2. **Map Input Pixel Values to Output Pixel Values**: Using the CDF as a look-up table, each pixel in the input image is mapped to a corresponding output pixel intensity.
3. **Scale the Equalized Image**: The CDF values are typically in the range of 0 to 1, so the equalized image is scaled to fit the range supported by the output image format.

The resulting image has a histogram that is approximately uniform, meaning that the pixel values are evenly distributed across the available range. This can significantly improve the contrast of the image, making it more visually appealing and easier to analyze.

For example, consider an image with a low-contrast histogram. After applying histogram equalization, the resulting histogram is more evenly distributed, and the image appears more detailed and clearer.

It's worth noting that histogram equalization can be compared to another contrast enhancement technique called contrast stretching. While both methods aim to improve the contrast of an image, they work differently. Contrast stretching linearly stretches the range of pixel intensity values to a specified range, usually [0,255]. In contrast, histogram equalization redistributes the pixel values to achieve a uniform distribution.

Sources:
- `./documents/Lec-2-Histogram (Whole Image Understanding).pdf`
- `./documents/Practicals - Histogram (Whole Image Understanding).pdf`",Histogram equalization is a technique used to improve the contrast of an image by redistributing the pixel intensities to achieve a more uniform distribution. This is done by computing the cumulative density function (CDF) and using it as a look-up table to map input pixel values to output pixel values.,"the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee —ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 – 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let’s BREAK this down…So, in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their","the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|in the input image into a corresponding pixel with level skLeeee ee ee ee ee eee ee eee ee —ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I-114University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Practice)fThe AlgorithmTo perform histogram equalisation1. Compute the CDF of the input image2. For each pixel in the input image, the corresponding output pixel intensity is calculated by using the CDF as a look-up table3. CDF values will be in the range of 0 – 1, scale the equalised image to fit the range supported by the output image formatThe histogram of the output image will be approximately uniformACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)| | I16An Example Consider a 64 x 64 pixel, 3 bit (8 grey level) image(Example from Gonzalez & Woods)rk nk Pr(rk) T(rk) sk Round 0 790 0.19 0.19 1.33 1 1 1023 0.25 0.44 3.08 3 2 850 0.21 0.65 4.55 5 3 656 0.16 0.81 5.67 6 4 329 0.08 0.89 6.23 6 5 245 0.06 0.95 6.65 7 6 122 0.03 0.98 6.86 7 7 81 0.02 1.00 7.00 7ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)17An Example Input histogram  T(rk) scaled back to 0 - 7  Equalised|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Histogram (Whole Image Understanding)Histogram Equalisationy=f(x) Pp(y)dy p(y) p(y)dy=p(x)dxACK: Prof. Tony Pridmore, UNUKHistogram of low-contrast image2irA Histogram Equalisation ExampleBy completing the table below, apply histogram equalisation to this data and show:The mapping from input pixel values to output pixel valuesThe normalised histogram of the output imageWhat quick way is there to check that your solution is valid?Input Pixel Value Frequency 0 1000 1 5000 2 1000 3 1000 4 1000 5 0 6 500 7 500ACK: Prof. Tony Pridmore, UNUK3SolutionN Input Pixel Value 1 0 2 1 3 2 4 3 5 4 6 5 7 6 8 7 Frequency 1000 5000 1000 1000 1000 0 500 500 Normalised Frequency 0.1 0.5 0.1 0.1 0.1 0 0.05 0.05 Cumulative Density 0.1 0.6 0.7 0.8 0.9 0.9 0.95 1.0 Output Pixel Value (rounded) 0.7 (1) 4.2 (4) 4.9 (5) 5.6 (6) 6.3 (6) 6.3 (6) 6.7 (7) 7 (7) Output 0 0.1 0 0 0.5 0.1 0.2 0.1NormalisedFrequency100001bp1ACK: Prof. Tony Pridmore, UNUK4Let’s BREAK this down…So, in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their",
What is the difference between sampling and quantization in digital image formation?,"Sampling and quantization are two fundamental processes in digital image formation.

**Sampling** refers to the process of digitizing the spatial coordinates of an image. It involves capturing the image at discrete points in space, effectively converting the continuous spatial information into a discrete representation. The sampling rate determines the spatial resolution of the image, with higher sampling rates resulting in higher resolutions.

**Quantization**, on the other hand, refers to the process of digitizing the light intensity function or the color values of an image. It involves assigning a digital value to the intensity or color of each pixel, effectively converting the continuous intensity or color information into a discrete representation. The number of quantization levels determines the grey level, color, or radiometric resolution of the image.

To illustrate the difference, consider a camera capturing an image. Sampling occurs when the camera captures the image at discrete points (pixels), while quantization occurs when the camera assigns a digital value to the intensity or color of each pixel.

As stated in the lecture notes: ""Sampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolution"" (Lec-1B-Digital Images and Point Processes.pdf).

In summary, sampling deals with the spatial coordinates, while quantization deals with the intensity or color values of an image. Both processes are essential in digital image formation, and their combined effects determine the overall quality of the resulting digital image.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1B-Digital Images and Point Processes.pdf"" target=""_blank"">Lec 1B Digital Images and Point Processes</a>

","Sampling is the digitization of spatial coordinates, determining the spatial resolution of the image (number of pixels). Quantization is the digitization of the light intensity function, determining the number of grey levels or colors that can be represented at each pixel.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE…Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE…Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:«< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‘@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression• Map vector values (R,G,B) onto scalar values• Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f’Building a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f’Building a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Basic terminology: sampling and quantisation - A little image processing: re-sampling, re-sizing, re-quantisation - Gain, bias, contrast stretching, gamma correction - Simplest methods, but useful - RGB, HSV, LAB etc. - Image pre-processing: choosing a colour space is a key step in practical applications, but not really IPHistogram- Histogram equalisation - Comparing images: histogram intersection, histogram ratio - An application: Image retrievalACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI,11A walk down MEMORY LANE…Spatial & Non-linear filtering- Convolution is key- Noise removal: mean, Gaussian filtering- Enhancement: unsharp masking, Laplacian filtering;-I ,- Median, anisotropic diffusion, bilateral filteringThresholding and Binary ImagesI- Otsu, Unimodal thresholding--- Connected components, morphology (erosion, dilation)_-Derivatives, Edges & Hough Transform'.- Edge detection: Roberts, Sobel, Marr-Hildreth, CannyACK: Prof. Tony Pridmore, UNUK- Hough transform to detect lines and circles'-COMP 2032Finale and RevisionI12I IA walk down MEMORY LANE…Segmentation  - Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  -|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|(i.e., quantising scalar values)Uniform scalarquantisation:quantisation:«< @8D><@24D><@40D><@...D>@248>Non-uniform scalarD1D2D3D4uantisation:quantisation:‘@** @** @rs @s@>ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression26Vector QuantisationPalettised image (gif)True colour R,G,B  8 bits each  1677216 possible colours ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression• Map vector values (R,G,B) onto scalar values• Multiple vectors map to each scalarVectorquantisationgif8 bits per pixel256 possible colours27Vector Quantisation(r, g, b)For each pixel inthe original imageFind the closestcolour in theColour TableACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressionr0  g0  b0  r1  g1  b1  r255 g255 b255  Colour Table Record the index of that colour (for storage or transmission) To reconstruct the image, place the indexed colour from the Colour Table at the corresponding spatial location28f’Building a PaletteA pixel corresponds to a point in the 3 dimensional R, G, B spaceACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression29f’Building a PaletteMap all pixels into R,G,B space, Clouds of pixels are formedACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compressiona a we 2",
What is aliasing in the context of image sampling?,"## Aliasing in Image Sampling

Aliasing occurs when two signals (images) become indistinguishable when sampled. In the context of image processing, it happens when the sampling rate is less than the Nyquist rate of the original image.

### Causes of Aliasing

*   Under-sampling: When the sampling rate is too low to capture the details of the original image.
*   High-frequency signals: When the original image contains high-frequency components that are not properly captured during sampling.

### Effects of Aliasing

*   The sampled image becomes distorted and loses its original details.
*   The human vision system reconstructs the sampled image, but it may not accurately represent the original image.

### Prevention of Aliasing

To prevent aliasing, it's essential to ensure that the sampling rate is greater than or equal to the Nyquist rate of the original image. If this is not possible, smoothing out high-frequency signals before sampling can help reduce the effects of aliasing.

### Example

For instance, when downsampling an image, it's crucial to smooth out high-frequency signals before reducing the sampling rate. This can be achieved by applying a low-pass filter to the original image before downsampling.

### Reference

The concept of aliasing is discussed in the lecture notes on Digital Images and Point Processes [./documents/Lec-1B-Digital Images and Point Processes.pdf].",Aliasing occurs when an image is under-sampled (sampled at a rate below the Nyquist rate). This results in artifacts where two different signals (images) become indistinguishable when sampled.,"fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing • Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original • Smooth out high frequency signals before sampling so its impossible to “see” the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? • When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels• Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can’t increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition “seamlessly” in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass ﬁltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter ﬁtting of a Butterworth low-pass ﬁlter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2","fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point ProcessesJ9fAnti-Aliasing • Aliasing can be introduced when an image is resampled, if the sampling rate of the new image is less than the Nyquist rate of the original • Smooth out high frequency signals before sampling so its impossible to “see” the aliasOriginal signal  Digital Images and Point Processes ACK: Prof. Tony Pridmore, UNUKCOMP 2032Smoothed signal10Re-Sampling & Re-Sizing26  ?  37  ?  26  37  ?  ?  ?  ?  35  44  35  ?  44  ?  ?  ?  ?  ? • When unsampling, need to interpolate from the known values to produce an estimate at the unknown pixels• Average the known values in the local region centred on each unknown pixel, fit some kind of function through known values…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes11irRe-quantisationPixel values are integers in a fixed range Grey level resolution can be dropped by dividing each pixel value by a constant, but there is a side effect Can’t increase grey level resolution of a single image Super-resolution methods exist that combine multiple exposures of the same scene, and so have more|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|is desirable to perform image composition “seamlessly” in order to make it believ- able. That is, we should not be able to detect where the paste occurred. However, the source and the destination images will often have differing spatial frequencies or contrast due to differ- ences in focus or lighting when the images were acquired. Thus, equivalencing of spatial frequencies and normalization of contrast is sometimes desirable in order to produce a convincing composi- tion.Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. Convincing composition often requires the spectra of the object and the destination image to match. This is accomplished by low-pass ﬁltering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter ﬁtting of a Butterworth low-pass ﬁlter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2",
How can noise be reduced in an image?,"Noise reduction in an image can be achieved through various methods, primarily based on the concept of spatial filtering. There are two main types of filtering: linear and non-linear filtering.

### Linear Filtering

Linear filtering involves convolving the image with a filter kernel. This process is done by sliding the filter over every pixel in the image and performing multiplication and addition operations. The output of each pixel is the weighted sum of its neighboring pixels.

Two common types of linear filters used for noise reduction are:

1. **Mean Filtering**: This involves replacing each pixel value with the average value of its neighboring pixels. The size of the neighborhood is determined by the kernel size. For example, using a kernel size of 3, 5, or 7 for mean filtering can help reduce noise.
   
2. **Gaussian Filtering**: This is similar to mean filtering but uses a Gaussian distribution to weight the neighboring pixels. The standard deviation (sigma) of the Gaussian filter controls the amount of smoothing. A larger sigma results in more smoothing but may also lose more detail. It's essential to adjust the mask size according to the sigma value.

### Example from Lab 3

In Lab 3, you can apply mean filtering using `blur` with kernel sizes of 3, 5, and 7, and Gaussian filtering using `GaussianBlur` with sigma values of 0.5, 1.0, and 1.5. Remember to adjust the mask size when changing the sigma value for Gaussian filtering.

### Non-Linear Filtering

While the provided context focuses on linear filtering, non-linear filtering methods are also effective for noise reduction. These methods do not follow a straightforward convolution process and can be more effective in certain scenarios, such as preserving edges.

### Averaging Multiple Images

If you have multiple images of the same scene, taking the mean value of each pixel across these images can significantly reduce noise. This method is based on the principle that noise is randomly added to the true pixel values, and averaging multiple estimates will cancel out the random noise.

### Reference
For more details on linear filters and their application, you can refer to the lecture notes on Linear Filters (`./documents/Lec-3A-Linear Filters.pdf`). Additionally, Lab 3 (`./documents/Lab 3.pdf`) provides hands-on experience with applying mean and Gaussian filtering for noise reduction.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-3A-Linear Filters.pdf"" target=""_blank"">Lec 3A Linear Filters</a>

<a href=""http://localhost:5003/documents/Lab3.pdf"" target=""_blank"">Lab3</a>

","Noise can be reduced using various filtering techniques. For additive noise like Gaussian noise, mean or Gaussian filtering can be effective. For ""salt and pepper"" noise, a median filter is often used.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I’) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‘‘1’* means the object has been correctly located, a “2” indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise• Noise is randomly added to each value• Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern – multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)×P(x-1,y-1)  + F(0,-1)×P(x,y-1)  + F(+1,-1)×P(x+1,y-1)  + F(-1,0)×P(x-1,y)  + …  + F(+1,+1)×P(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise••Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI• On average they give the right value• They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (μ) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2πσ  e  (x  --  2σ  μ)  2 μ28Gaussian NoiseThe level of noise is related to the Gaussian parameter, σσ = 1σ = 10σ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.","rate is shown more graphically in table 7.Because the convolution can be carried out on a reduced resolution image, Histogram Backprojection is very efficient. Its complexity on a sequential com- puter would be O(7 + c * I’) where is the number of pixels in the full-resolution image, J' is the number of pixels in the reduced resolution image, and c is the number of pixels in the convolution mask applied to the reduced resolution image.Table 7. Performance of Histogram Backprojection. The number in each square is the rank of peak that falls into the corresponding square in figure 5 when looking for the model whose image is in that square. A ‘‘1’* means the object has been correctly located, a “2” indicates the object created the second largest peak in the convolved backprojected image, etc.EEEHistogram Backprojection has been implemented in a Datacube image processor, with a Sun 4/260 work- station as its host. The Datacube can do histograms, subsample, and convolutions with 8 x8 masks within a frame time. Using 8 8 x8 (512 total) size histograms and a reduced image of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time|Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel will reduce noise• Noise is randomly added to each value• Mean value added is 0If you average a large set of estimates of the1same pixel, the random noise values willcancel out42 43 44 41 40 42 42 44 404 aACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters-| ""Se- Given only a single image, ' averaging over a local region i has a similar effectI - Ideally, we would choose the region to only include pixels that should have the -same value10irirSpatial Filtering: ConvolutionMany filters follow a similar pattern – multiplying each image value by a corresponding filter entry, and summing the resultsF(-1,-1)  F(0,-1)  F(+1,-1)  F(-1,0)  F(0,0)  F(+1,0)  F(-1,+1)  F(0,+1)  F(+1,+1) Filter Window |P(x-1,y-1)  P(x,y-1)  P(x+1,y-1)  P(x-1,y)  P(x,y)  P(x+1,y)  P(x-1,y+1)  P(x,y+1)  P(x+1,y+1) Picture Window F(-1,-1)×P(x-1,y-1)  + F(0,-1)×P(x,y-1)  + F(+1,-1)×P(x+1,y-1)  + F(-1,0)×P(x-1,y)  + …  + F(+1,+1)×P(x+1,y+1)  Result ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters11irFilteringMore generally, with a filter with radius r - px,y is the original image value at|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||illumination pattern, etc.  - Object identity  -  Viewed surface orientation, colour, etc  -  Illumination ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters6||Image Noise| NoiseNoiseSmall errors in image values=OriginalOriginalJPEGDifferenceDifference(Enhanced)ACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear FiltersImperfect sensors introduce noise Image compression methods are lossy: repeated coding & decoding adds noise••Noise is often modelled as additive:  Recorded value = true value +   random noise value 7Gaussian NoiseSensors often give a measurement a littleoff the true valueI• On average they give the right value• They tend to give values near the right value rather than far from itWe model this with a Gaussianah eaiennen tahini ian iain ial ietie inne (- \ ,- Mean (μ) = 0I-ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Linear Filters1 1 1|P(x)  =  1  2πσ  e  (x  --  2σ  μ)  2 μ28Gaussian NoiseThe level of noise is related to the Gaussian parameter, σσ = 1σ = 10σ = 20Image with varying degrees of Gaussian noise addedACK: Prof. Tony Pridmore, UNUKCOMP 2032Linear Filters9Noise Reductionpce ccce: cicrcisee torso: ,If you have multiple images, taking the-'mean value of each pixel|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.",
What is the difference between linear and non-linear filters?,"Linear filters and non-linear filters are two types of filtering techniques used in image processing.

**Linear Filters**

Linear filters are based on convolution, where a filter mask is slid over every pixel in the image, and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighboring pixels. Examples of linear filters include mean filtering and Gaussian filtering.

In mean filtering, the output pixel value is the average of the neighboring pixels. In Gaussian filtering, the output pixel value is a weighted sum of the neighboring pixels, with the weights determined by a Gaussian distribution.

**Non-Linear Filters**

Non-linear filters, on the other hand, do not use convolution. Instead, a non-linear mathematical operation is performed on each pixel independently, based on the neighboring pixel values. Examples of non-linear filters include median filtering, anisotropic diffusion, and bilateral filtering.

In median filtering, the output pixel value is the median of the neighboring pixels. This is useful for removing salt and pepper noise.

The key differences between linear and non-linear filters are:

1. **Operation**: Linear filters use convolution, while non-linear filters use non-linear mathematical operations.
2. **Output**: Linear filters produce a weighted sum of neighboring pixels, while non-linear filters produce an output based on a non-linear operation on neighboring pixels.
3. **Noise reduction**: Linear filters tend to smooth out noise, while non-linear filters can preserve or even enhance edges and details.

When to use each:

* Use linear filters (e.g., mean, Gaussian) when you want to smooth out noise or blur an image.
* Use non-linear filters (e.g., median, anisotropic diffusion, bilateral) when you want to preserve edges and details, or remove specific types of noise (e.g., salt and pepper).

Source: ['./documents/Lab3.pdf', './documents/Lec-3B-Non-Linear Filters.pdf']","Linear filters, like mean and Gaussian filters, compute a linear function of a set of pixel values. Non-linear filters, such as the median filter, do not follow this linear relationship and tend to preserve or even enhance sharp image changes.","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.● Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.● Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 – 0.1), K (0 – 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.● Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 – 100) and sigma (50 – 250). You may keep both sigma values the same for simplicity’s sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  • Median filtering  • Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‘esau ad oe € THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i• A false saturation gives a white spot in the image (salt)• A failed response gives a black spot in the image (pepper)• Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 3: Linear & Non-Linear FilteringThe noise reduction methods discussed in the last lecture are all based on the idea of spatial filtering, and as discussed, there are two types of filtering: linear and non-linear.1. Linear FilteringLinear filtering is done using convolution, whereby a linear filter is slid over every pixel in the image and multiplication/addition operations are performed. The output of each pixel is the weighted sum of the neighbouring pixels. We will explore two types of linear filters, which are mean filtering and Gaussian filtering. The Moodle page contains a version of the cameraman image where salt and pepper noise has been added. Using that image, apply the several types of filters to reduce the noise as much as possible.● Use blur to perform mean filtering on the image with a kernel size of 3, 5 and 7.● Use GaussianBlur to perform Gaussian filtering with sigma values of 0.5, 1.0 and 1.5. Remember that you will need to vary the size of the mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results.|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|mask as you change the Gaussian parameter. Refer to the lecture notes for guidance on mask size.● Examine your results. Where do you see differences between Gaussian filtering and mean filtering with a similarly sized filter?2. Non-Linear FilteringNon-linear filtering, on the other hand, is done without convolution. Instead, a non-linear mathematical operation is performed on each pixel independently based on the neighbouring pixel values, resulting in the output pixel value. The three non-linear filters we will explore are median filtering, anisotropic diffusion and bilateral filtering.● Use medianBlur to perform median filtering with a kernel size of 3, 5 and 7.● Use anisotropicDiffusion to perform anisotropic diffusion and understand the parameters in the function. Try experimenting with the parameter values: alpha (0.01 – 0.1), K (0 – 100) and with iterations of 1, 2 and 3. Compare the effects of changing the parameter values.● Use bilateralFilter to perform bilateral filtering and understand the parameters. Try experimenting with the parameter values d (1 – 100) and sigma (50 – 250). You may keep both sigma values the same for simplicity’s sake. Compare the effects of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 3B Non-Linear FiltersbisirLearning OutcomesIDENTIFY  1. Median Filtering  2. Anisotropic Diffusion  3. Bilateral Filtering COMP 2032Non-Linear Filters2Non-Linear Filters- Convolution with a mask of weights   compute a linear function of a set   of pixel values  - Many operations can be   implemented this way, but not all:  • Median filtering  • Anisotropic diffusion/Bilateral filtering  Linear filters smooth sharp image   changes, nonlinear filters tend to   Difference  preserve or even enhance them -,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersoii Ras | HANI saaaaaaa ie see a UIUITT eee || ‘esau ad oe € THT aaaaaaaa ,aaaaa **e a|Gaussian Smoothing|3University of m Nottingham P UK | CHINA | MALAYSIAMedian FilteringSalt and Pepper Noise- 'Sometimes sensors either fail to respond orsaturate in error*e i• A false saturation gives a white spot in the image (salt)• A failed response gives a black spot in the image (pepper)• Sometimes called speckle noise1%20%ACK: Prof. Tony Pridmore, UNUKAn image with varying amounts of salt and pepper noise addedCOMP 2032Non-Linear|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the",
Explain the concept of superpixels.,"Superpixels are a fundamental concept in image processing. They represent a group of pixels that share similar characteristics, such as color, texture, or intensity. The idea is to divide an image into smaller regions, called superpixels, which are more meaningful and easier to process than individual pixels.

To put it simply, superpixels are like a ""middle ground"" between pixels and objects in an image. They help to reduce the complexity of an image by grouping pixels into larger, more coherent regions.

Some key benefits of superpixels include:

1. **Reduced computational complexity**: By grouping pixels into superpixels, the number of units to process is significantly reduced, making it easier to perform tasks like object recognition, segmentation, and tracking.
2. **Improved representation**: Superpixels can capture more meaningful information about the image structure, such as boundaries, textures, and colors, which can be useful for various image analysis tasks.
3. **Flexibility**: Superpixels can be used as a preprocessing step for various image processing tasks, such as segmentation, object detection, and tracking.

There are several algorithms for generating superpixels, including the SLIC (Simple Linear Iterative Clustering) algorithm, which is a popular and efficient method. SLIC clusters pixels in the combined five-dimensional color and image plane space to generate compact, nearly uniform superpixels.

To illustrate this concept, consider an image of a landscape with different objects, such as trees, sky, and grass. Superpixels can be used to group the pixels representing the sky into a single region, or the pixels representing the trees into another region. This can be useful for tasks like object recognition or image segmentation.

If you'd like to know more about superpixels or have specific questions about their application, feel free to ask! 

You can refer to the paper ""SLIC Superpixels"" by Radhakrishna Achanta et al. for more information on superpixels and the SLIC algorithm. [./documents/SLIC_Superpixels.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","Superpixels are groups of pixels that are similar in color and proximity. They provide a convenient primitive from which to compute local image features, capturing redundancy in the image and reducing the complexity of subsequent image processing tasks.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oﬀer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eﬃcacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eﬃciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiﬁcation model for segmentation. ICCV (2003) 10–172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oﬀer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by ﬁnding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|for a low computational overhead. There are few superpixel algorithms that can oﬀer this and scale up for practical applications that deal with images greater than 0.5 million pixels. We present a novel O(N) complexity superpixel segmentation algorithm that is simple to implement and outputs better quality superpixels for a very low computational and memory cost. It needs only the number of desired superpixels as the input parameter. It scales up linearly in computational cost and memory usage. We prove the eﬃcacy of our superpixels in object category recognition and medical image segmentation, where compared to other state-of-the art algorithms, we obtain better quality and higher computational eﬃciency.7 AcknowledgementsThis work is supported by the National Competence Center in Research on Mobile Information and Communication Systems (NCCR-MICS), a center sup- ported by the Swiss National Science Foundation under grant number 5005- 67322.References1. Ren, X., Malik, J.: Learning a classiﬁcation model for segmentation. ICCV (2003) 10–172. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. SIGGRAPH (2005)3. Li, Y., Sun, J., Tang, C.K., Shum, H.Y.: Lazy snapping. SIGGRAPH|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|[13], but it remains computationally expensive for large images. The superpixels from NC05 have been used in body model estimation [6] and skeletonization [5].Fezenszwalb and Huttenlocher [8] (GS04) present another graph-based seg- mentation scheme that has been used to generate superpixels. This algorithm performs an agglomerative clustering of pixel nodes on a graph, such that each segment, or superpixel, is the shortest spanning tree of the constituent pixels. GS04 has been used for depth estimation [2]. It is O(NlogN) complex and is quite fast in practice as compared to NC05. However, unlike NC05, it does not oﬀer an explicit control on the number of superpixels or their compactness.A superpixel lattice is generated by [14] (SL08) by ﬁnding optimal vertical (horizontal) seams/paths that cut the image, within vertical (horizontal) strips of pixels, using graph cuts on strips of the image. While SL08 allows control of the size, number, and compactness of the superpixels, the quality and speed of the output strongly depend on pre-computed boundary maps.4 Achanta et al.2.2 Gradient-ascent-based algorithmsStarting from an initial rough clustering, during each iteration gradient",
What is the SLIC algorithm for generating superpixels?,"The SLIC (Simple Linear Iterative Clustering) algorithm is a novel method for generating superpixels. It clusters pixels in the combined five-dimensional color and image plane space to efficiently produce compact, nearly uniform superpixels.

Here's a simplified overview of how SLIC works:

1. **Initialization**: The algorithm starts by initializing cluster centers at regular intervals, depending on the desired number of superpixels (K).
2. **Clustering**: Pixels are then clustered based on their proximity to the cluster centers in the 5D space (color and spatial information).
3. **Distance Measure**: The distance between a pixel and a cluster center is calculated using a weighted sum of color and spatial distances.
4. **Iteration**: The clustering process is repeated iteratively until convergence or a stopping criterion is met.

The SLIC algorithm has several benefits, including:

* **Efficiency**: It has a low computational overhead compared to other state-of-the-art methods.
* **Compactness**: It produces compact, nearly uniform superpixels.
* **Simplicity**: The algorithm is easy to implement and use, with a single parameter (K) controlling the number of superpixels.

According to the provided context, SLIC outperforms other state-of-the-art methods in terms of segmentation quality, as measured by boundary recall and under-segmentation error [./documents/SLIC_Superpixels.pdf].

**Cited Materials:**

<a href=""http://localhost:5003/documents/SLIC_Superpixels.pdf"" target=""_blank"">SLIC Superpixels</a>

","SLIC (Simple Linear Iterative Clustering) is an algorithm that clusters pixels in a combined five-dimensional color and image plane space to generate compact, nearly uniform superpixels. It is known for its efficiency and simplicity, with a single parameter specifying the number of superpixels.","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciﬁc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oﬀer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481×321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required","SLIC Superpixels*Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S¨usstrunkSchool of Computer and Communication Sciences (IC) ´Ecole Polytechnique F´edrale de Lausanne (EPFL)Abstract. Superpixels are becoming increasingly popular for use in computer vision applications. However, there are few algorithms that output a desired number of regular, compact superpixels with a low com- putational overhead. We introduce a novel algorithm that clusters pixels in the combined ﬁve-dimensional color and image plane space to eﬃ- ciently generate compact, nearly uniform superpixels. The simplicity of our approach makes it extremely easy to use – a lone parameter speciﬁes the number of superpixels – and the eﬃciency of the algorithm makes it very practical. Experiments show that our approach produces superpix- els at a lower computational cost while achieving a segmentation quality equal to or greater than four state-of-the-art methods, as measured by boundary recall and under-segmentation error. We also demonstrate the beneﬁts of our superpixel approach in contrast to existing methods for two tasks in which superpixels have already been shown to increase|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|pixels irrespective of K. Note that, like [21], SLIC implicitly sets bounds on distance calculations, but does not need to com- pute or carry forward these bounds for the subsequent iterations. Unlike most of these segmentation schemes, which are very general in nature, SLIC is specif- ically tailored to perform superpixel clustering using the distance measure of Eq. (1) and is much simpler.4 ComparisonIn this section we compare our superpixel segmentation method with four state- of-the-art algorithms, namely, GS041 [8], NC052 [6], TP093 [12], QS094 [7] using publicly available source codes. GS04 and NC05 are graph based methods while TP09 and QS09 are gradient based approaches. NC05 and TP09 are designed to output a desired number of superpixels while GS04 and QS09 require parameter tuning to obtain a desired number of superpixels. This choice of algorithms should provide a good representation of the state-of-the-art.Fig. 2 provides a visual comparison of our output against these algorithms. To provide a qualitative comparison, we use the under-segmentation error and boundary recall measures, similar to the ones used by Levenshtein et al. [12] for this purpose, computed using the|review existing image segmentation algorithms and focus on their suitability for producing superpixels. Not all them were designed for this speciﬁc purpose and may lack the ability to control the size, number, and compactness of the segments, but we include them in our discussion nonetheless. We broadly classify superpixel algorithms into graph-based and gradient-ascent- based. Our survey, summarized in Table 1, considers the quality of segmentation, and the ability of these algorithm to control the size and number of superpixels.EPFL Technical Report 149300 3Table 1. Comparison of state of the art superpixel segmentation algorithms. N is the number of pixels in the image. GS04 and QS09 do not oﬀer explicit control of the number of superpixels. SL08 complexity given in this table does not take into account the complexity of the boundary map computation. GS04 is O(NlogN) complex but is comparable in speed to SLIC for images less than 0.5 million pixels while TP09 is also O(N) complex but is 10 times slower than SLIC for 481×321 pixel images. In the case of QS09, d is a small constant (refer to [10] for details). The number of parameters listed in the table is the minimum required",
What is the purpose of the activation function in a CNN?,"The purpose of the activation function in a Convolutional Neural Network (CNN) is to introduce non-linearity into the model. This is necessary because the convolution and pooling operations are linear, and without non-linearity, the model would not be able to learn complex representations.

Activation functions are used after the convolutional and fully connected layers to transform the output into a non-linear representation. Commonly used activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Softmax.

ReLU is often used in the hidden layers because it is computationally efficient and easy to compute. However, it can result in dying neurons if the input is negative.

Sigmoid and Softmax are often used in the output layer, depending on the task. Sigmoid is typically used for binary classification problems, while Softmax is used for multi-class classification problems.

In the context of CNNs, activation functions help the model to learn more complex and abstract representations of the input data, which is essential for tasks such as image classification, object detection, and segmentation.

You can refer to the lecture notes on CNNs [./documents/Lec-9-CNN.pdf] for more information on the role of activation functions in CNNs. 

To cite this source, you can use: 
(output_context_reference('./documents/Lec-9-CNN.pdf')) 

Here is an example of how to properly cite it in your work: 
As discussed in the lecture notes on CNNs [1], activation functions play a crucial role in introducing non-linearity into the model.

[1] = output_context_reference('./documents/Lec-9-CNN.pdf')","The activation function, also known as the transfer function, determines the output of a neural network. It introduces non-linearity into the model, allowing it to learn more complex patterns. Common activation functions include ReLU, Sigmoid, and Softmax.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU18Answer 9Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: SoftmaxD: Leaky ReLU19Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between overfitting and underfitting in the context of CNNs?,"Underfitting and overfitting are two common problems encountered when training Convolutional Neural Networks (CNNs).

**Underfitting** occurs when a model is **not complex enough** or hasn't learned sufficiently from the training data. As a result, it performs poorly on both the training and test datasets. In other words, the model is **not learning enough**.

**Overfitting**, on the other hand, happens when a model is **too complex** and learns the training data too well, including its noise and outliers. This results in excellent performance on the training dataset but poor performance on new, unseen data (test dataset). Here, the model is **learning too much** from the training data.

To summarize:

- Underfitting: Model is not learning enough (too simple or insufficient training).
- Overfitting: Model is learning too much (too complex or over-trained on the data).

Some strategies to mitigate these issues include data augmentation, regularization techniques (like dropout), and early stopping for overfitting, and increasing model complexity or training time for underfitting [./documents/Practicals with Solutions - CNN.pdf].

The correct definitions are reflected in option **B**: Underfitting: model is not learning enough; Overfitting: model is learning too much.

Source: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of Big Data, 8, pp.1-74.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Practicals with Solutions - CNN.pdf"" target=""_blank"">Practicals with Solutions   CNN</a>

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

","Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on new, unseen data. Underfitting happens when the model is not complex enough to capture the underlying patterns in the data and performs poorly on both training and test data.","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).","futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much11Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above12Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A. : tA | : [Training Error Complexity Zoumana K.Infographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnnsCOMP 2032 CNN7 Strategies to Mitigate OverFitting in CNNs Batch in Dropout normalization Pooling ayers ; a : oie % | Satta we ie. ae! wale | Noise Lt and La stopping injection — normalizations os Data augmentation —16University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAApplicationsFamous CNN Choose  LeNet  PDF  VGG  PDF  AlexNet  ResNet  GoogLeNet  MobileNet  R-CNN  Fast R-CNN  PDF  Faster R-CNN  PDF  PDF  PDF  PDF  PDF  PDF  Datagen (n.d.). Convolutional Neural Network:   Benefits, Types and Applications.   https://datagen.tech/guides/computer-vision/cnn-  convolutional-neural-network/ COMP 2032CNN18Some Practical Applications of CNNImage   Understanding   Classification  Climate  Medical Imaging  Face Recognition  Synthetic Data   Applications  Analysing   Generation  Documents  Advertising  Object Detection  Audio Processing Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).",
What are some common CNN architectures?,"Some common CNN architectures include:

1. **R-CNN (Region-based Convolutional Neural Networks)**: This architecture is used for object detection tasks. It first generates region proposals and then applies a CNN to classify and refine the proposals.

2. **Fast R-CNN**: An improvement over R-CNN, Fast R-CNN is also used for object detection. It improves the speed of R-CNN by sharing the computation for the entire image, rather than computing features for each region proposal separately.

3. **Faster R-CNN**: Faster R-CNN further improves upon Fast R-CNN by introducing a Region Proposal Network (RPN) that generates region proposals. This significantly speeds up the object detection process.

These architectures are all examples of CNNs and are used for various tasks, including object detection and image classification.

According to the provided context, the correct answer to the question ""Which of the following are CNNs?"" is **D: All the above**, as R-CNN, Fast R-CNN, and Faster R-CNN are all CNN architectures.

Source: Alzubaidi et al. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of Big Data, 8, pp.1-74.","Some well-known CNN architectures include LeNet, AlexNet, VGG, ResNet, GoogLeNet, and MobileNet. There are also region-based CNNs (R-CNN, Fast R-CNN, Faster R-CNN) used for object detection.","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:","of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above23Question 12Are the following CNN frameworks?A: True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns24Answer 12Are the following CNN frameworks?A: TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs).|concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following Activation Functions are commonly used in the output layer?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU10Question 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are|image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer10Answer 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much21Question 11Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following are CNNs?A: R-CNN B: Fast R-CNN C: Faster R-CNN D: All the above22Answer 11Extracted from:",
What is the watershed algorithm for segmentation?,"The Watershed algorithm is a segmentation technique used in image processing. It's an edge-based approach that views the image as a topographic map, where the intensity of each pixel represents the height.

Here's a simplified explanation of how it works:

1. **Sorting pixels by intensity**: The algorithm starts by sorting the pixels in the image by their intensity values.
2. **Flooding the landscape**: The algorithm then ""floods"" the landscape from the local minima (i.e., the lowest points). As the water rises, the different ""catchment basins"" (regions) are formed.
3. **Creating watersheds**: The lines that separate these catchment basins are called watersheds. These watersheds represent the boundaries between different regions in the image.

To illustrate this, let's consider an example from the provided context. Suppose we have an image fragment with intensity values:

3  4  2 2  2  3  3
1  1  2  3 2  1
1  2 3

The Watershed algorithm would sort these pixels by intensity and then label them accordingly. A sample solution is provided in the context, where the pixels are labeled as follows:

* The pixels with the lowest intensity (1) are labeled first.
* As the algorithm progresses, the neighboring pixels are labeled based on their intensity and connectivity.

The resulting segmentation is represented using letters of the alphabet to label regions.

The Watershed algorithm is useful for segmenting images into distinct regions, but it can be sensitive to noise and may produce over-segmentation. To address this, techniques like pre-processing the image or using more advanced variants of the algorithm can be employed.

For more information on the Watershed algorithm and its applications, you can refer to the provided context, specifically the document './documents/Practicals with Solutions - Segmentation.pdf' [1].

References:
[1] './documents/Practicals with Solutions - Segmentation.pdf'","The watershed algorithm is a region-based segmentation method. It treats the gradient magnitude image as a topographic surface, flooding basins from local minima until watershed lines (boundaries) are formed.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eﬃcient level-set based geometric ﬂow. The geometric ﬂow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the ﬁve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are – views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I°v0 “ACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -°boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|[7] and motion segmentation [16].We include two other segmentation methods in the category of gradient as- cent algorithms: Watersheds [17] (WS91) and Turbopixels [12] (TP09). General watershed algorithms perform gradient ascent from local minima in the image plane in order to obtain watersheds, i.e. lines that separate catchment basins. Vincent and Soille [17] propose a fast version based on queuing of pixels. Lazy Snapping [3] applies graph cuts to the graph built on the superpixels output by this algorithm.TP09 generates superpixels by progressively dilating a given number of seeds in the image plane, using computationally eﬃcient level-set based geometric ﬂow. The geometric ﬂow relies on local image gradients, and aims to distribute superpixels evenly on image plane. Unlike WS91, superpixels from TP09 are con- strained to have uniform size, compactness, and adherence to object boundaries.3 SLIC segmentation algorithmOur approach generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This is done in the ﬁve-dimensional [labxy] space, where [lab] is the pixel color vector in CIELAB color space, which is widely considered as|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions SegmentationSegmentation ApproachesSeeks groups of similar pixels, with no regard for where they are – views images as uncorrelated dataClustering' |!! 1 i;-  Focus on finding   physically connected   sets of pixels  - E.g., region growing,   split and merge  Region-based 1-.! |,~- I°v0 “ACK: Prof. Tony Pridmore, UNUKRECAP- Emphasise the--! by -°boundaries betweenregions- E.g., watershedsl- i:Edge-based2,I I,Watershed ExerciseBased on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)3  4  2 2  2  3  3 1  1  2  3 2  11  2 3 Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK3Watershed SolutionSeveral solutions exist, based on exactly how the pixels are ordered. So below is a sample solution:1. Sort the pixels (using x, y coordinates) as in the lecture slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!",
What is the purpose of a structuring element in morphological operations?,"## Purpose of a Structuring Element in Morphological Operations

A structuring element is a crucial component in morphological operations, which are used to process binary images.

### Definition and Role

According to the provided context from the lecture notes on Morphology (`./documents/Lec-5-Morphology.pdf`), a structuring element is defined as a binary mask that identifies pixels rather than weighting them, similar to filter masks used in convolution processes.

### Key Aspects

1. **Shape and Size**: Structuring elements vary in shape and size, influencing the outcome of morphological operations. Common shapes include elliptic, rectangular, and cross-shaped elements.
2. **Effect on Operations**: The size and shape of the structuring element determine the extent and direction of the morphological operation's effect. Larger structuring elements produce more extreme effects, while smaller elements can achieve similar effects through repeated operations.
3. **Origin and Symmetry**: The origin of the structuring element does not necessarily need to be at its center, and it need not be square.

### Practical Application

In the context of morphological operations like dilation, the structuring element is used to expand the foreground of a binary image. The choice of structuring element affects how the boundary of the foreground is expanded, thereby influencing the outcome of the operation.

For example, in Laboratory Sheet 5: Morphology (`./documents/Lab 5.pdf`), you can create structuring elements using `getStructuringElement` and observe how different shapes and sizes impact the dilation operation on an image like ""Square-Hole.jpg"".

### Summary

In summary, the purpose of a structuring element in morphological operations is to define the shape and extent of the operation's effect on a binary image. By choosing appropriate structuring elements, you can control the outcome of morphological operations such as dilation.

Output_context_reference: ['./documents/Lec-5-Morphology.pdf', './documents/Lab 5.pdf']",A structuring element is a small kernel used in morphological operations like erosion and dilation. It defines the shape of the neighborhood of a pixel that is considered when applying the operation.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony",
What is the difference between opening and closing in morphological image processing?,"Opening and closing are two fundamental morphological operations used in image processing.

### Opening Operation
The opening operation is defined as an erosion operation followed by a dilation operation using the same structuring element. It is used to:

* Remove noise or small objects from an image
* Separate touching objects
* Smooth the contours of objects

Mathematically, opening is represented as: `A ∘ B = (A ⊖ B) ⊕ B`, where `A` is the input image, `B` is the structuring element, `⊖` represents erosion, and `⊕` represents dilation.

### Closing Operation
The closing operation is defined as a dilation operation followed by an erosion operation using the same structuring element. It is used to:

* Fill small holes or gaps in objects
* Connect nearby objects
* Smooth the contours of objects

Mathematically, closing is represented as: `A ∙ B = (A ⊕ B) ⊖ B`, where `A` is the input image, `B` is the structuring element, `⊕` represents dilation, and `⊖` represents erosion.

### Key differences
The key differences between opening and closing are:

1. **Order of operations**: Opening involves erosion followed by dilation, while closing involves dilation followed by erosion.
2. **Effect on objects**: Opening tends to remove small objects or noise, while closing tends to fill small holes or gaps in objects.
3. **Application**: Opening is typically used for removing noise or separating touching objects, while closing is used for filling holes or connecting nearby objects.

To illustrate this, consider a binary image with small objects or noise. Applying an opening operation would remove these small objects, while applying a closing operation would fill small holes in larger objects.

You can refer to the provided context in `./documents/Practicals - Morphology.pdf` for more information on morphological operations.","Opening is an erosion followed by a dilation, which can remove small objects and smooth object contours. Closing is a dilation followed by an erosion, which can fill small holes and connect nearby objects.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5What size should the structuring element be?*pendent high- requires that, first, enough representatt|parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated goals. A feature gj ad through the processing: a time. For each subset, the feature of interest apped into a point in parameter. After theACK: Prof. Tony Pridmore, UNUKparacetr. After the nough boton proce: rameters oo corn of brages Ws a p. we goes. A feaoure » 1 thecne? the proewss: + teew flor each cut Ete toature ‘ ioterest ped nto a ovim in4irIs morphological a possible SOLUTION? Can we get rid of the text in the bottom left corner?*pendent high- requires that, first, enough representatt ‘action process be con rramelers. corresponding Edomaimn is of images is a paradigm alec) sraals. A feature space is vd Through the peocessing of a hme. Pour cach subset, a the fealiire of agrestis lapped inte a port the Piranieler. After the enireHow can we use morphological operators to remove the text?A: erode white C: close whiteB: dilate whiteD: open whiteHow do you decide what size should the structuring element should be?ACK: Prof. Tony Pridmore, UNUKNviivesINages5Is morphological a possible SOLUTION?Can we get rid of the text in the bottom left",
How can color histograms be used for image retrieval?,"Color histograms are a powerful tool for image retrieval, as they provide a compact representation of the color distribution within an image. According to the provided context, color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object (Swain1.pdf). This makes them an excellent representation to use for identifying objects.

To use color histograms for image retrieval, you can follow these steps:

1. **Compute the color histogram**: Calculate the color histogram of the query image and store it as a feature vector.
2. **Compare histograms**: Compare the query image histogram with the histograms of images in the database using a suitable distance metric, such as Histogram Intersection (Swain1.pdf).
3. **Rank images**: Rank the images in the database based on their similarity to the query image, as measured by the distance between their histograms.

Histogram Intersection is a technique that is especially suited to comparing histograms for recognition because it does not require the accurate separation of the object from its background or occluding objects in the foreground. This makes it robust to occlusion and changes in image resolution.

By using color histograms and Histogram Intersection, you can efficiently retrieve images from a large database that are similar to a given query image.

Output_context_reference: ['./documents/Swain1.pdf']","Color histograms can be used to represent the color distribution of an image. By comparing the color histogram of a query image with the histograms of images in a database using a similarity measure like histogram intersection, similar images can be retrieved.","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is","the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||problems arepresented and tested.Color Indexing 13Table I. The biological organization of cortex into What/Where modules may have a basis in computational complexity. Trying to match a large number of image segments to a large number of models at once may be too difficult. (From Ballard (1991)).Object to Match Against One Many Tmage One Identification: trying to iden- Portions tify an object whose location can be fixated Many | Location: trying to find a| Image interpretation: Too known object hard?1.3 OutlineSection 2 introduces the multidimensional color histo- gram. Given a discrete color space, a color histogram counts how much of each color occurs in the image. Color histograms are invariant to translation, rotation about an axis perpendicular to the image, and change only slowly with rotation about other axes, occlusion, and change of distance to the object. On the other hand, histograms for different objects can differ markedly, and there are a huge number of possible histograms (ex- ponential in the number of different colors in the color space). Therefore, the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is",
What is histogram intersection?,"Histogram intersection is a method used to compare two histograms, typically used in image processing for object recognition. It measures the similarity between two histograms by calculating the overlap between them.

Given two histograms, J and M, each containing n bins, the histogram intersection is defined as the sum of the minimum values between corresponding bins in the two histograms. Mathematically, it is represented as:

H(J, M) = ∑ min(Ji, Mi) from i=1 to n

To obtain a fractional match value between 0 and 1, the intersection is normalized by the number of pixels in the model histogram (M). The normalized histogram intersection match value is calculated as:

H(J, M) = (∑ min(Ji, Mi) from i=1 to n) 

This method is robust to distractions in the background, occlusion, and varying image resolution, making it suitable for object recognition tasks.

For example, if we have two histograms H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0) and H2 = (0, 40, 0, 0, 0, 6, 0, 110, 0), the histogram intersection would be calculated as:

HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0 + 0 = 6

This indicates the amount of overlap between the two histograms.

Histogram intersection has been shown to be effective in object recognition tasks, with a recognition rate of almost 100% in a database of 66 color histograms and 732 query images (Ballard and Swain, 1991) [./documents/Lec-2-Histogram (Whole Image Understanding).pdf].

You can refer to the source [./documents/Lec-2-Histogram (Whole Image Understanding).pdf] and [./documents/Swain1.pdf] for more information on histogram intersection.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram intersection is a method for comparing two histograms. It calculates the sum of the minimum values for each corresponding bin in the two histograms. This measure is robust to occlusion and changes in image resolution.,"Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‘(and ‘-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  å  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R – G - BY = 2 * B – R – G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly— distractions in the background of the object,— viewing the object from a variety of viewpoints,— occlusion,— varying image resolution,— varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to","Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Intersection-- ‘(and ‘-- Measure how much of the query may be present in the target image(and vice-versa)- A bin in the target histogram can have a larger value than thecorresponding query bin (and vice-versa)H1 = (10, 0, 0, 0, 100, 10, 30, 0, 0)HI(H1, H2)  =   n  å  min(  1  i HH  ,  2  i  )  i  =  1 H2 = ( 0, 40, 0, 0, 0, 6, 0, 110, 0)HI(H1, H2) = 0 + 0 + 0 + 0 + 0 + 6 + 0 + 0= 6How much do two representation overlaps? ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)|!I41irHistogram IntersectionIn the first histogram intersection paper (Ballard and Swain, 1991):- A database of 66 colour histogram.--732 query imagesRecognition   rate was   almost 100% ACK: Prof. Tony Pridmore, UNUK?COMP 2032Histogram (Whole Image Understanding)42irHistogram IntersectionBallard and Swain used opponent colour axes RG = R – G - BY = 2 * B – R – G - - WB = R + G + B (intensity) & matched images under a range of conditions Normal condition - Varying in view - Varying in image resolutions - - Occlusion (of bottom 1/3 and/or side 1/3 of image) Varying in bin resolutions - - Varying in light intensitySoe . conditions - . ' . ! image) I iACK: Prof. Tony Pridmore, UNUKCOMP|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|the color histogram is an excellent representation to use for identifying objects.Section 3 introduces a method of comparing image and model histograms called Histogram Intersection, which is especially suited to comparing histograms for recognition because it does not require the accurate sep- aration of the object from its background or occluding objects in the foreground. Experiments show that Histo- gram Intersection can distinguish models from a large database, that it is robust to occlusion as well as image and histogram resolutuion, and that only a small number of histograms are needed to represent a three- dimensional object. They also show that an effective color-constancy algorithm will be needed for Histogram Intersection to work under variable light conditions. The section also describes a modification of Histogram Intersection called Incremental Intersection that allows efficient indexing into a very large database.Section 4 shows how a model histogram can be used. to find the location of a known object (the target). The algorithm to solve this problem is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up|extremely insensi- tive to the histogram resolution uscd.Section 3.2 describes an incremental version of Histogram Intersection, called Incremental Intersection. By matching the largest bins from the image and the models, Incremental Intersection allows extremely fast indexing into a large database. Experiments show that Incremental Intersection does not sacrifice accuracy because most of the information is carried by the largest bins of the histograms.3.1 Histogram IntersectionBecause the model database may be large, we can afford only a highly restricted amount of processing per model, but at the same time we must be able to overcome the problems that hinder recognition, most importantly— distractions in the background of the object,— viewing the object from a variety of viewpoints,— occlusion,— varying image resolution,— varying lighting conditions.The matching method proposed here, called Histogram Intersection, is robust to the first four problems; the last is left to a color constancy module that operates on the input prior to the histogram stage.3.1.1. Description. Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to",
What are the different types of redundancy in image compression?,"According to the lecture notes on Image Compression from the University of Nottingham, there are three main types of redundancy in image compression: 

1. **Coding Redundancy**: This occurs when the number of bits used to represent each pixel value is not optimal. For example, if a natural binary code is used to represent grey levels, all pixels take the same number of bits, regardless of their frequency of occurrence. 
2. **Spatial Redundancy**: Although not explicitly defined in the provided context, spatial redundancy refers to the fact that neighboring pixels in an image often have similar values. 
3. **Psychovisual Redundancy**: This type of redundancy is related to the way humans perceive visual information. Certain details in an image may not be perceived by the human eye, and thus can be removed or compressed without significantly affecting the image quality.

These types of redundancy are discussed in the context of image compression techniques such as Huffman coding, GIF, and JPEG.

You can refer to the lecture notes for more information on these topics, specifically in ['./documents/Lec-10-Image Compression.pdf'] and ['./documents/Practicals with Solutions - Image Compression.pdf'].","There are three main types of redundancy in image compression: coding redundancy, spatial redundancy, and psychovisual redundancy.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=— =0,2,...,L—1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1å() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 10 Image CompressionomirLearning OutcomesIDENTIFY  1. Redundancy  2. Huffman Coding  3. Psychovisual Redundancy - GIF  4. A Compression System - JPEG COMP 2032Image Compression2University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIARedundancy0Image CompressionCan be large  Individual image(s)  Are easy to acquire,   collections increases   rapidly  In some applications, images are gathered   automatically  Luckily, image data is redundant in several ways  - Coding redundancy  - Spatial redundancy  - Psychovisual redundancy ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression4irCoding Redundancy|The grey level histogram of an image gives the probability (frequency)of occurrence of grey level rkna ee p(r, )=— =0,2,...,L—1 eee eee eee |If the number of bits used to represent each value of rk is l(rk), theaverage number of bits required to represent a pixel isLavg=L-1å() (rprlkk)k=0To code an MxN image requires MNLavg bitsACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression5irCoding RedundancyIf an m-bit natural binary code is used to represent grey level then- All pixels take|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1",
What is Huffman coding?,"Huffman coding is a lossless data compression technique used in image processing. It's a variable-length prefix code that assigns shorter codes to more frequently occurring pixel values in an image.

Here's a simplified overview of how it works:

1. **Sort pixel values by frequency**: Arrange the pixel values in order of their frequency or probability of occurrence.
2. **Build a binary tree**: Create a binary tree by combining the two least frequent pixel values and assigning a 0 and 1 to the branches.
3. **Assign codes**: Traverse the tree from the root to each leaf node, assigning a code to each pixel value based on the path taken (0 or 1).

The resulting Huffman code has the following properties:

* **Variable-length codes**: More frequent pixel values have shorter codes, while less frequent ones have longer codes.
* **Prefix-free**: No code is a prefix of another code, ensuring that the codes can be decoded unambiguously.

Huffman coding is useful for compressing images because it:

* Reduces the average code length
* Is a lossless compression technique, preserving the original image data

For example, consider an image with the following normalized histogram:

| Pixel Value | Normalized Frequency |
| --- | --- |
| 0 | 0.1 |
| 1 | 0.1 |
| 2 | 0.15 |
| 3 | 0.35 |
| 4 | 0.2 |
| 5 | 0 |
| 6 | 0.05 |
| 7 | 0.05 |

To derive a Huffman code for this image, you would sort the pixel values by frequency, build a binary tree, and assign codes accordingly.

You can refer to the provided lecture notes and practical exercises for more details on Huffman coding and its application in image compression [('./documents/Practicals with Solutions - Image Compression.pdf', './documents/Practicals - Image Compression.pdf')].","Huffman coding is a lossless data compression algorithm. It assigns variable-length codes to input characters, with the lengths of the assigned codes being based on the frequencies of corresponding characters. More frequent characters get shorter codes.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions Image CompressionImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3Huffman Code – Let’s BREAK it DOWN…Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05|:SortPixel Value 3 4 2 1 0 7 6 5 Normalised Frequency 0.35 0.2 0.15 0.1 0.1 0.05 0.05 0ACK: Prof. Tony Pridmore, UNUK4ull aHuffman Code – Let’s BREAK it DOWN…Original Source Source Reduction Pixel Value Normalised Frequency 1 2 3 4 3 0.35 0.35 0.35 0.35 0.35 4 0.2 0.2 0.2 0.2 0.25 2 0.15 0.15 0.15 0.2 0.2 1|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
How does JPEG compression work?,"JPEG compression is a widely used method for compressing images. Here's a step-by-step explanation of how it works:

1. **Discrete Cosine Transform (DCT)**: The image is divided into 8x8 blocks, and the DCT is applied to each block. This transforms the spatial domain image into the frequency domain.

2. **Quantization**: The DCT coefficients are then quantized, which reduces the precision of the coefficients. This is where the lossy compression occurs, as some of the data is discarded.

3. **Zigzag Coding**: The quantized DCT coefficients are rearranged in a zigzag pattern, which orders the coefficients from low to high frequency.

4. **Run-Length Encoding (RLE)**: The zigzag-coded coefficients are then encoded using RLE, which replaces sequences of zeros with a single value and a count of the number of zeros.

5. **Huffman Coding**: The RLE-encoded coefficients are then encoded using Huffman coding, which assigns variable-length codes to the coefficients based on their frequency.

The JPEG compression process takes advantage of the fact that the human eye is less sensitive to high-frequency information, so the quantization step can discard some of this information without significantly affecting the perceived image quality.

Increasing the amount of quantization reduces the file size but introduces artefacts, such as visible blocks.

As mentioned in the lecture notes ['./documents/Lec-10-Image Compression.pdf'], ""Increasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visible.""

This is a summary of the JPEG compression process. If you'd like more information or clarification on any of these steps, feel free to ask.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-10-Image Compression.pdf"" target=""_blank"">Lec 10 Image Compression</a>

","JPEG compression is a lossy compression method that involves dividing an image into blocks, applying a Discrete Cosine Transform (DCT) to each block, quantizing the DCT coefficients, and then using Huffman coding to compress the quantized coefficients.","Compression—26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 —3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.—.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -— . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf” te . = om «File size: } 49K es 100 dpi high JPEG compression ~ — ye ‘ ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compression🤔48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well…the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE…CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing – theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f’(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = æ ç ç è 1 MN M - 1 N - 1 åå x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 ö ÷ ÷ ø 2/1 SNR = M - 1 N - 1 åå ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - åå ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" 𝑇 𝑟 = ( 𝑝"" 𝑤 𝑑𝑤 !𝑑𝑠 𝑑𝑟 = 𝑑𝑇(𝑟) 𝑑𝑟 = 𝑑 𝑑𝑟 "" ( ! 𝑝"" 𝑤 𝑑𝑤 = 𝑝""(𝑟)𝑝# 𝑠 = 𝑝"" 𝑟 𝑑𝑟 𝑑𝑠 = 𝑝"" 𝑟 1 𝑝"" 𝑟 = 1 0 ≤ 𝑠 ≤ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram","Compression—26 -3 -6 2 2 -1 00 ; 0-2-4 1 1 000 —3 1 5-1 -1 000 I 3 1 29-10 000 7 7 . 1 0 0 0 0 000 0 0 0 0 8 9900 0 0 0 0 0 000 II45JPEG CompressionZigzag coding orders elements of quantised DCT block (roughly) on frequencyDC AC), ACo7 yy, Zo So.ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Image Compressionwire eee eee - Ordering produces long sequences of Zeros ! - Sequences of AC values can be ' represented using: euniteens cin -. L.—.--.-.+--.-. u I fe Uuiiieincistelire|-26 -3 0 -3 -2-6 2-41 -3....I46JPEG CompressionIncreasing the amount of quantisation reduces filesize but introduces artefacts: blocks become visibleer 100 dpi low JPEG compression ~ -— . . yey ow o= File size: } 248K Se 100 dpi medium JPEG compression gf” te . = om «File size: } 49K es 100 dpi high JPEG compression ~ — ye ‘ ia om = File size:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression47Summary(1. Redundancy 12. Huffman Coding (3. Psychovisual Redundancy - GIF 14. ACompression System - JPEGCOMP 2032Image Compression🤔48University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Finale and Revision|- Region growing, split and merge (quadtrees), watersheds  Interactive Segmentation  - Superpixels  - Live-wire  - Graph & Grabcut  - Blending (Distance transform)  - Geometric Transformation (well…the basics, at least)  A set of tools that can be used to create image   processing pipelines ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision13A walk down MEMORY LANE…CNN  - Convolutional, pooling, activation function, fully connected and output layers  - Utilises Image Processing techniques to extract features on which the   underlining neural network learns and perform classification  Compression  -  Increasingly important  - Types of redundancy: coding, spatial, psychovisual  - Structure of compression systems  - Components and complete schemes: Huffman coding, GIF, JPEG ACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and RevisionI ;14University of Nottingham , UK | CHINA | MALAYSIAEndgameModule Aims To introduce the fundamentals of digital image processing – theory & practice Assessed by in-lab test: how do techniques work, and what do they doTo gain practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for|are imperceptible; goal is to compress without noticeable change to the imageA simple method: Add a small random number to each pixel before quantizationACK: Prof. Tony Pridmore, UNUK256 grey levels16 grey levels16 grey levelsCOMP 2032Image Compression10irEvaluating Compression- Fidelity Criteria: success is judged by comparing original and|- , '- |compressed version- Some measures are objective, e.g., root mean square error (erms) and signal to noise ratio (SNR)- Let f(x,y) be the input image, f’(x,y) be reconstructed input image'- |from compressed bit stream, thenmee eee eee eee eee ee |e rms = æ ç ç è 1 MN M - 1 N - 1 åå x = 0 y = 0 ( ( yxf ' , ) - ( yxf , ) ) 2 ö ÷ ÷ ø 2/1 SNR = M - 1 N - 1 åå ( ( yxf ' , ) ) 2 x = 0 y = 0 M 1 N 1 - - åå ( ( yxf ' , ) - ( yxf , ) ) 2 x = 0 y = 0ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image CompressionII11fFidelity CriteriaC,ms = 6.93Ems = 6.78SNRim = 10.25 || SNR = 10.39ACK: Prof. Tony Pridmore, UNUKCOMP 2032Image Compression12Fidelity Criteria- Erms and SNR are convenient objective measures - Most decompressed images are viewed by human beings - Subjective evaluation of compressed image quality by human observers are often more|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Image CompressionbisImage Compression• Types of redundancy: coding, spatial, psychovisual• Structure of compression systems• Components and complete schemes: Huffman coding, GIF, JPEGACK: Prof. Tony Pridmore, UNUK?Why?• Compression affects image quality• Need to understand/choose methods/parameters• New methods are based on core concepts20 aHuffman Code – For you to try NOW!An image has the following normalised histogram. Derive a Huffman code for each pixel value, showing how you obtained your code.Pixel Value Normalised Frequency 0 0.1 1 0.1 2 0.15 3 0.35 4 0.2 5 0 6 0.05 7 0.05ACK: Prof. Tony Pridmore, UNUKMake sure to view the video FIRST!!!3University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of the image representation. For a 3-bit (8 value) image is done by dividing the CDF value by 1/(N-1) (where N is the number of Input Pixel Value rows) and rounding the result to a single integer value. Alternatively, you may also multiply the CDF value with (N-1) directly. The output histogram is obtained by considering each possible image value and working back through the mapping to see which bins in the input histogram map to that output bin. The output normalised frequency at a given bin is the sum of the input normalised frequencies of values that map to that bin.ACK: Prof. Tony Pridmore, UNUK6How does this work?P3(s) = P,(r). drSo P,(s) = P(r). =P,(s).ds r Pals) P,(r).drACK: Prof. Tony Pridmore, UNUK7ir aHow does this work?If we choose as the transformation function the cumulative distribution function or CDF:"" 𝑇 𝑟 = ( 𝑝"" 𝑤 𝑑𝑤 !𝑑𝑠 𝑑𝑟 = 𝑑𝑇(𝑟) 𝑑𝑟 = 𝑑 𝑑𝑟 "" ( ! 𝑝"" 𝑤 𝑑𝑤 = 𝑝""(𝑟)𝑝# 𝑠 = 𝑝"" 𝑟 𝑑𝑟 𝑑𝑠 = 𝑝"" 𝑟 1 𝑝"" 𝑟 = 1 0 ≤ 𝑠 ≤ 1P,(s).ds Ps "" P(r) Pi(r).dr drT(r) depends on pr(r), but the resulting ps(s) is always uniform.ACK: Prof. Tony Pridmore, UNUK8irA Histogram",
What is the difference between RGB and HSV color spaces?,"The main difference between RGB and HSV color spaces is how they represent colors.

RGB (Red, Green, Blue) is an additive color model where colors are created by mixing different intensities of red, green, and blue light. It's commonly used in digital displays.

HSV (Hue, Saturation, Value), on the other hand, is a color model that separates the color information into three components:
- **Hue**: represents the actual color (e.g., red, blue, etc.)
- **Saturation**: represents the purity or intensity of the color
- **Value**: represents the brightness or lightness of the color

The key differences between RGB and HSV are:
1. **Color representation**: RGB represents colors as a combination of red, green, and blue intensities, while HSV represents colors based on their hue, saturation, and value.
2. **Separation of color and intensity**: HSV separates color (hue) from intensity (value), making it less sensitive to illumination changes. In contrast, RGB combines color and intensity information.
3. **Application**: HSV is often used in image processing and computer vision applications where color-based segmentation or object recognition is required, as it's more intuitive to work with colors in terms of hue, saturation, and value.

According to the lecture notes from Prof. Tony Pridmore, UNUK COMP 2032 Digital Images and Point Processes, HSV is ""based on colour rather than light"" and ""separates colour from intensity (value) making it less sensitive to illumination changes"" [./documents/Lec-1B-Digital Images and Point Processes.pdf].","RGB (Red, Green, Blue) is an additive color model based on light, commonly used in displays. HSV (Hue, Saturation, Value) is a color model based on human perception of color. Hue represents the color, saturation represents the intensity of the color, and value represents the brightness. HSV separates color information from intensity, making it less sensitive to illumination changes.","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie• RGB is OK, but there are other general &application-specific colour spaces…•If you want to work with plants, you might use|just G, or ‘greenness’:mGG – (R+B)/2HSV is based on colour rather   than light  • Hue - what general colour is it  • Saturation – how strongly coloured is it  • Value – how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject’s surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255  R GRGB is additive, other colours aremade by mixing these together} _!22irAlternative Colour Spacesr | ie• RGB is OK, but there are other general &application-specific colour spaces…•If you want to work with plants, you might use|just G, or ‘greenness’:mGG – (R+B)/2HSV is based on colour rather   than light  • Hue - what general colour is it  • Saturation – how strongly coloured is it  • Value – how bright or dark is it ||-IACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes23HSV SpaceValue indicates how dark light the colour isis Saturation indicates howstrong the colour isACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesHue indicates the angle around the colour wheel24HSV Space HSV separates colour from intensity (value) making it less sensitive to illumination changesWhat makes it DIFFERENT to RGB Space?Colour ImageHueSaturationValueACK: Prof. Tony Pridmore, UNUK(presented as a colour image)COMP 2032Digital Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Another14. Swain and Ballardproblem is that the equivalence is all or nothing. Presumably, as the difference in color of two object patches increases the probability of them being the same object patch decreases smoothly. The binary threshold used to define the tesselation serves as a crude approx- imation to the probability function.Histograms whose bins define overlapping bell- shaped (e.g. , Gaussian) functions of color space would address some of the concerns of the previous paragraph, as would interpolation coding (Ballard 1987). Exten- sions such as these have not been considered because histograms in their simplest form have worked well. Why do histograms works, despite their inherent prob- lems? Objects tend to have surfaces that are made up of regions of color. Because of shading and camera noise these regions are blurred in color space, and so span more than one bin in a historgram. When image and model histograms of the same object are matched, a high match value is obtained because the regions match well, even if point-by-point matches on the ob- ject’s surface are not always reliable.Strat (1990) has matched cumulative histograms with a match algorithm similar to|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is gamma correction?,"Gamma correction is a non-linear transformation used to adjust the brightness and contrast of an image. It's a point process that operates on each pixel independently.

The formula for gamma correction is given by: $O = \left(\frac{I}{255}\right)^\gamma \times 255$, where $I$ is the input intensity, $O$ is the output intensity, and $\gamma$ is the gamma value.

The gamma value determines the shape of the curve that maps the input intensity to the output intensity. If $\gamma$ is:

* Less than 1, the mapping is weighted towards brighter output values.
* Greater than 1, the mapping is weighted towards darker output values.
* Equal to 1, the mapping is linear, and there is no correction.

In essence, gamma correction helps to compensate for the non-linear response of display devices, such as monitors, to the input voltage. By applying gamma correction, we can ensure that the image displayed on the screen is perceived correctly by the human eye.

For example, if a display device has a gamma of 2.5, we can apply a gamma correction with $\gamma = \frac{1}{2.5}$ to the image before displaying it, so that the final output is linear.

You can refer to the lecture notes './documents/Lec-1B-Digital Images and Point Processes.pdf' for more information on gamma correction and its applications in image processing [./documents/Lec-1B-Digital Images and Point Processes.pdf].",Gamma correction is a non-linear operation used to adjust the brightness and contrast of an image. It is often used to compensate for the non-linear response of display devices.,"a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾 is the gamma value.𝛾𝑂 = 𝐼 255 × 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if’ University of Nottingham —~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values 𝐼 to create 𝑂 (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:● Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.● Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.• When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform• You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think°.RAMP=25L ≈ V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction• We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each","a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾 is the gamma value.𝛾𝑂 = 𝐼 255 × 255 (1)Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 5if’ University of Nottingham —~ UK | CHINA | MALAYSIASchool of Computer ScienceThe gamma value specifies the shape of the curve that maps the intensity values 𝐼 to create 𝑂 (see Figure 1). If gamma is less than 1, the mapping is weighted towards brighter output values. If gamma is greater than 1, the mapping is weighted towards darker output values. If gamma is equal to 1, the mapping is linear, and there is no correction.Gamma <1 Gamma = 1 Gamma > 1 Output Intensity Output intensity 200 250 0 50 100 150 200 250 0 50. 100 Input Intensity 150 200 250 Input Intensity 100 150 Input intensityFigure 1Using the cameraman.tif image from the Moodle page, perform the following:● Using (1), apply gamma correction to the image with gamma values 0.5, 1 and 1.5 and display the results.● Negate the image and display the results.Gain and bias adjustments are used to control the brightness of an image. Gain adjustment multiplies the pixel values by a scale factor, whereas bias adjustment adds|g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|preserved  High intensities selected, others reduced to low levelSelected intensities set to black, others (blood vessels & kidneys) preservedAn aorticangiogramACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes39irGamma Correction.• When an image is displayed on a screen, the hardware used effectivelyapplies an intensity transform• You send a voltage proportional to the intensity of a pixel, the screen displays an intensity that is related to that, but not how you may think°.RAMP=25L ≈ V2.5 depending on deviceACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI I II40irirGamma Correction• We need to transform the image, so that it generates a voltage which will   display what we want  Create a new image in which  g(x,y) = f(x,y)1/2.5  then  V = g(x,y)  L = (f(x,y)1/2.5)2.5 = f(x,y)  Gamma Correction  Because the equation is usually written  g(x,y) = f(x,y)y |ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes4100Key Points Point processes operate on   each pixel independently  Linear processes change the   appearance of the whole   image  REMEMBER  Non-linear processes can differentiate   object/image regions ACK:|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each",
What is anisotropic diffusion?,"Anisotropic diffusion is a technique used in image processing to smooth an image while preserving its edges. It's an extension of the traditional diffusion process, which is equivalent to mean and Gaussian filtering.

The basic idea behind anisotropic diffusion is to make each pixel more like its neighbors, but only if they are similar. This is achieved by using a similarity function, s(p,q), that weights the contribution of neighboring pixels based on their similarity to the center pixel.

The similarity function s(p,q) has values in the range from 0 to 1, where:
- s(p,q) is close to 1 if pixels p and q are similar
- s(p,q) is close to 0 if pixels p and q are different

The new value at a pixel p is computed as a weighted average of its neighbors, q, using the similarity function s(p,q).

Mathematically, this can be represented as: 
p' = ∑(q * s(p,q)) / ∑s(p,q)

Anisotropic diffusion is related to bilateral filtering, which also uses a similar weighting scheme to smooth images while preserving edges.

You can refer to the lecture notes './documents/Lec-3B-Non-Linear Filters.pdf' for more information on anisotropic diffusion and its implementation.","Anisotropic diffusion is an edge-preserving smoothing technique. Unlike isotropic filters like Gaussian blur that smooth uniformly in all directions, anisotropic diffusion encourages smoothing within regions of similar intensity while inhibiting it across strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is bilateral filtering?,"Bilateral filtering is a non-linear image processing technique used to smooth images while preserving edges. It modifies Gaussian smoothing by incorporating two Gaussian filters: one that weights pixels based on their spatial proximity to the source pixel (space weight) and another that weights pixels based on their intensity similarity to the source pixel (range weight).

The bilateral filter output at a pixel `p` is calculated as a weighted average of neighboring pixels `q`, where the weights depend on both the spatial distance between `p` and `q` and the intensity difference between `p` and `q`. This is represented by the formula:

`BF[I]_p = (1/W_p) * Σ G_s(||p-q||) * G_r(|I_p - I_q|) * I_q`

where `G_s` is the spatial Gaussian filter, `G_r` is the range Gaussian filter, `||p-q||` is the spatial distance between `p` and `q`, `|I_p - I_q|` is the intensity difference between `p` and `q`, and `W_p` is a normalization factor.

Bilateral filtering is effective in reducing noise while preserving edges because it only averages pixels with similar intensities, thus avoiding the blurring of edges that occurs with traditional Gaussian smoothing.

For more information, you can refer to the lecture notes on Non-Linear Filters (`./documents/Lec-3B-Non-Linear Filters.pdf`) and the practical exercises on Linear and Non-Linear Filters (`./documents/Practicals - Linear and Non-Linear Filters.pdf`).","Bilateral filtering is a non-linear, edge-preserving smoothing filter. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels. The weights depend not only on the spatial distance of pixels but also on the difference in their intensity values.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding…Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x—)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ° Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|of Nottingham UK | CHINA | MALAYSIABilateral FilteringaBilateral Filtering- Anisotropic Diffusion is related to mean filtering.- .-If the similarity function is always 1 we get a mean filter-|p'=å ´q~— 8(P.9)s(p,Sums pixel values in a regionås(p,q)—Counts pixel values in a region|-Bilateral filters modify Gaussian smoothing in a similar wayDe ee I...- One Gaussian weights pixels that are near the source- Another Gaussian weights pixels that have similar intensity to the source pixelACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters18olGaussian Smoothing AgainIGB ][ p = å G s ( || qp - q Î S || ) I qnormalizedGaussian function10ACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filters19Bilateral Filteringnot newBF [I], =2,normalization space weight factorACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear Filtersnewrange weightIq20A One Dimensional Example1D image = line of pixelsBetter visualised as a plotpixelintensityACK: Prof. Tony Pridmore, UNUKpixel positionCOMP 2032|Non-Linear Filters21Gaussian & Bilateral Smoothingp  ][  IGB  =  q  p  q q å  ][  IGB  =  p  q  S  Î  1  å  ][  =  p  W  S  q  Î  p  normalization  G  s  G  s  s  (  ||  qp  -  space  (  ||  qp  -  space  |||MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Linear and Non-Linear FiltersTime to TEST your understanding…Local processing on the window Source image Target image2Noise Reduction FiltersSpeckle NoiseMedian FilterAdditive Noise  Mean  Gaussian  Edge   Preserving  ?  Anisotropic   Bilateral   Diffusion  Filtering ACK: Prof. Tony Pridmore, UNUK3oUnderstanding NoiseNoise 1 ~(x—)? e 20 ovV2TT P(x)= aYou are setting up or using an image acquisition system and want to get some idea of the noise that will present in the images you take. Can you do this with:A: a single image B: a series of images C: neither D: bothACK: Prof. Tony Pridmore, UNUK4Convolution1  2  1  0  0  0  -1  -2  -1  10  10  10  11  13  12  20  20  20 What is the result obtained when a 3 x 3 mask (top) containing the above-mentioned weights are applied to the image fragment (below):A: 126ACK: Prof. Tony Pridmore, UNUKB: -40C: 4050Convolution10 10 10 11 13 12 20 20 20When performing a 5 x 5 mean filter, what is the value in each of the mask entries?A: 25B: 5C: 1/25ACK: Prof. Tony Pridmore, UNUK60Convolution10 10 10 11 13 12 20 20 20If you want to apply a|ChandesaPage 2 of 3University of Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceinal Image 100 Sigma = 150 ° Sigma 0) 100Figure 5: Bilateral FilteringPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3",
What is the Canny edge detector?,"The Canny edge detector is an edge detection algorithm developed by John Canny. It's considered an extension of the Sobel edge detection method. The Canny edge detector works by first reducing noise in the image using Gaussian smoothing. Then, it calculates the gradient using Sobel filters in both the x and y directions.

To obtain thin edges, the algorithm performs non-maximum suppression, saving only the local maximums of the gradient in the gradient direction. Finally, hysteresis thresholding is applied to select the edges using two threshold values (min and max). Edges above the max threshold are considered edges, those below the min threshold are non-edges, and edges between the two thresholds are classified based on their connectivity to an edge or non-edge.

The Canny edge detector is designed to satisfy three main criteria:

1. **Good Detection**: It should mark all the edges and only all the edges.
2. **Good Localisation**: The points marked should be as close to the real edge as possible.
3. **Minimal Response**: Each edge should be reported only once.

The optimal detector is approximated by the 1st derivative of a Gaussian, which is used in the algorithm.

For more details, you can refer to the lecture notes on Derivative and Edges [./documents/Lec-6A-Derivative and Edges.pdf].","The Canny edge detector is a multi-stage edge detection algorithm. It involves noise reduction using a Gaussian filter, finding intensity gradients, non-maximum suppression to thin edges, and hysteresis thresholding to connect weak edges to strong ones.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian",
What is unsharp masking?,"Unsharp masking is an image processing technique used to enhance the edges in an image. It works by comparing the original image with a smoothed version of itself. Here's a step-by-step breakdown:

1. Take the original image.
2. Gaussian smooth the original image to create a blurred version.
3. Subtract the smoothed version from the original to create an ""unsharp mask.""
4. Add the unsharp mask to the original image to make the edges appear more obvious.

This process makes edges more noticeable and can even enhance noise. The technique relies on the smoothing effect of a Gaussian function, which introduces a difference between the original and processed images.

Unsharp masking is often used to make images appear sharper, but it can also amplify noise. The choice of Gaussian filter size (σ value) affects the outcome. A larger σ value will result in a more subtle enhancement, while a smaller σ value will produce a more pronounced effect.

For example, consider an image enhanced using unsharp masking with two different Gaussians. The result with the largest σ value (e.g., σ = 7.5) will generally be less sharp than the one with a smaller σ value (e.g., σ = 1.5) [./documents/Lec-6A-Derivative and Edges.pdf].

If you'd like to explore more examples or have specific questions about unsharp masking, feel free to ask.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Unsharp masking is an image sharpening technique. It works by subtracting a blurred version of the image from the original image to create a mask of the image's high-frequency components, which is then added back to the original image to sharpen it.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -———__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -——__, I eee C ooo —_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask — En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make’ ° the edge appear more obvious -—™—_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  •  Even if they are noise  •  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f’Derivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I• Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKA σ = 1.5Bσ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|-2  -1  0  0  0  1  2  1  Gy COMP 2032Derivative and EdgesThese operators are most commonly associated with edge detection12University of Nottingham UK | CHINA | MALAYSIAImage Sharpening Edge Enhancement: Unsharp Masking- -- Edges are important -- Sometimes we want to enhance them without (much) affecting the rest of the image ee allnin 1 -1. Take the original image -———__, Loe eee ee eee te ete tee _"" meee n eee [ooo , -2. Gaussian smooth it -——__, I eee C ooo —_ "" i ee eee mee 1 . 3. Subtract the smoothed version from [ I the original to make an unsharp mask — En _ ee i Pomc crc chan eeeeeen. 1 -4. Add the mask to the original to make’ ° the edge appear more obvious -—™—_, eee dACK: Prof. Tony Pridmore, UNUKOriginal signal a Blurred signal a7 Unsharp mask gS Sharpened signalCOMP 2032Derivative and Edges140Unsharp MaskingMakes edges noticeable sharper  •  Even if they are noise  •  Sometimes too much ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI ;15f’Derivative FiltersUnsharp filtering enhances edges by comparing the original with a smoothed image: I• Relies on the smoothing effect of a Gaussian function introducing adifference between original and processed|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKABUnsharpThis image has been enhanced using unsharp masking, using two different Gaussians. Which results came from unsharping with the largest σ value?ACK: Prof. Tony Pridmore, UNUKA σ = 1.5Bσ = 7.5irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B 2 = magnitude,3 = Gx1 = Gx,C2 = magnitude,1Z23 = GyACK: Prof. Tony Pridmore, UNUK316irEdge DetectionThis time the flower image has been put though the 3 stages of the Sobel edge detector: convolution with Gx, Gy, and computation of edge magnitude. Which image shows which?A 1 = Gy,2 = Gx,3 = magnitude1 = Gy,   B  2 = magnitude,  3 = Gx  ANSWER:  1 = Gx,   C  2 = magnitude, 123 = GyACK: Prof. Tony Pridmore, UNUK317iu aiu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK18iu a1st vs. 2ndExplain the difference between 1st and 2nd derivative methods. Please indicate one associated filter for|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization",
What is a structuring element in morphology?,"A structuring element in morphology is a binary mask used to process binary images. It is similar to a filter mask used in convolution, but instead of weighting pixels, it identifies pixels to be considered during morphological operations.

Here are some key characteristics of a structuring element:

*   It can be of various shapes and sizes.
*   Larger structuring elements produce more extreme effects.
*   It need not be square, and its origin need not be at the center.
*   Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element.

Examples of structuring elements include:

*   Rectangular
*   Elliptic
*   Cross-shaped

You can create structuring elements using functions like `getStructuringElement` in OpenCV.

The choice of structuring element depends on the specific morphological operation being performed, such as dilation or erosion, and the desired outcome.

For more information, refer to the lecture notes on morphology, specifically the section on structuring elements in `./documents/Lec-5-Morphology.pdf`.","A structuring element is a small matrix of pixels, typically with a defined shape (e.g., square, disk, cross), used as a probe in morphological operations. It defines the neighborhood of a pixel to be processed.","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B• The boundaries between foreground and background are often smoothed in the process• The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I• Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 5: Morphology1. Structuring ElementMorphological operations require an input binary image and a structuring element. Structuring elements vary in shape and size, similar to filter masks used in spatial filtering. You can create structuring elements using getStructuringElement:● Create an elliptic structuring element of sizes 3, 5 and 7.● Do the same for a rectangular and cross-shaped structuring element. (Note: You can also create a rectangular structuring element using NumPy’s ones function)2. DilationDilation is a morphological operation which expands the foreground using a structuring element. This is done by expanding the boundary of the foreground, which in turn closes the gaps between regions. The amount and direction of expansion are based on the chosen structuring element.● Read in the Square-Hole.jpg image from Moodle.● Perform dilation with dilate using the 9 structuring elements you have created. Compare the differences when using different shapes and sizes of structuring elements. Notice the effect on both the hole and the borders (particularly corners).● Using the|of erosion with a square 3 x 3 structuring element- Erosion is the dual of dilation, i.e., eroding foreground pixels is equivalent to dilating the background pixels with the same structuring elementACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology19Erosion - Counting objects (cells, coins) can be difficult if they touch- Erosion can separate them- Erosion can be used for edge detection too – giving pixels on the inside of the boundaryREMEMBERACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology20Asymmetric Structuring Elementsabc dj ff FIGURE 9.4 (a) Set A. (b) Square structuring element (dot is the center). (c) Dilation of A by B, shown shaded. (d) Elongated structuring element. (e) Dilation of A using this element. d/4 ap A®BACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyDilation21Asymmetric Structuring ElementsAGB a/4 abc dj FIGURE 9.6 (a) Set A. (b) Square structuring element. (c) Erosion of A by B, shown shaded. (d) Elongated structuring element. (e) Erosion of A using this element.ACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyErosion 22Break University of Nottingham UK | CHINA | MALAYSIAOpening & ClosingCombining Dilation and ErosionIts rare to need only erosion and|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|2032MorphologyMorphology8University of Nottingham UK | CHINA | MALAYSIAWhat is Mathematical MorphologyMathematical Morphology• A patch of the binary image to be processed • A Structuring Element, similar to the mask in a convolution process Underlying mathematics is beyond the scope of COMP2032. Instead, we will focus on how they work in practice and what DISCLAIMER they do‘- ' '- ‘- ‘-- A branch of image processing which treats images as sets of pixels and usesset theoretic operations to process them- Developed for binary images, extended to grey level images- Elements of sets are (x,y) coordinates of black (or white) pixels- Perform operations by combining two sets:ACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology10| I 1Structuring Elements1. '-- Binary masks, c.f. filter masks but identifying rather than weighting pixels- Larger structuring elements produce more extreme effects- Very similar effects can be achieved by repeated operations using a smaller but similarly shaped structuring element- With larger structuring elements, it is quite common to use an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony|an approximately disk-shaped structuring element- Need not be square, origin need not be in the centreACK: Prof. Tony Pridmore, UNUKti] 1 | 1 [=O] =| (del | teti] 1 | 1 [=O] =| (del | teCOMP 2032MorphologyMorphology111 II IirMorphological Operations:Expands a foreground (or background) object A usingstructuring element BDilationErosionShrinks a foreground (or background) object A usingstructuring element B• The boundaries between foreground and background are often smoothed in the process• The amount and the way objects grow and shrink depend upon the choice of the structuring element! ! I• Dilating or eroding without specifying the structural element makes no sense than trying to filter an image without specifying the filterIACK: Prof. Tony Pridmore, UNUKCOMP 2032Morphology; '|12University of m Nottingham P UK | CHINA | MALAYSIAErosion & DilationDilation - Binary image to dilate 1 1 1 2 inputs - Set of points to be considered; a structuring element 1 1 1 - Origin is the central element 1 1 1 • The structuring element is superimposed on each of the background pixels such that origin of the structuring element coincides with the input pixel position Algorithm • If any of the ‘1’",
How can you use a binary image as a mask?,"## Using a Binary Image as a Mask

A binary image can be used as a mask to select which pixels an operation is applied to. This is a useful technique in image processing, as it allows you to isolate specific regions of an image and apply operations only to those regions.

### How it Works

1. **Binary Image**: A binary image is an image where each pixel has a value of either 0 (black) or 255 (white).
2. **Masking**: When a binary image is used as a mask, the pixels with a value of 0 (black) are considered ""masked out"" or ignored, while the pixels with a value of 255 (white) are considered ""unmasked"" or selected.
3. **Applying Operations**: When an operation is applied to an image using a binary mask, the operation is only applied to the pixels that correspond to the unmasked (white) regions of the binary image.

### Example Use Case

For example, if you have a binary image where the foreground objects are white (255) and the background is black (0), you can use this binary image as a mask to apply an operation (e.g., filtering, thresholding) only to the foreground objects.

### Python Support

Python, specifically libraries like OpenCV, provides support for using binary images as masks. You can use the binary image as a mask when applying various image processing operations.

### Reference

This information is based on the provided context from the document `./documents/Practicals - Morphology.pdf`, which states: ""Binary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.""","A binary image can be used as a mask to apply an operation to a specific region of another image. Pixels in the binary mask with a value of 1 (or True) indicate the region where the operation should be applied, while pixels with a value of 0 (or False) are ignored.","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing’ a ume. For each subse€; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated|UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds, independently, to two copies of the image, then combine the results as I did with the Normal and Lighter images. This gave an image that was very hard to interpret, so here I have just shown them separately.domain. ; is of images is a pardiy ited goals. A feature spa J through the processing’ a ume. For each subse€; the feature of interest apped into a point in parameter. After the engHere the white area of the mask is coloured grey and the black area shows pg thresholded at 0.36Here the black area of the mask is coloured grey and the white area shows pg thresholded at 0.68ACK: Prof. Tony Pridmore, UNUK20University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!",
What is the Nyquist rate?,"The Nyquist rate is the minimum sampling rate required to accurately capture and reconstruct a continuous signal. According to the provided context from the lecture notes, it is defined as ""twice the frequency of the highest frequency component to be reconstructed"" (Source: Lec-1B-Digital Images and Point Processes.pdf).

In simpler terms, the Nyquist rate is the sampling rate at which the samples must be taken to avoid under-sampling and subsequent aliasing. Sampling below this rate can lead to artefacts and loss of information in the reconstructed signal.

To put it mathematically, if the highest frequency component of a signal is f_max, then the Nyquist rate is 2*f_max.

Is there anything else you'd like to know about the Nyquist rate or sampling in general?",The Nyquist rate is the minimum sampling rate required to avoid aliasing. It is twice the frequency of the highest frequency component in the signal (or image) being sampled.,"rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes6Aliasing• Aliasing occurs when two signals (images) become indistinguishable when sampled • In our case the two signals are the true image (the image that would be seen if there were no quantisation) and the one reconstructed by the human vision system from a sampled imageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesAdequately Sampled Signal Aliased Signal Due to Undersampling7Quantisation How many grey levels to store? ?• Determines the number of levels ofcolour/intensity to be represented at each pixel256, 64, 16, 4&• Sampling and quantisation occur naturally duringimage acquisition, but can also be applied toexisting imagesE.g., during resizing &compressionACK: Prof. Tony Pridmore, UNUKDigital Images and Point Processes8COMP 2032Re-Sampling & Re-Sizing• Most basic form of image processingBEIEIc [role zfelol@ fa [ufos]26353744• When downsampling, need to compute a summary pixel value fromie ‘each Se k. |Pick one, mean, weighted mean…ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingLecture 1B Digital Images and Point ProcessesirLearning OutcomesIDENTIFY  1. Digital Images Formation  2. Acquiring & Using Colour Images  3. Colour spaces  4.  Intensity Transform COMP 2032Digital Images and Point Processes2University of m @ Nottingham a UK | CHINA | MALAYSIADigital ImageFormationImage Formation & AcquisitionDigital Images and Point Processes ACK: Prof. Tony Pridmore, UNUK4hySampling & QuantisationSamplingDigitisation of the spatial coordinatesDigitisation of the light intensity functionQuantisationPridmore, UNUKACK: Prof. Tony Pridmore, UNUKSampling determines spatial resolution, quantisation determines grey level, colour or radiometric resolutionCOMP 2032Digital Images and Point Processes5hySampling?How many samples to take? i.e., how many pixels in the image?• Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed • Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate Nyquist RateArtefacts that result from under-samplingAliasingACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between a point process and a spatial filter?,"To understand the difference between a point process and a spatial filter in image processing, let's break down the concepts involved.

1. **Point Process**: A point process in image processing is an operation where the output pixel value depends solely on the corresponding input pixel value. It does not depend on the neighboring pixel values. The transformation is applied to each pixel independently.

   - **Example**: Adjusting the brightness and contrast of an image is a point process. For instance, if we have an image and we want to make it brighter or darker, we apply a transformation to each pixel based on its current value. A simple linear transformation like \(g(x,y) = a.f(x,y) + b\), where \(f(x,y)\) is the original pixel value, \(a\) is the gain (controlling contrast), and \(b\) is the bias (controlling brightness), is a classic example of a point process.

2. **Spatial Filter**: A spatial filter, on the other hand, involves operations where the output pixel value is determined by the input pixel value and its neighboring pixel values. The filter ""moves"" over the image, applying the same operation at each position to generate the output.

   - **Example**: Smoothing an image using a Gaussian filter is a spatial filtering operation. Here, each output pixel is a weighted average of the corresponding input pixel and its neighbors, with the weights determined by the Gaussian distribution.

The key differences are:
- **Dependency**: In a point process, the output depends only on the corresponding input pixel. In a spatial filter, the output depends on the input pixel and its neighbors.
- **Operation Scope**: Point processes operate on individual pixels, while spatial filters operate on a neighborhood of pixels.

To illustrate this further, consider the lecture notes from './documents/Lec-1B-Digital Images and Point Processes.pdf', which discuss point processes like intensity transformations, and './documents/Lec-3B-Non-Linear Filters.pdf', which covers spatial filters such as Gaussian and anisotropic diffusion filters.

In summary, if you're adjusting pixel values based solely on their current value, you're applying a point process. If you're adjusting pixel values based on their current value and the values of neighboring pixels, you're applying a spatial filter.","A point process operates on each pixel independently, where the output pixel value depends only on the corresponding input pixel value. A spatial filter operates on a local neighborhood of pixels, where the output pixel value depends on the values of the input pixel and its neighbors.","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in’ the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‘action process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in’ the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‘tapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Images and Point Processes25Hue, Saturation & SKIN• We don’t always need all 3 values  • Human skin is tightly clustered in H, S space ACK: Prof. Tony Pridmore, UNUKS HHistogram back-projectionCOMP 2032Digital Images and Point Processes2600Key Points Colour space transformation   is an example of a point   process  Alternative 3D colour spaces   exist  REMEMBER  Not all applications require all 3 colour   planes to be considered, & application-  specific colour spaces exist ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI|27University of Nottingham UK | CHINA | MALAYSIAIntensityTransformairirIntensity TransformSource  pixel  Source image  Target pixel value   depends only on   source pixel and   (maybe) some   parameters  Target  pixel  Target image ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes29irLinear TransformsCommonly used point processes are multiplication by   2  and addition of a constant, i.e.,   g(x,y) = a.f(x,y) + b  f(x)  x  a  Is the gain, and controls contrast  Is the bias, and controls brightness b  g(x)  g(x) xxACK: Prof. Tony Pridmore, UNUKCOMP 2032|Digital Images and Point Processes30Gaing(x,y) = 1.1.f(x,y) +|A feahire space ig ed Through the processing of aohme. Por each subset, a the fealire of interest ds wpped inte a pomnt in’ the piranieler. After the entireACK: Prof. Tony Pridmore, UNUK7ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?rpendent high- Tequires that, first, enough representati ‘action process be con aramelers correspanding t domain Is Gt nnages is a paradigm aled yraals. A feature space is d through the processing of a ohm. Pur each subset, a the fealare of interest is lapped inte aw point in’ the paranieler. After the entire =! - - 4 :Should we preprocess the image with:A: a median filter B: a small Gaussian C: a large Gaussian?ACK: Prof. Tony Pridmore, UNUK8An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process be et irameters corresp t domain. is of images is a parad ated goals. A feature space ad through the processing 6 a time. For each subset, | the feature of interest i ‘tapped into a point in th parameter. After the entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4|,;- Median filtering is good given small regions of speckle noise, less good if edges are important - There exist explicit edge- preserving smoothing opsDiffusion - Spreading out - Mean and Gaussian filters can be seen as diffusion processesACK: Prof. Tony Pridmore, UNUKCOMP 2032Non-Linear FiltersAnisotropic Not the same in all directionsAnisotropicBasic IDEA • Mean and Gaussian filters make each pixel more like its neighbours • Anisotropic diffusion makes each pixel more like those neighbours that it is already similar to10Anisotropic DiffusionWe have a similar function, s(p,q) • s(p,q) has values in the range from 0 to 1 • If the pixels p and q are similar then s(p,q) is close to 1 • If the pixel p and q are different then s(p,q) is close to 0 We use s(p,q) to compute a weighted average of pixel values - The new value at a pixel p, is based on all its neighbours, qACK: Prof. Tony Pridmore, UNUKCOMP 2032|Non-Linear Filtersp'  =  å ´  q  s(p,  q)  å  s(p,  q) 11[TheThe Similarity Function• The smoothing function, s(p,q) needs to be found•If d is the difference between p and q and D is the maximum possibledifference we can use:dD -Del ; ‘Other functions often used include: | Pee|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
How can you convert an RGB image to grayscale?,"Converting an RGB image to grayscale can be achieved through two primary methods: simple averaging and weighted averaging.

### Simple Averaging

The simple averaging method involves calculating the average of the red, green, and blue (RGB) values of each pixel. This is done by adding the RGB values together and dividing by 3.

`I = (R + G + B) / 3`

This method is straightforward but doesn't account for the human eye's varying sensitivity to different colors.

### Weighted Averaging

The weighted averaging method, on the other hand, takes into account the sensitivity of the human eye to different colors. The standard weights used are:

- Red: 0.30
- Green: 0.59
- Blue: 0.11

The grayscale value `I` is calculated as:

`I = 0.30R + 0.59G + 0.11B`

This method is more commonly used because it provides a more accurate representation of the image's luminance.

### Implementation

In Python, using the NumPy library, you can convert an RGB image to grayscale using the weighted average method as follows:

```python
import numpy as np
import cv2

# Load the RGB image
image = cv2.imread('image.jpg')

# Convert the image to grayscale using weighted average
grayscale_image = np.dot(image[...,:3], [0.30, 0.59, 0.11])

# Alternatively, use OpenCV's cvtColor function
grayscale_image_opencv = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
```

Note that OpenCV reads images in BGR format by default, so the conversion using `cvtColor` is from BGR to grayscale.

### Example Use Case

To display the original RGB image and its grayscale version:

```python
import matplotlib.pyplot as plt

# Display the original RGB image
plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.title('Original RGB Image')
plt.axis('off')

# Display the grayscale image
plt.subplot(1, 2, 2)
plt.imshow(grayscale_image, cmap='gray')
plt.title('Grayscale Image')
plt.axis('off')

plt.show()
```

This will display the original image alongside its grayscale version.

For further details, you can refer to the lecture notes on Digital Images and Point Processes (`./documents/Lec-1B-Digital Images and Point Processes.pdf`).","An RGB image can be converted to grayscale by taking a weighted average of the red, green, and blue channels. A common formula is:
Grayscale = 0.30*R + 0.59*G + 0.11*B, which takes into account the human eye's different sensitivities to different colors.","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.● Display its red, green and blue components as three separate grey-level images.● Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)● Perform the above again but using NumPy’s array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)● Produce and display an image of the atrium showing the ‘greenness’ value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.● Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel • Makes processing easier • Reduces the amount of information • Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so…ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*• Most common starting point• Retinal cells are sensitive tothree primary colours R, G, B• Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255","read in Python are arrays of type uint8 so some operations can cause overflows. You may find it useful to convert to int or float before applying some processes and then convert it back to uint8.● Display its red, green and blue components as three separate grey-level images.● Convert the RGB image to greyscale. First, by simple averaging, then using the weighting function described in Lecture 3. Perform this by explicitly writing a loop. Then, compare your results with the OpenCV function cvtColor. (Hint: Use empty or zeros from NumPy)● Perform the above again but using NumPy’s array manipulation facilities. (Hint: It may be helpful to create a Python function for the weighted function)● Produce and display an image of the atrium showing the ‘greenness’ value described in the lecture. This can be done either by explicitly writing a loop or (more efficiently) using the NumPy library.● Convert the RGB image to HSV and display each channel as separate images.3. Point Image ProcessingGamma correction is used to adjust the brightness and contrast of an image by applying a function which maps the intensity values in the image 𝐼 to new values in 𝑂. The formula is shown in (1), where 𝛾|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|ee yg LSFee a Se sn TL as i AS races OeColour ImageRedACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesREMEMBERBlue18Colour Vs. GreyscaleSometimes we want a single value at each pixel • Makes processing easier • Reduces the amount of information • Makes some of the theory simpler Many image processing methods were developed for single value images This value is usually the intensity or grey level - We can compute the grey value using a Early CCDs only produced grey simple average of red, green and blue level; rare now - But our eyes are more sensitive to green light so…ACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes19Colour Vs. GreyscaleWe can convert an RGB image to greyscale using:AverageACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point ProcessesI= 0.30R + 0.59G + 0.11BWeighted20University of m Nottingham P UK | CHINA | MALAYSIAColour SpacesirRepresenting Colour: RGB.* -* , ,*• Most common starting point• Retinal cells are sensitive tothree primary colours R, G, B• Light, not pigments|INTERESTING FACTACK: Prof. Tony Pridmore, UNUKCOMP 2032Digital Images and Point Processes'Let’s Walk DOWN Memory LaneB  255  255  255",
What is the purpose of histogram backprojection in object tracking?,"Histogram backprojection is used in object tracking to locate an object within an image or a sequence of images. Its primary purpose is to identify the location of a target object by comparing its histogram with the histogram of the image. 

The process involves the following steps:
1. Creating a histogram of the target object (model histogram).
2. Computing the ratio histogram by dividing the model histogram by the image histogram.
3. Backprojecting this ratio histogram onto the image to highlight areas that are similar to the target object.
4. Convolving the backprojected image to refine the location.

Histogram backprojection is robust to occlusion, meaning it can still locate an object even when parts of it are obscured. This is demonstrated in the provided context, where the algorithm successfully finds objects even when only four-ninths of the image of each object remains visible.

The efficiency of histogram backprojection allows it to be used not only for locating objects but also for tracking objects moving relative to a robot or camera, with the ability to execute multiple times per second.

For more information on histogram backprojection and its application in object tracking, you can refer to the source: [./documents/Swain1.pdf].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Swain1.pdf"" target=""_blank"">Swain1</a>

",Histogram backprojection produces a probability map of object locations based on a model histogram. This probability map is commonly used with tracking algorithms such as mean-shift to estimate and update the object's position across frames.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‘gained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|of size 32 X32 (1024 pixels) for the convolutions, the algorithm can be executed four times a second. The real-time experiments show that because Histogram Backprojection is extremcly cffi- cient it is useful not only for locating an object but also for tracking an object moving relative to the robot. Histogram Backprojection, like Histogram Intersec-Histogram Backprojection, like Histogram Intersec- tion, is robust to occlusion. Tf instead of using figure 6 as the crowded scene, we use figure 12 in which onlyfour-ninths of the image of each object remains, the location of each object can be found almost as well as in the image with no occlusion. They only target for which the effectiveness of Histogram Backprojection suffers badly is that of Charmin paper towels, the ob- ject that already had a portion occluded in the original image. As for the previous experiment, we show the backprojected ratio histogram for the blue and white striped shirt (see figure 18 on color page 32) and the combined results of looking for each of the objects in the image (see figure 19 on color page 32). In all cases but six the largest peak in the convolved image cor- responds to the correct object. The|can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im- age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.1 IntroductionIn recent years a new set of ideas about the goals and the methods of computer vision has ‘gained promin- ence, and may be on its way to becoming the dominant paradigm, because it promises the quickest route to constructing working vision systems. The term used to describe this set of ideas is animate vision, introduced by Ballard (1989, 1991). Similar ideas have recently been expressed using the terms active perception (Bajesy 1985, 1988), active vision (Aloimonos et al. 1988; Aloimonos 1990), qualitative vision (Nelson 1989, 1991), inexact vision (Thompson 1986), and dynamic vision (Dickmanns 1988). As in many other areas of science and technology, one of the driving forces behind the change in research|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the difference between first and second derivative methods for edge detection?,"The main difference between first and second derivative methods for edge detection lies in how they identify edges in an image.

**First Derivative Methods:**

1. Identify edges by looking for peaks in the gradient magnitude.
2. Strong response at edges, but also respond to noise.
3. Require peak detection and threshold selection, which can be challenging.

**Second Derivative Methods:**

1. Identify edges by looking for zero crossings in the second derivative.
2. Provide well-defined, easy-to-detect edges.
3. Tend to form smooth, connected contours, but may round off corners.

While both methods can be used for edge detection, **first derivative methods are more common in practical applications**, partly due to the work of John Canny.

To illustrate the difference, consider the following:

* First derivative methods, like the Sobel operator, calculate the gradient of the image intensity function. Edges are identified where the gradient magnitude is significant.
* Second derivative methods, like the Laplacian of Gaussian (LoG), calculate the second derivative of the image intensity function. Edges are identified where the second derivative crosses zero.

In summary, first derivative methods are more sensitive to noise but can be more effective in certain situations, while second derivative methods provide more precise edge localization but may be more prone to rounding off corners.

Source: ./documents/Lec-6A-Derivative and Edges.pdf (COMP 2032 Derivative and Edges slide 42)","First derivative methods (e.g., Sobel, Roberts) look for peaks in the image gradient to find edges. Second derivative methods (e.g., Laplacian of Gaussian) look for zero-crossings in the second derivative of the image to find edges.","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|vs 2nd Derivative MethodsPeaks in 1st Derivative Zero crossings in 2nd derivative • Strong response at edges, but - Well-defined, easy to detect also respond to noise • Peak detection and threshold VS - Must form smooth, connected contours selection need care - Tend to round off corners 1st derivative methods are much more common in practical applications, In part because of John CannyACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges42University of Nottingham UK | CHINA | MALAYSIAThe Canny OperatoraWhat Canny DidJohn Canny tried to find the optimal edge detector,assuming a perfect step edge in Gaussian noiseGood Detection  It should mark all the edges and only all   the edges  Optimal  Good Localisation  The points marked should be as close to   the real edge as possible  Minimal Response  Each edge should be reported only once Canny used the Calculus of Variations: finds thefunction which best satisfies some functionalACK: Prof. Tony Pridmore, UNUKI ee ICOMP 2032|Derivative and Edges44The Canny Operator- The optimal detector was a sum of 4--exponential terms, but is very closely' |approximated by the 1st derivative of aGaussianeei.e., 1st derivative of aGaussian|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in",
What are the main steps in the Canny edge detection algorithm?,"The Canny edge detection algorithm is a multi-step process. According to the provided context, the main steps are:

1. **Noise Reduction**: The first step is to reduce the noise in the image by using Gaussian smoothing.
2. **Gradient Calculation**: Calculate the gradient using Sobel filters in both the x and y directions.
3. **Non-Maximum Suppression**: Perform non-maximum suppression to obtain thin edges by saving only the local maximums of the gradient in the gradient direction.
4. **Hysteresis Thresholding**: Apply hysteresis thresholding to select the edges by using two threshold values (min and max). Edges above the max threshold are considered edges, edges below the min threshold are considered non-edges, and edges between the two thresholds are dependent on their connectivity to an edge or non-edge.

These steps enable the Canny algorithm to effectively detect edges in an image.

Source: `./documents/Lec-6A-Derivative and Edges.pdf`","The Canny edge detection algorithm involves: 1) Gaussian smoothing to reduce noise, 2) finding the intensity gradient, 3) non-maximum suppression to thin edges, and 4) hysteresis thresholding to link edges.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the “objects” in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¢ EX 0 if ||X@) — XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 6A Derivative and Edges=irLearning OutcomesIDENTIFY  1. Derivative Filters  2. Sharpening  3. What is Edge Detection?  4. Edge Detection using 1st Derivatives  5. Edge Detection using 2nd Derivatives  6. The Canny Operator COMP 2032Derivative and Edges2University of m Nottingham P UK | CHINA | MALAYSIADerivative FiltersIn 1 DimensionA  B  intensity Image features are often   characterised by changes in   intensity 2 _4~ Isolated point renee i so % Ra i Ste i 2 4 ~~ Ramp i Thinline PN} 24 « po } ~ . ‘ {Flat segment 7‘, ; 2 ‘. i 4 ‘ ; S| “e rok \ a *, H *s b--sia-¢ ‘w---»-8 0 Enage strip] 5[5]4]3]2]1]0] 0) 0]6]0}o)o/0}1/3]1)0/0/0]0]7) 7] 7/7] +] «|ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges4001st DerivativeThe 1st derivative of a function can be approximated by:  ¶  ¶  f  x  =  xf  (  -+  )1  xf  )( , _The difference between neighbouring values and measures the rate of change of the functionACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edgesa51st DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the “objects” in Fig. 8 have rather ill- defined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.We have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G = (V,E) by taking each pixel as a node and define the edge weight w;; between node i and j as the product of a feature similarity term and spatial proximity term:(B =IXO-XOIS *¢ EX 0 if ||X@) — XG) <r otherwise, wy=e 7where X(i) is the spatial location of node i, and F(i) is a feature vector based on intensity, color, or texture informa- tion at that node defined as:.e F(i) =1, in the case of segmenting point sets,Note that, in all these examples, the algorithm is able toextract the major components of scene while ignoring small intracomponent variations. As desired, recursive partition- ing",
What is the purpose of non-maximum suppression in the Canny edge detector?,"The purpose of non-maximum suppression in the Canny edge detector is to obtain thin edges by saving only the local maximums of the gradient in the gradient direction. This step helps to reduce the thickness of edges detected in the image, resulting in a more precise representation of the edges.

To break it down:

1. **Gradient Calculation**: The Canny edge detector first calculates the gradient of the image intensity function in the x and y directions using Sobel filters.
2. **Non-Maximum Suppression**: It then applies non-maximum suppression to the gradient magnitude image. This involves comparing the gradient magnitude of a pixel with its neighbors along the direction of the gradient. If the gradient magnitude of the pixel is greater than its neighbors, it is considered a local maximum and is retained; otherwise, it is suppressed.

By doing so, non-maximum suppression helps to:

* Thin the edges, making them more precise and less thick.
* Reduce the number of edge pixels, which can improve the overall quality of edge detection.

This step is crucial in the Canny edge detection algorithm as it enhances the accuracy of edge detection by eliminating non-essential edge pixels.

Source: './documents/Lec-6A-Derivative and Edges.pdf'

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Non-maximum suppression is used to thin the edges found by the gradient calculation. For each pixel, it checks if the gradient magnitude is the largest in the direction of the gradient. If not, the pixel is suppressed (set to zero).","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is hysteresis thresholding?,"Hysteresis thresholding is a technique used in edge detection to determine the edges in an image. It's a two-stage process that involves setting two threshold values: a high threshold and a low threshold.

The high threshold is used to identify strong edges, while the low threshold is used to identify weak edges. The idea is that weak edges are considered valid only if they are connected to strong edges.

Here's how it works:

1. **Strong edges**: Pixels with gradient values above the high threshold are considered strong edges.
2. **Weak edges**: Pixels with gradient values between the low and high thresholds are considered weak edges.
3. **Edge tracking**: Weak edges are retained only if they are connected to strong edges.

By using hysteresis thresholding, you can effectively reduce noise in the edge detection output while preserving the continuity of edges.

This technique is part of the Canny edge detection algorithm, which is considered the industry standard for edge detection.

Source: Lec-6A-Derivative and Edges.pdf ( slide 50)

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-6A-Derivative and Edges.pdf"" target=""_blank"">Lec 6A Derivative and Edges</a>

","Hysteresis thresholding is a technique used in the Canny edge detector to connect weak edges to strong edges. It uses two thresholds, a high and a low. Pixels with a gradient magnitude above the high threshold are considered strong edges, and pixels between the two thresholds are considered weak edges. Weak edges are only kept if they are connected to strong edges.","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of","g1,g2,...,gM and a superpixel output s1,s2,...,sL, the under-segmentation error for a ground truth segment gi is quantiﬁed as:M U=5 S SS Isl} -N (3) i=1 \[sjlsj A gi>Bwhere |.| gives the size of the segment in pixels, N is the size of the image in pixels, and B is the minimum number of pixels that need to be overlapping. The expression sj () gi is the intersection or overlap error of a superpixel s; with respect to a ground truth segment g;. B is set to be 5% of |s;| to account for small errors in ground truth segmentation data. The value of U is computed for each image of the ground truth and then averaged to obtain the graph in Fig.4.3 Boundary recallWe adopt the standard boundary recall measure which computes what fraction of ground truth edges fall within one pixel of a least one superpixel boundary. We use the internal boundaries of each superpixel. The boundary recall of each of the considered methods is plotted against the number of superpixels in Fig. 3(b).4.4 Computational and memory eﬃciencyFor images of size 480 × 320, SLIC is more than 10 times faster than TP09 and more than 500 times faster than NC05. What is encouraging is that it is even10Achanta et al.25 20 g 8 is|- Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges;i53Summary'1. Derivative Filters (2. Sharpening 13. What is Edge Detection? \4. Edge Detection using 1%t Derivatives 15. Edge Detection using 2"" Derivatives |6. The Canny OperatorCOMP 2032Derivative and Edges🤔54University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:Hough Transform|pixel independently:   edges aren’t really independent, they make up lines  • The industry standard edge thresholding method  - Allows a band of variation, but assumes continuous edges  - User still selects parameters, but its easier, less precise ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and EdgesI49Thresholding with HysteresisThe effect is to keep weak edges if they connect strong edges,   as long as  - The strong edges are really strong  - The weak edges aren’t really weak ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges50Thresholding with HysteresisHysteresis fills in most of the gapsACK: Prof. Tony Pridmore, UNUKProblem: pixels along this edge didn’t survive the thresholdingCOMP 2032Derivative and Edges51Thresholding with HysteresisLow thresholdHigh thresholdACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and EdgesHysteresis520What Canny Did Showed that 1st derivative of a Gaussian smoothed image is the optimal way to detect step edges in noise Explained why 1st derivatives are a good idea Designed the industry standard thresholding method - Non-maximal suppression - Thresholding with hysteresis Effectively solved the edge detection problem' '🥳ACK: Prof.|in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendi- cular to each other. In particular, z;, the second smallest eigenvector, is perpendicular to zp. Translating this state- ment back into the general eigensystem (6), we have: 1) y =1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 = 2) z = y!D1, where y, is the second smallest eigenvector of (6).Now, recall a simple fact about the Rayleigh quotient [11]:Let A be a real symmetric matrix. Under the constraint that « is orthogonal to the j-1 smallest eigenvectors 21,...,2j-1, the quotient vps is minimized by the next smallest eigenvector x; and its minimum value is the corresponding eigenvalue X,.As a result, we obtain:2?D-(D — W)D2z Zz = arg.mingrz—0 wsand, consequently,T = arami y (D-W)y yy = arg.minyrpi—o Dy (9)Thus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normal- ized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y that y(i) takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of",
What is the difference between the Sobel and Roberts edge detectors?,"The Sobel and Roberts edge detectors are both used for edge detection in image processing. 

The main difference between them lies in how they calculate the gradient of the image intensity function.

The Roberts edge detector uses a 2x2 kernel to compute the gradient, whereas the Sobel edge detector uses a 3x3 kernel. The Roberts operator is very quick to compute as it only involves subtractions and additions of 4 pixels. However, it is very sensitive to noise and only gives a strong response to very sharp edges.

On the other hand, the Sobel operator is more stable in noise, especially when larger Sobel operators are used. Although both operators use a user-supplied threshold, the Sobel operator is still widely used today, whereas the Roberts operator is less common.

In summary, while both edge detectors have their uses, the Sobel edge detector is generally more robust and less sensitive to noise compared to the Roberts edge detector.

Source: './documents/Lec-6A-Derivative and Edges.pdf'","The Sobel and Roberts edge detectors are both first-derivative edge detectors, but they use different kernels to approximate the gradient. The Sobel operator uses 3x3 kernels, while the Roberts operator uses 2x2 kernels.","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute – 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e© Nottingham “i UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V°L f(x. ») * G(x. y)] = V°G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <——<——37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the","blur and Laplacian parameters and compare the effects on the resulting image.Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCannyCanny edge detection can be said to be an extension of Sobel edge detection. The first step is to reduce the noise in the image by using Gaussian smoothing before calculating the gradient using Sobel filters in both the x and y. To obtain thin edges, non-maximum suppression is performed by saving only the local maximums of the gradient in the gradient direction.Lastly, hysteresis thresholding selects the edges by using two threshold values (min and max). Edges above the max threshold are edges, edges below the min threshold are non- edges, and for edges in between the two thresholds, it is dependent on their connectivity to an edge/non-edge.● Use the Canny function on the Opera house.jpeg image, and by varying the parameters, extract:a. the structure of the opera house roofb. the waves on the waterIdentify parameters in the Canny function that will:a. extract the boundaries of the cars (but not their shadows) from the Highway.jpeg image.b. identify all the flowers in|Cross OperatorVery quick to compute – 4 pixels, only subtractions and additions, but is very sensitive to noise and only gives a strong response to very sharp edgesOriginalOriginalCross OperatorThresholdedACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges33Sobel vs Roberts- Both use a super-supplied threshold. Sobel is still in use. Roberts isless common, nowadays.- Larger Sobel operators are more stable in noiseOriginalOriginalRobertsSobelACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges34University of m @e© Nottingham “i UK | CHINA | MALAYSIAEdge Detection using 2nd Derivative FiltersYaThe TheoryQUICK RECAP!An i ceaoed image of an i ceaoedA1st derivative: a peakzc ie2""¢ derivative: a zero-crossingTo detect edges find peaks in the 1st derivative or intensity or zero-icrossings in the 2nd derivativeACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges3602nd Derivatives: Marr-HildrethBiologically inspired  Gaussian smooth, compute Laplacian  OR  Convolve with the Laplacian of Gaussian V°L f(x. ») * G(x. y)] = V°G(x. y) * f(x,y) UNUKACK: Prof. Tony Pridmore, UNUK|Derivative and EdgesCOMP 2032QoQ <——<——37Laplacian of Gaussian (LoG){) | ) l * l 2) 410 2 ()|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 6: Edge Detection and Hough Transform1. Edge DetectionOpenCV provides many types of edge detection algorithms and functions, including the ones discussed in the lectures (e.g., Sobel, Roberts). Each edge detection method provides slightly different results, and the key to successful edge detection is a good choice of parameters rather than the method choice.Note: The images should be converted to grayscale before performing any of the following edge detection methods. Make sure to consider all the parameters of each edge detection function: what it is and its effect on the resulting image.1st Derivative: SobelSobel edge detection works by identifying areas with significant intensity changes as edges. This is done by calculating the gradient in both the x and y directions individually and then combining them to calculate the magnitude of the gradient at that specific pixel. Pixels with a high magnitude are then classified as an edge.● Read in the Opera house.jpeg image from Moodle.● By using the Sobel function, tweak the parameters and show the results of applying the Sobel|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the",
What is a color histogram?,"A color histogram is a representation of the distribution of colors in an image. It is obtained by discretizing the image colors and counting the number of times each discrete color occurs in the image array. The image colors that are transformed to a common discrete color are thought of as being in the same 3D histogram bin centered at that color.

To illustrate, consider an image with various colors. The color histogram would count the occurrences of each color in the image, providing a statistical representation of the color distribution.

Color histograms have some useful properties, such as being invariant to translation and rotation about the viewing axis. They also change only slowly under changes in the angle of view, scale, and occlusion.

For example, as shown in Figure 1 of the referenced document [./documents/Swain1.pdf], a color camera output can be used to generate a color histogram, which provides a visual representation of the color distribution in the image.

Would you like to know more about the applications of color histograms or how they are used in image processing tasks?",A color histogram is a representation of the distribution of colors in an image. It counts the number of pixels for each color in a discrete color space.,"is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,","is called Histogram Backprojection. It finds the region in the image where the colors in the model histogram show up together, relying more on those colors that show up about as much as expected than those which show up much more, and therefore occur in other objects besides the target..The experiments in section 4.1.2 show that Histo- gram Backprojection works well even when the objects containing the same colors occur in the image and when the object is partially occluded.2 Color HistogramsGiven a discrete color space defined by some color axes (e.g., red, green, blue), the color histogram is obtained by discretizing the image colors and counting the num- ber of times each discrete color occurs in the image array. The image colors that are transformed to a com- mon discrete color are usefully thought of as being in the same 3D histogram bin centered at that color. To illustrate, figure 1 (see color figures on page 29) shows the output from a color camera together with a color histogram obtained from the image.Histograms are invariant to translation and rotation about the viewing axis, and change only slowly under change of angle of view, change in scale, and occlusion (see color|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|size of the histogram over two orders of magnitude, from 64 accessible bins (8x 8X4 bins total) to 8125 accessible bins (40x40 x20 bins total). There are only small changes in the match effec- tiveness over the entire range of histogram sizes. Note that matches in the high resolution histograms rely on the fact that there are smooth distributions of colors on the objects. These distributions arise from the large regions of constant color being blurred in color by shading and camera noise. In the highest resolution histograms the same pixels are not matching each other, but different pixels from the same color region.In a set of real-time experiments employing a Datacube pipelined image processor, 8X88 (red, green, blue) histogram were used instead of the 16x168 opponent color histograms, with good results, so the choice of color axes is not crucial either. The col- or camera was mounted in our mobile robot platform and panned across a floor containing the database of colored shirts. The shirts were spread out on the floor, but no special effort was made to lay them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,",
How can you use a histogram to improve image contrast?,"To improve image contrast using a histogram, you can employ two key techniques: Histogram Equalisation and Contrast Stretching.

### Histogram Equalisation

Histogram Equalisation is a method used to improve the contrast of an image by redistributing the pixels to achieve a uniform distribution of pixel values. This is done by transforming the image in such a way that the transformed image has a nearly uniform distribution of pixel values.

Here's how to do it:

1. **Create and display the histogram** of the original greyscale image.
2. **Apply Histogram Equalisation** using a function like `equalizeHist` from OpenCV.
3. **Display the histogram** of the resulting image after Histogram Equalisation.

The resulting image will have improved contrast, and its histogram will be more evenly distributed.

### Contrast Stretching

Contrast Stretching is another technique used to improve the contrast of an image. It works by linearly stretching the range of pixel intensity values to a specified range, usually [0, 255].

To perform Contrast Stretching:

1. **Convert the image to greyscale**.
2. **Apply Contrast Stretching** using a function like `normalize` from OpenCV.
3. **Display the resulting image and its histogram**.

By comparing the original image, the Histogram Equalisation output, and the Contrast Stretching output, you can observe the differences in contrast improvement achieved by each method.

### Key differences between Histogram Equalisation and Contrast Stretching

*   Histogram Equalisation aims to achieve a uniform distribution of pixel values, while Contrast Stretching linearly stretches the pixel intensity values to a specified range.
*   Histogram Equalisation is more general and doesn't require specifying parameters, whereas Contrast Stretching requires specifying the target range for the pixel intensity values.

By applying these techniques, you can effectively use histograms to improve the contrast of images.

References:
['./documents/Lab 2.pdf']

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-2-Histogram (Whole Image Understanding).pdf"" target=""_blank"">Lec 2 Histogram (Whole Image Understanding)</a>

<a href=""http://localhost:5003/documents/Lab 2.pdf"" target=""_blank"">Lab 2</a>

",Histogram equalization can be used to improve image contrast by redistributing the pixel intensities to create a more uniform histogram.,"the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image • To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values • More general than linear or piecewise contrast stretching - No parameters to specify, “apply a transform that makes the output histogram FLAT” Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability","the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by|total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability of the corresponding grey level appearing in the   image  - The probabilistic interpretation is valuable in e.g., contrast enhancement and   automatic thresholding  REMEMBER ACK: Prof. Tony Pridmore, UNUKCOMP 2032Histogram (Whole Image Understanding)5Histogram Dark Light  Histogram (Whole Image Understanding) I I I Histogram of light imageLight ACK: Prof. Tony Pridmore, UNUKCOMP 20326High Contrast Histogram of low-contrast image I T T T Histogram of high-contrast imageHistogram Low ContrastHigh Contrast ACK: Prof. Tony Pridmore, UNUK7Histogram (Whole Image Understanding)COMP 2032University of m @©Nottingham a UK | CHINA | MALAYSIAHistogram Equalisation (Theory)irHistogram EqualisationTo improve the contrast of an image • To transform an image in such a way that the transformed image has a nearly uniform distribution of pixel values • More general than linear or piecewise contrast stretching - No parameters to specify, “apply a transform that makes the output histogram FLAT” Histogram Transform - Map an input histogram r onto a new histogram s - Assume r has been|a back-of-the envelope calculation we can try to account for the unequal distribution by reducing the number of histo- gram bins to, say 100, as figure 2 would suggest. The fraction of histogram space is still very small, 3x107!'. A more accurate analysis might consist of generating a Monte-Carlo distribution of histograms throughout color space and measuring their overlap.3.1.2. Experiments. Experiments were performed to see if a large number of objects can be distinguished and to test the senstitivity of the recognition technique to changes in view, image resolution, and occlusion.An experimental test of histogram intersection shows that the technique is capable of indexing into a large databasc, that is, eliminating most of the possible matches leaving only a small number for further con- sideration. The 32 images in figure 6 were matched to each of the 66 models in the database in figure 516 Swain and Ballard(see color figures on page 31). ‘I'he color axes used for the histograms were the three opponent color axes, defined as follows (Ballard & Brown 1982):the database shown in figure 5, only about 200 (5x5 x8) of the 512 receive an appreciable number of counts, as is|0) 200 400 600 5000 15000 4 15000 4000 10000 4 10000 Histogram 3000 2000 5000 + 5000 1000 0) 0) 0) 0 50 100 150 200 250 ie] 50 100 150 200 250 ie] 50 100 150 200 250 5000 { 15000 | 15000 4 4000 - Histogram 10000 4 10000 4 Equalisation 5000 4 5000 4 le) le) 100 150 200 250 0 50 100 150 200 #250 0) 50 100 150 200 250Figure 4: Varying Gain, Bias and GammaOriginal Image Histogram Equalisation Contrast Stretching 100 200 300 300 400 400 ie] 100 200 300 400 500 600 0 100 200 300 400 500 600 ie] 100 200 300 400 500 600 5000 4 5000 4 5000 5 4000 4 4000 7 4000 4 3000 + 3000 4 3000 + 2000 2000 4 2000 1000 1000 4 1000 fe) fe] fe) 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250Figure 5: Histogram Equalisation vs Contrast StretchingPrepared by Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 3|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 2 Histogram (Whole Image Understanding)omirLearning OutcomesIDENTIFY  1. Why Histogram?  2. Histogram Equalisation - Theory  3. Histogram Equalisation - Practical  4.  Image Matching with Colour Histogram  5. Histogram Intersection  6. Using Histogram - Object Location COMP 2032Histogram (Whole Image Understanding)2University of m Nottingham P UK | CHINA | MALAYSIAWhy Histograms?0Histograms ee ; ' The histogram of a digital image with grey levels in the range [0, L-1]is a |discrete functioneeee ee ee ee ee P(r) =, eesa -.- K=0,1,..., L-1 7 r, is the Kn grey level, 7n, is the number of pixels in the image with that grey level ,Histograms provide useful global information about the image, ease computation of some image properties, and can be manipulated to improve the image,ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Histogram (Whole Image Understanding)I4,Normalised HistogramsA normalised histogram is a discrete function   P(rk) = nk /n  Where:  -  N = width x height is the total number of pixels in the image  - The bins in a normalised histogram sum to 1.0  - Each bin gives the probability",
What is a binary image?,"A binary image is an image where pixels can have only two values, typically 0 (black) and 255 (white) or 0 and 1. This type of image is often used in image processing for edge detection and segmentation.

In a binary image, pixels are classified as either foreground (object) or background. The process of converting a grayscale or color image into a binary image is called thresholding or binarization.

For example, if we have a grayscale image, we can apply thresholding by choosing a threshold value, T. Pixels with intensity values less than T are considered object (foreground), while those with values greater than or equal to T are considered background.

Binary images are useful for simplifying image analysis and are often used as a preprocessing step for more complex image processing tasks.

Source: Lec-4-Thresholding and Binary Images.pdf, Lab 4.pdf","A binary image is an image where each pixel can have only one of two values, typically 0 or 1 (or black and white).","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform","University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 4 Thresholding & Binary ImagesomirLearning OutcomesIDENTIFY  1. What is Thresholding  2. Adaptive Thresholding COMP 2032Thresholding & Binary Images2University of m @e© Nottingham “i UK | CHINA | MALAYSIAWhat isThresholding?irBinary ImagesMany image processing operations make a decision:ee ee ee eee : .- Pixels a binary image; = '_ Binary eee eee, I | s=T(r) =| rH~ DarkIs this the colour/object/edge I am interested in?enhancement, etc. - Pixels can have only two values: 0 or 1eee ‘The PeeThe result is a binary image- Binary images also need noise removal,enhancement, etc.s=T(r) =| rH~ ! Dark LightACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary Imagesl4Binarisation: Thresholding• A dark object on a light background in a grey-level image • Choose a threshold value, T • Consider each pixel in turn - If the brightness at a pixel is less than T, that pixel is object - Otherwise, it is part of the background Basic idea extends to colour; define sets of colour values that correspond to objectsACK: Prof. Tony Pridmore, UNUKCOMP 2032Thresholding & Binary ImagesThreshold, T = 965Too|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform",
How can you create a binary image from a grayscale image?,"## Creating a Binary Image from a Grayscale Image

To create a binary image from a grayscale image, you can use a process called thresholding. Thresholding involves setting a threshold value and comparing each pixel's intensity value in the grayscale image to this threshold.

### Steps:

1. **Choose a Threshold Value**: Decide on a threshold value. Pixels with intensity values above this threshold will be considered foreground (usually set to 255, white), and those below will be considered background (usually set to 0, black).

2. **Apply Thresholding**: Compare each pixel's intensity in the grayscale image to the chosen threshold value. If the pixel's intensity is greater than the threshold, set its value to 255 (white); otherwise, set it to 0 (black).

### Methods of Thresholding:

- **Manual Thresholding**: Manually select a threshold value. This can be effective for images where the foreground and background have distinct intensity ranges.
  
- **Otsu's Thresholding**: An automatic method that calculates the optimal threshold value by minimizing the variance within the foreground and background classes. It's particularly useful when the histogram of the image is bimodal.

### Example using OpenCV:

You can use OpenCV to perform thresholding. For example, to apply Otsu's thresholding, you would use the `cv2.threshold` function with the `cv2.THRESH_OTSU` flag.

```python
import cv2

# Load the image in grayscale mode
image = cv2.imread('image_path.jpg', cv2.IMREAD_GRAYSCALE)

# Apply Otsu's thresholding
_, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

# Display the binary image
cv2.imshow('Binary Image', binary_image)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

### Reference:
For more details on thresholding techniques and their implementation, you can refer to the provided laboratory sheet on thresholding and binary images [./documents/Lab 4.pdf].

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lab 4.pdf"" target=""_blank"">Lab 4</a>

","A binary image can be created from a grayscale image using thresholding. A threshold value is chosen, and all pixels with an intensity value above the threshold are set to one value (e.g., 1), while all pixels below the threshold are set to the other value (e.g., 0).","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find","if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 4: Thresholding & Binary Images1. Binary ImagesThresholding commonly produces a binary image with pixel values of either 0 (black) or 255 (white). Binary images are often used in image processing for edge detection and segmentation. To obtain a binary image, thresholding must take place. Figure 2 shows the binarized image obtained from performing thresholding on Figure 1.Figure 1: Original ImageFigure 2: Binarized Image2. ThresholdingThresholding classifies the pixels in an image to either foreground or background based on the intensity values of the pixels. The pixel values of the image are set to either 0 or 255, depending on if it is above or below the decided threshold value. Using the some text.png image from the Moodle page:● Use the threshold function and the THRESH_BINARY flag to perform thresholding on the image with a threshold value of your choice.● Perform Otsu thresholding by adding the THRESH_OTSU flag and compare the results with your selected threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|pair among all image path - label pairs. First, let’s retrieve an image path - label pair according to the index.class MNIST(Dataset):def __getitem__(self, index):image_path_label_pair = self.image_path_label_pairs[index]After that, obtain the image path and label of the image, respectively.class MNIST(Dataset):def __getitem__(self, index):…image_path = image_path_label_pair[0]label = image_path_label_pair[1]Then, let’s read the image by loading the image path with OpenCV. Note that the image should be loaded in greyscale.class MNIST(Dataset):def __getitem__(self, index):...image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)Next, the image should be converted from OpenCV (NumPy array) to PyTorch tensor.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.to_tensor(image)Finally, the image is normalised with the given mean and standard deviation, and the processed image and its label are returned.class MNIST(Dataset):def __getitem__(self, index):...torch_image = TF.normalize(torch_image, mean=[0.1307], std=[0.3081])return torch_image, labelPrepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 16f° University of | wae Nottingham UK ||each pixel. Gener- ally 0 < @, < 1, but for hard segmentation a, € {0,1}, with 0 for background and | for foreground. The parameters @ describe image310foreground and background grey-level distributions, and consist of histograms of grey values:q = fh(z;a);a = 0;1g; (1)one for background and one for foreground. The histograms are assembled directly from labelled pixels from the respective trimap regions TB;TF. (Histograms are normalised to sum to 1 over the grey-level range:Rzh(z;a) = 1.)The segmentation task is to infer the unknown opacity variables a from the given image data z and the model q.2.2 Segmentation by energy minimisationAn energy function E is defined so that its minimum should cor- respond to a good segmentation, in the sense that it is guided both by the observed foreground and background grey-level histograms and that the opacity is “coherent”, reflecting a tendency to solidity of objects. This is captured by a “Gibbs” energy of the form:E(a;q;z) =U(a;q;z)+V(a;z) : (2)The data term U evaluates the fit of the opacity distribution a to the data z, given the histogram model @, and is defined to be:U(a@,0,2) = —logh(zn; @n)- (3) nThe smoothness term can be written|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find",
What are connected components in a binary image?,"## Connected Components in Binary Images

Connected components in a binary image refer to the groups of pixels that are connected to each other based on certain criteria, typically being the same color (either black or white). In the context of binary images, where pixels are either 0 (background) or 1 (foreground/object), connected components are the sets of pixels with the value 1 that are adjacent to each other.

### Definition and Purpose

The process of identifying connected components involves labeling each group of connected pixels with a unique identifier. This is useful for various image processing tasks, such as object detection, image segmentation, and analysis.

### Types of Connectivity

There are two main types of connectivity used to define connected components:

1. **4-Connectivity**: Pixels are considered connected if they share an edge (up, down, left, or right).
2. **8-Connectivity**: Pixels are considered connected if they share an edge or a corner (up, down, left, right, and diagonals).

### Importance in Image Processing

Identifying connected components is a fundamental step in many image processing applications, as it allows for the isolation and analysis of individual objects within an image.

### Example from Lecture Notes

According to the lecture notes on Morphology (Lec-5-Morphology.pdf), connected component algorithms are used to give each connected set of black pixels (if black pixels are objects and white pixels are background) a different label. This is achieved through a two-pass process over the image, where the first pass involves scanning the image and assigning labels based on the neighbors of each pixel.

For further details, you can refer to the lecture notes on Morphology, specifically the section on Connected Components.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

","Connected components are sets of connected pixels in a binary image that share the same value (usually the ""on"" value). They represent distinct objects or regions in the image.","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 – 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) – 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‘ —s Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython’s array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -","image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |4Some ExercisesOutput Pixel Value sl. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?Agrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged5irSome ExercisesPython’s array manipulation is very well suited to image processing. What will this code fragment do?>> img =|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises Digital Images and Point ProcessesSome ExercisesWhen contrast stretching is used to transform an image containing grey levels in the range 10-40 into one with grey levels in the range of 0 – 255, what is the computation used?A: g(x,y) = f(x,y).(30/255) B: g(x,y) = ((f(x,y) – 10/30).255 C: g(x,y) = 10 + ((f(x,y)/255).30ACK: Prof. Tony Pridmore, UNUK2Some Exercisesrc ‘ —s Output Pixel Value J. = a a SeIntensity transforms are often presented as diagrams in which lines or curves illustrate the mapping between input grey levels and output grey levels. What effect would this transform have on an input image?grey levels below a are mapped to whiteAgrey levels above b are mapped to whitegrey levels between a and b are increasedBgrey levels below a are mapped to black grey levels above b are mapped to black grey levels between a and b are unchangedACK: Prof. Tony Pridmore, UNUKCgrey levels below a are mapped to white grey levels above b are mapped to white grey levels between a and b are unchanged| |3irSome ExercisesPython’s array manipulation is very well suited to image processing.|2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -",
How can you use connected components analysis?,"Connected Components analysis is a technique used in image processing to identify and label distinct objects or regions within a binary image.

### What is Connected Components Analysis?

Connected Components analysis is a process that converts a binary image into a label image, where each connected set of pixels (either foreground or background) is assigned a unique label. This allows for the identification and analysis of individual objects or regions within the image.

### How to Use Connected Components Analysis

1. **Binary Image**: Start with a binary image where objects are represented by foreground pixels (usually black) and the background is represented by background pixels (usually white).
2. **Labeling**: Apply a connected components algorithm to the binary image. The algorithm scans the image and assigns a unique label to each connected set of foreground pixels.
3. **Analysis**: Once the image is labeled, you can analyze the properties of each connected component, such as its size, shape, and location.

### Applications of Connected Components Analysis

* **Object Counting**: Count the number of distinct objects in an image.
* **Object Recognition**: Identify specific objects based on their shape, size, or other characteristics.
* **Image Segmentation**: Separate objects from the background or from each other.

### Example

Suppose you have a binary image of flowers, where the flowers are represented by black pixels and the background is white. You can use connected components analysis to:

1. Label each flower with a unique identifier.
2. Count the number of flowers in the image.
3. Analyze the size and shape of each flower.

### Code Example (Python using OpenCV)

```python
import cv2
import numpy as np

# Load the binary image
img = cv2.imread('image.png', cv2.IMREAD_GRAYSCALE)

# Apply thresholding to ensure binary image
_, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY)

# Find connected components
num_labels, labels = cv2.connectedComponents(thresh)

# Print the number of connected components
print(""Number of connected components:"", num_labels)

# Display the labeled image
cv2.imshow('Labeled Image', labels.astype(np.uint8) * 255 // num_labels)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

This code example demonstrates how to use OpenCV to perform connected components analysis on a binary image.

### References

For more information on connected components analysis, you can refer to the lecture notes on Morphology (Lec-5-Morphology.pdf) and the practical exercises on Morphology (Practicals - Morphology.pdf and Practicals with Solutions - Morphology.pdf).

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-5-Morphology.pdf"" target=""_blank"">Lec 5 Morphology</a>

","Connected components analysis is used to identify and label the distinct objects in a binary image. Once labeled, properties of each component, such as its area, perimeter, and shape, can be calculated.","basic information about the image.● Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.● Use rotate to rotate the image by 90 degrees. Display the result.● Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.● imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated","basic information about the image.● Use shape on the image and print the results. How many values do you get, and what does each value represent? Set the IMREAD_UNCHANGED flag to the imread function and print the results of shape again. Is there a difference, and why?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 3if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceImage manipulation functions can affect the whole image.● Use rotate to rotate the image by 90 degrees. Display the result.● Try to rotate the image by 30 and 60 degrees. Can rotate be used? If not, why and how can we go about it? Display the results.● imshow displays each image in a separate window, but it can get very messy having so many windows popping up. Try to display the results of the three rotations in a single window (Hint: Use subplots and imshow from the Matplotlib library, and set the cmap parameter of imshow).NOTE: Images read in Python are arrays of type uint8, which range from 0 to 255. Some operations can generate large or negative numbers that cause overflows. You may find it useful to convert to int or float before applying some processes. When you use the imshow and|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingLecture 5 MorphologyomirLearning OutcomesIDENTIFY  1. Connected Components  2. What is Mathematical Morphology  3. Erosion and Dilation  4. Opening and Closing  5. ROIs and Masks COMP 2032Morphology2University of m @ Nottingham a UK | CHINA | MALAYSIAConnectedComponentsConnected Componentsi ,If black pixels are objects & white are background, give each connected set of black pixels a different labelare objects & white are background, give each connected setACK: Prof. Tony Pridmore, UNUKCOMP 2032MorphologyMorphology|4irConnected Components- Connected component algorithms are slow and often a bottleneck - Keep a separate output array for labels - Two passes over the image A sequential algorithm using 4-neighbours - Scan image top left to bottom right - Look at top and left neighbours (already given labels) - Can we assign either of their labels to the current pixel?First passACK: Prof. Tony Pridmore, UNUKCOMP 2032|Morphology5Connected Components- If the current pixel is foreground, there are 3 cases: 1. left = top = background → assign current pixel a new label 2. one of left and top is|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032 Introduction to Image ProcessingPractical Exercises Morphology=Connected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of images is a ated",
What is the difference between 4-connectivity and 8-connectivity?,"In image processing, the connectivity between pixels is crucial for various tasks such as segmentation and object detection. There are two primary types of connectivity: 4-connectivity and 8-connectivity.

4-connectivity considers a pixel to be connected to its four nearest neighbors: the pixels directly above, below, to the left, and to the right of it.

On the other hand, 8-connectivity considers a pixel to be connected to its eight nearest neighbors: the four neighbors considered in 4-connectivity (above, below, left, and right) plus the four diagonal neighbors.

To illustrate the difference, let's consider a pixel at position (x, y). In 4-connectivity, its neighbors are (x-1, y), (x+1, y), (x, y-1), and (x, y+1). In 8-connectivity, its neighbors are (x-1, y-1), (x-1, y), (x-1, y+1), (x, y-1), (x, y+1), (x+1, y-1), (x+1, y), and (x+1, y+1).

The choice between 4-connectivity and 8-connectivity depends on the specific application and the desired outcome. For example, in some segmentation tasks, 4-connectivity might be preferred to avoid connecting diagonally adjacent pixels, while in other cases, 8-connectivity might be more suitable to capture the full neighborhood of a pixel.

You can find more information on this topic in the lecture notes, specifically in the segmentation section, which is likely covered in `./documents/Lec-7-Segmentation.pdf`.","In a 2D grid, 4-connectivity considers only the top, bottom, left, and right neighbors of a pixel. 8-connectivity considers these four neighbors as well as the four diagonal neighbors.","< k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than “ie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an—-2am—«£ +1)-—k+3an—- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) — assoc(A, A) assoc(A, V) 4 assoc(B, V) — assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2— Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","< k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or that c > 4(k? +1). This is so because c = 2M(n +1) + k+3an and M > 2k?. OoACKNOWLEDGMENTSFig. 18. (a) shows a weighted graph on a regular grid. The missingFig. 18. (a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers @1,%2,..-,0n, M is a large number (M > 2k), and a is very small number (0 <a<1/n). (b) shows a cut that has a Ncut value less than “ie This cut, which only goes through edges with weight equal toa or na, has the property that the x;s on each side of the partition sum up to k.dim = 2M(n > 2M(n = 2). +1)+k+3an—-2am—«£ +1)-—k+3an—- 2amThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algo- rithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|= (3)where assoc(A,A) and assoc(B,B) are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of associa- tion and disassociation of a partition is that they are naturally related:cut(A, B) soc(A, V) : ssoc(A, V) — assoc(A, A) assoc(A, V) 4 assoc(B, V) — assoc(B, B) (BV) assoc(B, my) assoc(B,V) Neut(A, B) assoc a (en assoc(A,V) = 2— Nassoc(A, B).Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the889890IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000groups, are in fact identical and can be satisfied simulta-neously. In our algorithm, we will use this normalized cut as the partition criterion.Unfortunately, minimizing normalized cut exactly is NP-complete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
What is a Region of Interest (ROI)?,"A Region of Interest (ROI) is a subset of an image that is of particular interest or importance for a specific task or analysis. It's a way to focus on a specific part of the image, rather than processing the entire image.

In the context of image processing, ROIs can be defined in various ways, such as:

* By selecting a specific area or shape within the image
* By using masks to isolate certain regions
* By applying certain criteria or features to identify the region of interest

The concept of ROIs is powerful because it allows you to apply different processing techniques or analysis to specific parts of the image. For example, you can apply different thresholding techniques to different ROIs within the same image.

As mentioned in the lecture notes, ""The concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is applied."" (Source: Lecture notes on Morphology, available in `./documents/Lec-5-Morphology.pdf`)

By using ROIs, you can simplify image processing tasks, reduce computational complexity, and improve the accuracy of your results.",A Region of Interest (ROI) is a selected area of an image that is targeted for a specific purpose or analysis.,"pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‘pendent high-levet requires that, first, the MIVURU representat : ‘action process be contnes aramelers corresponding £ domain, is Of Images is a paradigm alec sroals. A feahire space ig “dA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon § domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‘ the feature of interest im into a point in thes parameter. After the entiaction process be et rame‘ers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each…8 & &€ 8 8 8 8 8B 7250 150o §@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds,","pasameter. A netbeceess aot eahure space ig ' processing of carl subset, a of interest ig point in the ftur the entire ae0.240.640.530.381631359761.2ACK: Prof. Tony Pridmore, UNUKThreshold by threshold3An Adaptive Thresholding ProblemB: Horizontal strips (100 pixels wide)mIVUyIT Tepes action process be rameters correspond domain. is Of images is a paradi ited goals. A feature space d through the processing Ob a time. For each subset, @ the feature of interest ig apped into a point in the parameter. After the entire mbesow-level tasks sh ‘pendent high-levet requires that, first, the MIVURU representat : ‘action process be contnes aramelers corresponding £ domain, is Of Images is a paradigm alec sroals. A feahire space ig “dA through the processing of atime. for each subset, a the fealtre of interest ig lapped inte a point in the Parameter. After the entireACK: Prof. Tony Pridmore, UNUK0.391000.531350.561424An Adaptive Thresholding ProblemC: Rectangles (150 x 200 pixels)0.510.3013077raction process be 0 arameters correspon § domain. is of images is a para ated goals. A feature space d through the processing Of a time. For each subset, @ the feature of interest ig tapped into a point|Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|sized binary image- ROIs defined by colourThe concept of ROIs is very powerful; code can be written to allow the output of one process to determine where another is appliedee — | 1ACK: Prof. Tony Pridmore, UNUKCOMP 2032| MorphologyMorphology31Summary1. Connected Components 2. What is Mathematical Morphology 3. Erosion and Dilation 4. Opening and Closing 5. ROIs and MasksCOMP 2032Morphology🤔32University of Nottingham UK | CHINA. | MALAYSIA2 QuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT:Derivates & Edges +Hough Transform|MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000Fig. 2. A gray level image of a baseball game.Fortunately, our graph partitioning has the follow- ing properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision require- ment for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is O(mn)+O(mM(n)) [11], where m is the maximum number of matrix-vector computations required and M(n) is the cost of a matrix-vector computation of Az, where A=D>(D- Ww)D>. Note that the sparsity struc- ture of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only O(n).To see why this is the case, we will look at the cost of the inner product of one row of A with a vector x. Let y; = Aj-x = yj Aj;x;. For a fixed i, Ajj is only nonzero if node j is in a spatial neighborhood of i. Hence, there are only a fixed number of operations required for each|@ ‘ the feature of interest im into a point in thes parameter. After the entiaction process be et rame‘ers correspol domain. 5 of images is a para ited goals. A feature space} d through the processing a time. For eact ibset, a the feature of interest ify apped into a point in parameter. After the ent‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rr‘pendent high-level ii requires that, first, the low enough representation oF Hie rameter rraction process be rameters correspond domain s of images is a paradi ited goals. A feat d through the process a time. For each subset, @ the feature of interest i apped into a point im the parameter. After the entire ssNew18ir aCombining Images_, darkT = thresh(pg[mask == 0]) 0.3608 92 _, lightT = thresh(pg[mask == 1]) 0.6784 173Thresholds can be calculated for and applied to mask regions, though the same thresholding method might not work for each…8 & &€ 8 8 8 8 8B 7250 150o §@ 8 8 8 8 8 B@plt.hist(pg, 256)plt.hist(pg[mask == 0], 256)plt.hist(pg[mask == 1], 256)ACK: Prof. Tony Pridmore, UNUKWhich method(s) would you try?19Combining ImagesMy intention was to apply the light and dark regions’ thresholds,",
How is a graph used in the Livewire segmentation algorithm?,"The Livewire segmentation algorithm represents the input image as a graph. To answer your question, let's break down the steps involved in the Livewire algorithm.

According to the provided context from the COMP2032 Laboratory Sheet on Livewire, implementing a Livewire consists of six significant steps, one of which is ""Converting Image to Graph."" 

1. The input image is processed.
2. The image is then converted into a graph, where pixels or regions of the image are represented as nodes, and the edges between these nodes are weighted based on certain criteria such as gradient magnitude or color similarity.

The graph representation allows the algorithm to apply graph-theoretic algorithms, such as finding the shortest path between two points. This is crucial for the Livewire algorithm as it enables the segmentation of objects by ""snapping"" to their boundaries.

In the context of Livewire, the graph is used to find the optimal path (or contour) around an object of interest by minimizing a cost function that is typically based on the image gradient. The algorithm uses this graph representation to compute the shortest path between a user-defined seed point and other points in the image, effectively segmenting the object.

Therefore, the Livewire segmentation algorithm indeed uses graph theory as its underlying concept.

To directly answer your question: A graph is used in the Livewire segmentation algorithm by representing the image as a graph, where pixels are nodes, and edges are weighted based on image features. This allows for the application of graph algorithms like shortest path to segment objects.

You can refer to the COMP2032 Laboratory Sheet on Livewire for more details on the steps involved in the Livewire algorithm.

Output_context_reference: ['./documents/COMP2032 Laboratory Sheet8b - Livewire.pdf']","In the Livewire algorithm, the image is converted into a graph where each pixel is a node. The edge weights between adjacent pixels are determined by the image's edge information, such as intensity differences.","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|positions (tg, ty, and ty) are shown in green.Livewire is famously used to segment out ROI for further processing, Is the concept based on graph theory?A. True  B. False ACK: Prof. Tony Pridmore, UNUK3Question 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Ey Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK4Answer 2E. N. Mortensen and W. A. Barrett, Intelligent Scissors for Image Composition, in ACM Computer Graphics (SIGGRAPH °95), 1995 Figure 2: /mage demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments Jrom previous free point positions (tg, ty, and ty) are|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|(tg, ty, and ty) are shown in green.Before Livewire, name the method that was famously used to extract a ROI based on its contour?A. Active ContourB. Contour TracingC. Non-Active ContourACK: Prof. Tony Pridmore, UNUK3Some Questions to THINK about…1. How does the Livewire algorithm represent the input image?2. Why is the image gradient included in the cost function as well as the presence of a zero-crossing? (there is an explanation in the Mortensen paper if you're not sure)3. During cooling, how are path stability and coalescencemeasured?ACK: Prof. Tony Pridmore, UNUK4Question 3Simple Linear Iterative is achieve by performing a clustering approach. What is it know as?A. Gaussian Mixture ModelB. Hierarchical ClusteringC. K-means5ull aQuestion 4The concept of Graph Cut is inspired by… A. Graph theory B. Markov Chain theory C. Chaos theoryRother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.61g aQuestion 5Normalised cut is the solution to minimum cut. Why was normalised cut introduced? A. Because normalised cut is computationally more efficient. B. Because min-cut can",
What is the role of Dijkstra's algorithm in Livewire?,"Dijkstra's algorithm plays a crucial role in the Livewire segmentation algorithm. According to the provided context, one of the significant steps in implementing Livewire is the ""Shortest Path Algorithm"" [./documents/COMP2032 Laboratory Sheet 8b - Livewire.pdf].

In Livewire, the image is converted into a graph where each pixel represents a node, and the edge weights between adjacent pixels are determined based on the image's edge information. Dijkstra's algorithm is used to find the shortest path between the seed point and the free point in this graph.

The algorithm is utilized to compute the minimum cost path, which is essential for the live-wire to adapt and snap to an object boundary as the free point moves. The live-wire segment is updated dynamically and interactively with each movement of the free point by following the optimal path pointers from the free point back to the seed point.

The use of Dijkstra's algorithm in Livewire is also supported by the fact that the `heapq` library, which is often used to implement priority queues in graph algorithms like Dijkstra's, is required to run the LiveWireAlgo.py implementation.

Therefore, Dijkstra's algorithm is used in Livewire to efficiently compute the shortest path between the seed point and the free point, enabling the live-wire to effectively segment the object of interest.",Dijkstra's algorithm is used in the Livewire algorithm to find the shortest path between a user-selected start point and end point on the image graph. This shortest path corresponds to the object boundary.,"f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.• Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if’ University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:• The image is converted into a graph where each pixel represents a node.• The edge weights between adjacent pixels are determined based on the image's edge information.• Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:• The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively “snap” and “wrap” the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. Speciﬁcally, note the live-wire segments corresponding to user-speciﬁed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-ﬂy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input","f° University of Es Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet: Livewire Segmentation Algorithm1. LivewireAn alternative name for the Livewire segmentation algorithm is ""Intelligent Scissors"". The algorithm's overarching concept is to avoid traversing object boundaries while segmenting images using available information.Implementing a livewire consists of six significant steps:1. Image ProcessingoOakwWN2. Converting Image to Graph3. Shortest Path Algorithm4. Interactive Visualisation5. Displaying the Image6. Event HandlingNote that Livewire is not available in OpenCV. As a result, an unofficial implementation will be used in this lab session. A Python implementation (LiveWireAlgo.py) and two test images are available on Moodle, or you may also use your images.To use LiveWireAlgo.py, you need the following libraries:• Matplotlib: Used for creating plots and visualisations.• Skimage: Used for colour space conversion and manipulation in images.• Math: Used for computing the absolute value of a floating-point number.• Heapq: Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Used for implementing priority queues, often used in graph algorithms like Dijkstra's algorithm.Itertools: Used for cycling through sequences endlessly.• Numpy: Used for numerical computing and array manipulation.After installing the given libraries, you will not see any error. In the next step, import an image into your IDE such as PyCharm or Visual Studio. The LiveWireAlgo.py will not work in Google Colab due to its limited support for GUI. In the following step, copy the image name and replace it with the current one.img_name = ""img.jpg""Then click on the Run button, and a popup will appear on the screen with the image you provided as shown in Fig 1.Prepared by Irfan Yaqub & Dr Tissa ChandesaPage 1 of 5if’ University of } wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceLive-Wire Tool 50 100 150 4 200 250 300 0 100 200 300 400Fig 1: source image2. Process:Graph Representation:• The image is converted into a graph where each pixel represents a node.• The edge weights between adjacent pixels are determined based on the image's edge information.• Typically, the edge weight represents the intensity difference or gradient between neighbouring pixels.Initialisation:• The|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|like a live-wire as it adapts to the new minimum cost path by following the optimal path pointers from the free point back23913Figure 2: Image demonstrating how the live-wire segment adapts and snaps to an object boundary as the free point moves (via cursor move- ment). The path of the free point is shown in white. Live-wire segments from previous free point positions (t0, t1, and t2) are shown in green.to the seed point. By constraining the seed point and free points to lie near a given edge, the user is able to interactively “snap” and “wrap” the live-wire boundary around the object of interest. Figure 2 demonstrates how a live-wire boundary segment adapts to changes in the free point (cursor position) by latching onto more and more of an object boundary. Speciﬁcally, note the live-wire segments corresponding to user-speciﬁed free point positions at times t0, t1, and t2. Although Fig. 2 only shows live-wire segments for three discrete time instances, live-wire segments are actually updated dynamically and interactively (on-the-ﬂy) with each move- ment of the free point.When movement of the free point causes the boundary to digress from the desired object edge, interactive input",
What are the main steps in the GrabCut algorithm?,"The GrabCut algorithm is an interactive foreground extraction technique that uses iterated graph cuts. The main steps involved in the GrabCut algorithm are:

1. **User Input**: The user draws a rectangle around the object of interest to indicate the foreground.
2. **Initialization**: The algorithm initializes the mask indicating foreground and background, and the foreground and background models.
3. **Graph Cut**: The algorithm applies graph cut to separate the foreground from the background. This involves creating a graph and computing the maximum flow/minimum cut.
4. **Iterative Refinement**: The algorithm iteratively refines the segmentation by updating the mask, foreground, and background models.
5. **Border Matting**: The algorithm estimates the alpha-matte around the object boundary and the colors of foreground pixels.

In the provided context, the steps to implement GrabCut using OpenCV are:
- Load the image
- Allow the user to draw a rectangle to indicate the foreground of interest
- Initialize the mask, background model, and foreground model
- Call the `cv2.grabCut` method with the image, mask, rectangle, background model, foreground model, and number of iterations.

These steps are demonstrated in the `quickGrabCut.py` example provided in the laboratory sheet. 

For further details, you can refer to the original GrabCut paper by Rother et al. (2004) as cited in `./documents/grab_cut.pdf`.","The GrabCut algorithm involves an iterative process of: 1) assigning GMM components to pixels, 2) learning GMM parameters from the data, 3) estimating the segmentation using a min-cut algorithm, and repeating until convergence.","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let’s try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‘input’ to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if’ University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:","SegmentationI-! I |!.29The MAGIC of GrabCutCOMP 2032Interactive SegmentationThe first row shows the original images with superimposed user input (red rectangle). The second row displays all user interactions: red (background brush), white (foreground brush) and yellow (matting brush). The degree of user interaction increases from left to right. The results obtained by GrabCut are visualised in the third row. The last row shows zoomed portions of the respective result which documents that the recovered alpha mattes are smooth and free of background bleeding.Extracted from:Rother, C., Kolmogorov, V., & Blake, A. (2004). "" GrabCut"" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.30hyGraph Cut Vs. GrabCutDifference1  GrabCut is a practical   implementation that uses graph   cuts for image segmentation,   specialising on foreground   extraction. Whereas graph cut   is a broader term. Graph cut GrabCutGraph Cut Vs. GrabCut21GeeksforGeeks. (n.d.). Foreground Extraction in an Image using Grabcut Algorithm. https://www.geeksforgeeks.org/python-foreground-extraction-in-an-image-using-grabcut-algorithm/ 2Rother, C., Kolmogorov,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|® —\GrabCut"" | Interactive Foreground Extraction using Iterated Graph CutsCarsten Rother*Vladimir Kolmogorov* Microsoft Research Cambridge, UKAndrew Blake*Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically.AbstractThe problem of efficient, interactive foreground/background seg- mentation in still images is of great practical importance in im- age editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) infor- mation, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a ro- bust algorithm for “border matting” has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic|as the mask parameter of grabCut when conducting further interactions.Now let’s try this method. Create a Python file called quickGrabCut.py and follow the steps below:Step 1: Import OpenCV and NumPy library.Step 2: Load a test image.image = cv2.imread(""CMS_livingroom.png"")Step 3: Allow the user to draw a rectangle to indicate the foreground of interest and save the coordinates of the rectangle drawn by the user.rect = cv2.selectROI('input', img, False)Here, what we did was create a window named ‘input’ to display the image so that the user can draw a rectangle on it.Step 4: Initialise the three parameters of grabCut: mask, bgdModel, and fgdModel.# initialise the mask indicating foreground and background with 0mask = np.zeros(img.shape[:2], np.uint8)# initialise the foreground and background modelbgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)Step 5: Call the grabCut method.mask, bgdModel, fgdModel = cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 5if’ University of | wee Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceStep 6:",
What is the purpose of the Transformer in the Vision Transformer (ViT) model?,"The Transformer in the Vision Transformer (ViT) model is used for image recognition tasks. According to the provided context, the ViT model utilizes a Transformer Encoder, which is a key component of the ViT architecture.

The Transformer Encoder in ViT is used to process a sequence of embedded patches from the input image. The input image is divided into patches, and these patches are then linearly projected into a higher-dimensional space, resulting in embedded patches. The Transformer Encoder takes these embedded patches as input and outputs a representation that can be used for image classification.

The Transformer Encoder in ViT is similar to the Transformer Encoder used in natural language processing tasks, but it is adapted for vision tasks by processing the embedded patches. The use of the Transformer Encoder in ViT allows the model to capture long-range dependencies and spatial relationships between different parts of the image.

To cite this information, you can refer to the source: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.

Output Context Reference: 
```json
{
  ""reference"": [
    {
      ""source"": ""Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929."",
      ""title"": ""An image is worth 16x16 words: Transformers for image recognition at scale""
    }
  ]
```

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-9-CNN.pdf"" target=""_blank"">Lec 9 CNN</a>

",The Transformer in a Vision Transformer (ViT) model is used to capture both local and global relationships within an image by treating the image as a sequence of patches.,"words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNN🤔22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.1 Extracted and modified from: Hettiarachchi, H. (12 August 2023). Unveiling Vision Transformers: Revolutionizing Computer Vision Beyond Convolution. https://medium.com/@hansahettiarachchi/unveiling-vision-transformers-revolutionizing- computer-vision-beyond-convolution-c410110ef06121Summary(1. Background 12. Components 3. Applications (4. FutureCOMP 2032 CNN🤔22University of Nottingham UK | CHINA. | MALAYSIAQuestionsQuestionsUniversity of Nottingham UK | CHINA. | MALAYSIA.NEXT:Image Compression|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What is the main difference between CNNs and ViTs?,"To address your question about the main difference between CNNs (Convolutional Neural Networks) and ViTs (Vision Transformers), we need to understand the fundamental architecture and processing mechanisms of both.

### CNNs
CNNs are designed to process data with grid-like topology, such as images. They use convolutional layers that apply filters to small regions of the image, scanning the image in a sliding window fashion. This process allows CNNs to capture local patterns and features, such as edges and textures, effectively. The key characteristics of CNNs include:
- **Local Connectivity**: CNNs focus on local regions of the image.
- **Shared Weights**: The same filter is applied across the entire image, reducing the number of parameters.
- **Hierarchical Representation**: CNNs build representations of images in a hierarchical manner, from simple features like edges to more complex features.

### ViTs
ViTs, on the other hand, are inspired by the Transformer architecture, originally designed for natural language processing tasks. ViTs treat an image as a sequence of patches (or tokens) and apply self-attention mechanisms to model the relationships between these patches. The main characteristics of ViTs include:
- **Global Attention**: ViTs can model the relationships between different parts of the image directly, thanks to the self-attention mechanism.
- **Sequence Processing**: Images are divided into patches and processed as a sequence, similar to how text is processed in NLP tasks.

### Main Difference
The main difference between CNNs and ViTs lies in how they process and understand images:
- **Local vs. Global Processing**: CNNs are inherently designed to capture local features through convolutional operations, whereas ViTs are capable of modeling global relationships between different parts of an image through self-attention.
- **Architecture and Inductive Bias**: CNNs have an inductive bias towards local connectivity and translation equivariance, which is beneficial for image processing tasks. ViTs, by treating images as sequences, have a different inductive bias that allows for more global understanding but may require larger datasets to learn effectively.

According to the context provided, particularly from Dosovitskiy et al. (2020), ViTs have been shown to be competitive with or even outperform CNNs in certain image recognition tasks, especially when pre-trained on large datasets. This suggests that while CNNs have been the traditional workhorse for image processing tasks, ViTs offer a promising alternative with different strengths.

To cite this information, you can refer to:
```python
output_context_reference([""./documents/Practicals with Solutions - CNN.pdf"", ""./documents/Practicals - CNN.pdf"", ""Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.""])
```","CNNs use convolutional layers to extract features from an image, which are inherently local operations. ViTs, on the other hand, divide the image into patches and use a Transformer architecture to learn relationships between these patches, allowing them to capture more global context.","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by","TrueB: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns25olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.26irAnswer 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Sez Ha—S Be aeee Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: TrueB: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|True B: FalseInfographics extracted from: Keita, Z. (Nov 2023). An Introduction to Convolutional Neural Network (CNNs). https://www.datacamp.com/tutorial/introduction-to- convolutional-neural-networks-cnns13olQuestion 13Transformer Encoder Vision Transformer (ViT) Transformer Encoder Patch + Position i ag BI) a) 5) Ol) I) Bl) 1) Embedding * Extra learnable [class] embedding Taz Ha8—_S Ene eeaee sui Linear Projection of Flattened Patches Embedded PatchesIs CNN better than ViT?A: True B: FalseExtracted from: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.14University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!-Thank you for trying!|the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the histogram.2. Histogram EqualisationHistogram equalisation is a technique in image processing used to improve the contrast of an image. It is done by redistributing the pixels to achieve a uniform distribution of pixel values. Using the greyscale image:● Create and display the histogram.● Use equalizeHist to perform histogram equalisation and display the histogram of the result.● Vary the gain, bias and gamma of the image and look at its effect on the image, histogram and the output of histogram equalisation.3. Contrast StretchingContrast stretching is also a technique to improve the contrast of an image by linearly stretching the range of pixel intensity values to a specified range of values, which is usually [0, 255]. Using the greyscale image:● Perform contrast stretching on the image and display the resulting image and histogram (Hint: Use normalize from OpenCV).● Display and compare the three images and histograms (original, histogram equalisation, contrast stretching). What is the difference between histogram equalisation and contrast stretching?Prepared by",
What is the purpose of zero-padding in convolutional layers?,"Zero-padding in convolutional layers is used to add zeroes to every side of the input boundaries. This ensures that filters can fit the input image, especially when the filter size is larger than the input size or when you want to maintain the spatial dimensions of the input data.

According to the provided context from the lecture notes, ""Zero-padding - Adds zeroes to every side of the input boundaries – ensures filters fit the input image."" [./documents/Lec-9-CNN.pdf]

The main purposes of zero-padding are:

1. **Preserving spatial dimensions**: By padding the input with zeros, you can maintain the spatial dimensions of the input data, which is particularly useful when you want to stack multiple convolutional layers.
2. **Allowing filters to reach edges**: Zero-padding enables filters to be applied to the edges of the input data, which would otherwise be skipped.

By using zero-padding, you can control the output size of the convolutional layer and ensure that the filters can be applied effectively to the input data.

For further clarification, you can refer to the lecture notes or contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my) during their office hours.",Zero-padding is the process of adding zeros around the border of an input image before applying a convolutional filter. This is done to control the spatial size of the output feature map and to ensure that the filter can be applied to pixels at the edge of the image.,"Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep","Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer6ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer7iy aQuestion 7Average Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74In which layer filter hyperparameters (# of filter, stride, zero-padding) are required?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer11ol aQuestion 6Average 4 8 fas ea 9 20 8 14 | | 1 ! 1 1 | 1 |a 4 8 33. ‘17 34 172 16 34 = Max Ih 4 4 I 1 1 5 13 24.7 {eee ae 16° | Relies L___] Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which layer does the above occur in?A: Fully Connected Layer B: Pooling Layer C: Convolutional Layer D: Output Layer120 aAnswer 6Average iz as] a 17 34 ax Global AverageExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep",
What is stride in a convolutional layer?,"Stride in a convolutional layer refers to the number of pixels the filtering window moves after each operation. In other words, it is the step size of the convolutional operation as it slides over the input image.

For example, if the stride is 1, the filter moves one pixel at a time. If the stride is 2, the filter moves two pixels at a time. A larger stride reduces the spatial dimensions of the output feature map, while a smaller stride retains more information.

To illustrate this, consider the following:

* Stride = 1: Filter moves 1 pixel at a time, resulting in a larger output feature map.
* Stride = 2: Filter moves 2 pixels at a time, resulting in a smaller output feature map.

The choice of stride depends on the specific application and the desired output. A common choice is a stride of 1 or 2.

You can find more information on convolutional layers in your lecture notes, specifically in the section on CNNs (Convolutional Neural Networks). 

If you need further clarification, you can also refer to the provided sources: `./documents/Lec-9-CNN.pdf` or contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) during his office hours.","Stride is the number of pixels by which the filter moves across the image at each step. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time, resulting in a smaller output feature map.
QA about Material Content","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights","Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 3, 7 x 7 mask)ACK: Prof. Tony Pridmore, UNUK14An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & ‘tapped into a point in th parameter. After the entitB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony Pridmore, UNUK15ir aAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entireB: a small gaussian (𝝈 = 7, 35 x 35 mask)ACK: Prof. Tony|Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights are only adjusted via  Animation extracted from: Mishra, M. (27 Aug 2020). Convolutional Neural Networks, Explained. Medium. https://towardsdatascience.com/convolutional- neural-networks-explained-9cc5188c4939Filter weights are only adjusted via backpropagation & gradient decent Note during the training processNote8irConvolutional Layer# of Filters  Defines the depth of the output feature map(s)  IMPORTANT:  Parameters for   training process  Stride  # of pixels the filtering window moves after each   operation  Zero-padding  Adds zeroes to every side of the input   boundaries – ensures filters fit the input image  |COMP 2032CNNI!9Convolutional LayerInput Image depth = 3  (one for each colour   Applying 2   channel) Input Image depth = 3 (one for each colour channel)Applying 2types of filtersResulting Feature Maps|COMP 2032CNNIllustration extracted from: Zvornicanin, E. (18 March 2024). What is Depth in Covolutional Neural Network? https://www.baeldung.com/cs/cnn-depth10Pooling Layer?  Occurs after a convolutional layer. Reduces the   dimensionality of the resulting conv|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|layer8Answer 4Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Which of the following sequence is accurate?A: Output layer -> Activation Function -> Pooling layer ->FC layer -> Input -> Conv layer B: Input -> Conv layer-> FC layer ->Activation Layer -> Pooling layer -> Output layer C: Input -> Conv layer-> Pooling layer ->Activation Layer -> Output layer-> FC layer D: Input -> Conv layer-> Pooling layer ->Activation Layer -> FC layer -> Output layer9Question 5Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep|FLATTEN = ConNecreD SOFTMAX FEATURE LEARNING CLASSIFICATIONExtracted from: Raghav, P. (4 May 2018). Understanding of Convolutional Neural Network (CNN) – Deep Learning. Medium. https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148COMP 2032CNN5University of m @e=-s Nottingham 4 ~ UK| CHINA | MALAYSIAComponentsCNN Components InputConvolution Layer  Pooling Layer  Activation Function  Fully Connected Layer  Output Layer COMP 2032CNN7Convolutional Layer- Most important layer - Performs major computations - Takes in input image, performs filtering which produces feature map(s) - Filters using image processing techniques (e.g., edge detection, blur and sharpen) - Filtering is performed using 3x3 kernels to perform the dot product|COMP 2032CNNResulting array [with weights] (shown in grey in the animation below) is known as feature map or activation mapgrey in the animation below) is known as   feature map or activation map  Animation extracted from: Mishra, M. (27 Aug   2020). Convolutional Neural Networks,   Explained. Medium.   https://towardsdatascience.com/convolutional-  neural-networks-explained-9cc5188c4939  Filter weights",
What are the deliverables for the coursework?,"The deliverables for the coursework in the Introduction to Image Processing module (COMP 2032) are:

1. **Python codes**: .py files submitted as a **ZIP folder**.
2. **A 6-page conference paper**: 2000 words maximum, in **PDF format**.

You are required to work in groups of 4 to develop a software solution using Python to a real image processing problem and collaboratively produce the conference paper describing and critically evaluating your solution.

Source: Assessment Sheet COMP2032 Coursework 2025.pdf.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework2025</a>

","The coursework requires a group submission of Python code files (as a ZIP folder), a 6-page conference paper (2000 words max, PDF format), and result images (as a ZIP folder). There is also an individual submission of a peer assessment form.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the deadline for the coursework submission?,"The deadline for the coursework submission is Friday, 25 April 2025, by 11:59 pm. Work submitted after this deadline will be subject to a penalty of 5 marks for each late working day. 

You can find this information in the Module Assessment Sheet for Introduction to Image Processing (COMP 2032) for Semester 2, 2024-2025.","The submission date for the coursework is Friday, 25 April 2025, by 11:59 pm.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the penalty for late submission?,"The penalty for late submission is not directly related to the topic of Image Processing. However, I can check if there's any information about late submission penalties in the provided context.

After reviewing the available context, I found a document titled ""Assessment Sheet COMP2032 Coursework 2025"" which might contain information about late submission penalties. Unfortunately, the provided snippet doesn't contain the relevant information.

If you're looking for information on late submission penalties for your coursework, I suggest checking the full ""Assessment Sheet COMP2032 Coursework 2025"" document or contacting Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or the Teaching Assistant, Mr Irfan Yaqub (hcxiy1@nottingham.edu.my) for clarification.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

",Work submitted after the deadline is subject to a penalty of 5 marks for each late working day.,"reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3","reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3",
What is the topic of the coursework?,"The coursework for the Introduction to Image Processing module involves writing programs to manipulate digital images and submitting a 2000-word conference paper that includes a Python application, literature review, explanation, and evaluation of results. The assessment is entirely based on coursework, with a combination of individual and group work, including an in-lab test where you will need to answer all questions individually. [./documents/Lec-1A-Introduction.pdf]

**Cited Materials:**

<a href=""http://localhost:5003/documents/Lec-1A-Introduction.pdf"" target=""_blank"">Lec 1A Introduction</a>

",The coursework involves developing a software solution in Python for the semantic segmentation of flowers. This includes creating an image processing pipeline to separate flowers from the background and then classifying them.,"practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What are the main parts of the coursework?,"The main parts of the coursework for the Introduction to Image Processing module (COMP2032) are:

1. **Programming and 2000-word conference paper**: This involves applying Python to an image processing task, and writing a literature review, explanation, and evaluation of the results.
2. **In-lab test**: This is a group assessment where you will need to answer all questions.

The conference paper should be submitted in PDF format and should not exceed 6 pages (additional pages will result in a penalty). You will also need to submit result images as a ZIP folder.

Additionally, each group member will need to submit a peer assessment form separately.

You can refer to the Assessment Sheet COMP2032 Coursework 2025 document for more details.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The coursework has two main parts: a SEGMENTATION PART where you design and implement an image processing pipeline to segment flowers from images in Dataset 1, and a SEMANTIC PART where you classify the entire flower dataset (Dataset 2) using a provided Transformer-based model.","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
What are the requirements for the output images in the segmentation part?,"According to the provided context, the output images in the segmentation part should have a black background with only the segmented flower. The output images should be saved as .jpg files in the output folder, categorized within three sub-folders (e.g., easy, medium, and hard) with the same name as the input image.

For example, if the input image is named ""image1.jpg"", the corresponding output image should be saved as ""image1.jpg"" in one of the sub-folders (e.g., ""easy"", ""medium"", or ""hard"") within the output folder.

Additionally, the output images should be of size 256x256 pixels. If the segmentation output is of a different size, it will be resized automatically, which may lead to distortions.

You can refer to the ""Guidelines for Using the Segmentation Evaluation Model"" PDF for further details on the requirements for the output images.","The output images must have a black background with only the segmented flower. They should be saved as .jpg files in an ""output"" folder, categorized into sub-folders named ""easy,"" ""medium,"" and ""hard,"" corresponding to the input images.","f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided","f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided",
What evaluation metrics should be used for the segmentation part?,"For evaluating the segmentation part, two key metrics are used:

1. **Cosine Similarity**: This measures the high-level similarity between the segmented images and the ground truth. The score ranges from 0 to 1, with higher values indicating better similarity.

2. **Accuracy**: This measures the pixel-wise accuracy between the segmented masks and the ground truth masks.

These metrics provide a comprehensive evaluation of the segmentation performance, assessing both the overall similarity and the pixel-level accuracy.

Source: Guidelines for Using the Segmentation Evaluation Model.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Guidelines for Using the Segmentation Evaluation Model.pdf"" target=""_blank"">Guidelines for Using the Segmentation Evaluation Model</a>

",The evaluation of the segmentation pipeline can be both qualitative and quantitative. The coursework suggests using mean IoU (Intersection over Union) as a metric.,"f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| — MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results….BeWhat we can do with metrics?• Automatic evaluation• Comparison between differenti* / ,*approaches• Evaluation from multiple perspectives• …Siméoni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry é é 70 ‘slgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the","f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingMetric for Image SegmentationirLearning OutcomesIDENTIFY  1. Metrics for Image Segmentation  2. Prior Steps  3. Core Concepts COMP 2032Metric for Image Segmentation2Why do we need metrics for image segmentation?| — MaskDistill TokenCut Im = = oaWhich algorithm is the best?| |ellHard to answer by looking at the results….BeWhat we can do with metrics?• Automatic evaluation• Comparison between differenti* / ,*approaches• Evaluation from multiple perspectives• …Siméoni, Oriane, et al. ""Localizing Objects with Self-Supervised Transformers and no Labels."" BMVC 2021- 32nd British Machine Vision Conference. 2021.ACK: Dr. Armaghan Moemeni, UNUKVan Gansbeke, et al. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363 (2022).COMP 2032Metric for Image SegmentationWang, Yangtao, et al. ""Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut."" IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).3|1I I I|First thing to do: preparing ground truthDelete File ceiy Create Polygons 4 Edit|the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments.Fig. 4 shows the final segmentation for the image shown in Fig. 2.Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.3.2 Recursive Two-Way NcutIn summary, our grouping algorithm consists of following steps: theSHI AND MALIK: NORMALIZED CUTS AND IMAGE SEGMENTATIONorg; eigenvalue , % 8 .004| .009| % 2 ry é é 70 ‘slgerwestor (a) (9) (h) (i)Fig. 3. Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system (11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.1. Given a set of features, set up a weighted graph G = (V,E), compute the weight on each edge, and summarize the",
What is the purpose of Dataset 1?,"To understand the purpose of ""Dataset 1"", I need to know the context in which ""Dataset 1"" is mentioned. However, based on the provided context, it appears that the discussion revolves around image processing and data preparation for a deep learning model, specifically using PyTorch.

The context shows examples related to segmentation and a code snippet for creating a custom dataset class (`MNIST`) that inherits from `torch.utils.data.Dataset`. This suggests that the dataset in question is likely being used for image segmentation tasks.

If ""Dataset 1"" refers to the dataset being loaded and processed in the `MNIST` class or a similar context, its purpose would be to provide the data necessary for training or testing a deep learning model, specifically for image segmentation or classification tasks.

To give a more precise answer, could you please provide more context or clarify what ""Dataset 1"" refers to?","Dataset 1 contains images of three species of flowers, categorized into ""easy,"" ""medium,"" and ""hard"" sub-folders, along with their corresponding ground truths. This dataset is to be used for developing and evaluating the image processing pipeline for the segmentation part of the coursework.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What is the purpose of Dataset 2?,"To answer your question, let's refer to the provided context. According to the retrieved context, Dataset2 consists of 847 images with their corresponding ground truths. The purpose of Dataset2 is to be used when fulfilling the semantic part of the coursework.

The context states: ""Dataset2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).""

Therefore, the purpose of Dataset2 is to support the semantic segmentation task in the coursework.

If you need more information about the semantic part or any other details, you can refer to the Assessment Sheet COMP2032 Coursework 2025.pdf for more information. 

You can also contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr Irfan Yaqub (hcxiy1@nottingham.edu.my) for further clarification.","Dataset 2, consisting of 847 images with ground truths, is to be used for the semantic part of the coursework, which involves classifying the entire flower dataset.","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 32An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1(4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  Labels 33An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032| SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels 34An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
What are the requirements for the conference paper?,"The conference paper requirements are as follows:

1. The paper should be a maximum of 6 pages in PDF format.
2. Additional pages will result in a deduction of 5% for every additional page for the entire group.
3. The paper should be submitted in a specific template (COMP2032-CW-GroupXXX.docx), where XXX is the group number.
4. The formatting and headers of the template should not be changed.
5. The provided table and text colored red and blue should be replaced with the group's own written words.

The conference paper should also include the following content:

1. Details of the chosen method(s) and specific image processing technique(s) employed.
2. Explanation of why those technique(s) and method(s) were chosen, potentially including pseudocodes.
3. Presentation of the results obtained from the practical implementation.
4. Critical evaluation of the method(s) based on the results, highlighting strengths and weaknesses.
5. Critical evaluation of the classification results, highlighting strengths and weaknesses, using metrics such as cosine similarity and accuracy.

These requirements are based on the Assessment Sheet COMP2032 Coursework 2025.","The conference paper should be a maximum of 6 pages (2000 words), detailing the chosen methods, justifying their selection, presenting the results, and critically evaluating the implemented pipeline and classification results. It must be submitted in PDF format using the provided template.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|as a way to improve the quality of the original dataset prior to being classified. Please refer to the guidelines for using the segmentation evaluation model PDF for further details of using the two provided files.CONFERENCE PAPER: When writing your conference paper, consider the following points:1. Detail the chosen method(s) and specific image processing technique(s) employed.2. Explain why you choose those technique(s) and method(s). You may include the pseudocodes of the chosen technique(s) and method(s) (if applicable).3. Present the results obtained from your practical implementation.4. Critically evaluate your method(s) on the basis of those results: what are its strengths and weakness? This section should make explicit reference to features of the results you obtained as well as using the relevant metric for segmentation to support your provided arguments/justifications.5. Critically evaluate your classification results: highlighting the strengths and weakness of your entire pipeline by means of performing a pixel-wise classification. Please use the cosine similarity and accuracy metrics, generated from your semantic part to further support your provided|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
Are we allowed to modify the provided Transformer and Model_input Python files?,"It seems you're working on a task related to the Introduction to Image Processing module, and you're unsure about modifying certain Python files.

Based on the provided context, it appears that you're referring to the `MNIST` class definition in one of the Python files. The context mentions that when creating a custom dataset class, certain method names (`__init__`, `__getitem__`, and `__len__`) are required and should not be changed.

However, I couldn't find any information about a `Transformer` or `Model_input` Python file in the provided context. If you're referring to a specific file or task, could you please provide more context or clarify which files you're working with?

That being said, if you're asking about modifying the provided Python files in general, I would recommend checking the specific instructions or guidelines provided by your instructor or teaching assistant (Dr. Tissa Chandesa or Mr. Irfan Yaqub) regarding any restrictions on modifying the code.

If you're still unsure, you can contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my) for clarification on the specific task or assignment you're working on.

**Cited Materials:**

<a href=""http://localhost:5003/documents/Laboratory Sheet 9 - CNN.pdf"" target=""_blank"">Laboratory Sheet 9   CNN</a>

","No, you are not allowed to modify any of the provided parameters in the transformer and Model_input files.","test images which are not in the training dataset. First, let’s create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","test images which are not in the training dataset. First, let’s create a Python file called inference.py and import the necessary dependencies.from DataLoader import MNISTfrom net import ToyModel from torchvision.transforms.functional import normalize import torch.utils.data as Data import torch import numpy as npfrom net import ToyModelfrom torchvision.transforms.functional import normalizeimport torch.utils.data as Dataimport torchimport numpy as npimport cv2Then create the denormalize() method which is the inverse of normalisation. This is needed because the image was normalised by the data loader and it is difficult to visualise a normalised image.def denormalize(tensor, mean, std):mean = np.array(mean)std = np.array(std)_mean = -mean / std_std = 1 / stdreturn normalize(tensor, _mean, _std)Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 13 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceBesides the preparation of the optimizer and the loss function, the procedure before inference is similar to that of training the model.# initialise inference devicestest_device = torch.device(""cuda:0"" if|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def __init__(self):super(MNIST, self).__init__()def __getitem__(self, index):…def __len__(self):…In self.__init__(), we need to load the paths of all the images (you can also load every image instead if your device has enough RAM, but it is not recommended because we can do it in self.__getitem__()), and match each image with its ground truth label). First of all, let’s add a parameter to self.__init__() to indicate which part of the dataset. It has two values: “train” or “test” depending on if we want to load the training set or test set, respectively.class MNIST(Dataset):def __init__(self, partition=""train""):...After that, let’s set the root path of our dataset according to the partition. Here we are using a relative path since our .py scripts and dataset are under the same folder or you can change it to an absolute path according to the location of the dataset in your devices.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the peer assessment form for?,"The peer assessment form is for individual submission, where you will assess the contributions of your group members for the group coursework. It is a separate submission from the group coursework deliverables, which include the Python codes and the conference paper. Failure to submit the peer assessment form will result in a penalty of 10% of your overall group mark. You can access the peer assessment form by clicking on the ""Peer Assessment"" link on Moodle. 

Source: ./documents/Assessment Sheet COMP2032 Coursework 2025.pdf


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Peer Assessment Form Query&body=I have a question about the peer assessment form."">Send email</a>",Each group member must individually submit a peer assessment form. Failure to do so will result in a 10% penalty on the overall group mark.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
What are the assessment criteria for the coursework?,"The assessment criteria for the coursework in the Introduction to Image Processing module (COMP2032) are as follows:

1. **Justification of chosen algorithms based on performed literature review**: 15%
2. **Critical analysis and discussion of your findings**: 25%
3. **Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result**: 30%

Additionally, there is an in-lab test that accounts for 50% of the assessment, which focuses on theories covered in the lectures and practical exercises.

The coursework also includes a 2000-word conference paper and a Python application, which are part of the programming and literature review components.

Source: './documents/Assessment Sheet COMP2032 Coursework 2025.pdf'

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The Python code is worth 30%, and the conference paper is worth 70%. The paper is broken down into justification of algorithms (15%), critical analysis of findings (25%), and discussion of strengths and weaknesses (30%).","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|there is no flow from the source to the sink (f = 0) and edge capacities in the residual graph G0 are equal to the original capacities in G. At each new iteration, the algorithm finds the shortest s ! t path along nonsaturated edges of the residual graph. If a path is found, then the algorithm augments it by pushing the maximum possible flow df that saturates at least one of the edges in the path. The residual capacities of edges in the path are reduced by df while the residual capacities of the reverse edges are increased by df. Each augmentation increases the total flow from the source to the sink f ¼ f þ df. The maximum flow is reached when any s ! t path crosses at least one saturated edge in the residual graph Gf.The Dinic algorithm uses breadth-first search to find the shortest paths from s to t on the residual graph Gf. After all shortest paths of a fixed length k are saturated, algorithm starts the breadth-first search for s ! t paths of length k þ 1 from scratch. Note that the use of shortest paths is an important factor that improves theoretical running time complexities for algorithms based on augmenting paths. The worst-case running time complexity for the Dinic",
What is the format for the in-lab test?,"The question about the format for the in-lab test is outside the provided context, which mainly discusses coursework guidelines, histogram backprojection, and image processing concepts. However, according to the provided context from the file './documents/Assessment Sheet COMP2032 Coursework 2025.pdf', the assessment for COMP2032 is 100% coursework, and it includes programming and a 2000-word conference paper. There's no mention of an in-lab test.

To clarify, I would suggest contacting Dr. Tissa Chandesa or Mr. Irfan Yaqub for more information on the assessment format.

Dr. Tissa Chandesa's contact details are:
Email: Tissa.Chandesa@nottingham.edu.my
Office: Room BB71 Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia

Mr. Irfan Yaqub's contact details are:
Email: hcxiy1@nottingham.edu.my

They should be able to provide you with the necessary information regarding the assessment format.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=In-lab test format query&body=Dear Dr. Tissa Chandesa, I am writing to inquire about the format for the in-lab test for the Introduction to Image Processing module. Could you please provide more information about the test format?"">Send email</a>",The in-lab test is an individual assessment worth 50% of the final grade. It focuses on theories covered in the lectures and practical exercises.,"of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|f° University of | Wa Nottingham UK | CHINA | MALAYSIASchool of Computer Science COMP2032 Coursework 2024-2025Guidelines for Using the Segmentation Evaluation Model1. Input Image RequirementsStudents must provide segmented images and ground truth masks that meet the following criteria:1.1 File Format• Accepted formats: .jpg, .png• Ensure both segmented images and ground truth masks use the same format. • Example:o segmented_folder/image_001.pngo ground_truth_folder/image_001.png1.2 Image Size & ResolutionImages must be 256×256 pixels (or will be resized automatically).• If your segmentation output is of a different size, it will be resized automatically, which may lead to distortions.2. Folder Structure Example/segmentation_submission/ │── /segmented/ # Student's segmented outputs │ ├── image_001.png │ ├── image_002.png │── /ground_truth/ # Provided ground truth masks │ ├── image_001.png │ ├── image_002.png3. Evaluation MetricsThe model evaluates segmentation using two metrics:1. Cosine Similarityo Measures high-level similarity between segmented images and ground truth.o Score ranges from 0 to 1 (higher is better).2. Accuracyo Measures pixel-wise accuracy between segmented masks|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
When is the in-lab test?,"The in-lab test is scheduled for 29 April 2025, from 2:30 pm to 3:30 pm, and will take place in TCR 1. 

If you have any further questions or need clarification, feel free to ask!","The in-lab test is scheduled for 29 April 2025, from 2:30 pm to 3:30 pm in TCR 1.

QA about Out-of-the-Domain Questions","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300Original Image USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliestOtsu Thresholding USTIFIEO ection $13 extends the time in which co run away if the ap- icant was outside Califer- ia when the kitten appeared t leaves the state after it ap- eared, It reads: ""If, when the ute kitten appears beside a erson, he is aut of the State, € may run away at the earliest 150",
What is the capital of France?,"I'm happy to help with questions related to the Introduction to Image Processing module. However, the question about the capital of France is outside the domain of image processing. I can assist you with topics related to image processing, such as morphology, segmentation, or histogram analysis, as covered in the provided context. If you have any questions within these topics, I'd be more than happy to help.","I'm sorry, I can only answer questions related to image processing.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t “ enough representatt ‘action process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane» Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","Pridmore, UNUKhes Lee eabrese3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (5, 5))se7 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (7, 7))8ir aIs morphological a possible SOLUTION?Can we get rid of the text in the bottom left corner?D: Open white"" pendent high: uires t “ enough representatt ‘action process be con arameters corresponding t domain is OF images is a paradigm alec) sraals. A feature space is vd through the processing of aohme. Pur each subset, a the fealiire of interest is lapped inte vw point in the puranieler. After the entire Barats 4ACK: Prof. Tony Pridmore, UNUKvaction process be aramelees corresponding E domain ix ot images is a paradigm aled ywals. A feahore space ig d thratigh the processing of ane» Por cach subset, a Hie fealiine of interest is Japped inte a point an the Poramelee After the entirefachon process be aramelers comraponding tdonasan ik omeges ix a paradignn ale] pels. A feat me space ie do lhrousgh the proces af A Dee Por ocd naebeet, a tha headin od poberast ie mag sped wok 4 pom in the Pirarredes Alot the entingse3 = cv2.getStructuringElement (cv2.MORPH_ELLIPSE, (3, 3))se5 =|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK4University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you explain the theory of relativity?,"I'm happy to help with questions related to Image Processing for your university module. However, the theory of relativity is outside the domain of image processing. It's a fundamental concept in physics developed by Albert Einstein.

If you're looking for information on the theory of relativity, I recommend reaching out to the Physics department at your university. They should be able to provide you with the resources and support you need.

For image processing-related questions, I'm here to help. Please feel free to ask me anything related to the topic, and I'll do my best to assist you using the provided context and materials.","I'm sorry, I can only answer questions related to image processing.","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What are the main causes of climate change?,"I'm happy to help you with questions related to Image Processing for the Introduction to Image Processing module. However, the question about the main causes of climate change is outside the domain of image processing. 

If you're looking for information on climate change, I suggest contacting your lecturers from the relevant module or searching for resources on environmental science. For image processing-related questions, I'd be more than happy to assist you. Please feel free to ask your question, and I'll do my best to provide a clear and concise answer based on the provided context.","I'm sorry, I can only answer questions related to image processing.","connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:","connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base python file.Note: COMP2032-CW-GroupXXX.docx is to be converted to PDF prior to submission and all text should be coloured in black.Semantic Segmentation of FlowersBackgroundThe need to separate one material from another, or identify a particular material, arises in many image processing applications. From industrial inspection, where you might be asked to identify pixels depicting correctly manufactured cloth, to human-computer interaction, where you might need to detect human skin, the core problem remains the same: develop a processing pipeline that converts a colour image into a binary image, labelling different materials.Biological science is a current growth area for image processing and computer vision. Biologists now have access to powerful tools that allow them to study the genetic structure of plants and animals, but lack the accurate, quantitative data on plant and animal structure as well as behaviour needed to understand the effect different genetic factors have. Plant science is an area of particular activity, as improved plant species are urgently needed|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Underfitting and Overfitting is a common problem in CNN. What are they?A: Underfitting: model is learning too much; Overfitting: model is not learning enough B: Underfitting: model is not learning enough; Overfitting: model is learning too much20Answer 10Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning:",
"Who wrote the play ""Hamlet""?","I'm happy to help you, but I must inform you that your question is outside the domain of Image Processing. As per our guidelines, I can only assist with topics related to Image Processing. The question about who wrote the play ""Hamlet"" is related to literature, not Image Processing.

However, I can tell you that the play ""Hamlet"" is generally attributed to William Shakespeare, but I'm not going to elaborate on that as it's not relevant to our Image Processing module.

If you have any questions or need help with Image Processing topics, such as thresholding, edge detection, or histogram backprojection, I'd be more than happy to assist you.","I'm sorry, I can only answer questions related to image processing.","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )ü if q’s cost changed, make q point back to pØ Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (lé*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ” 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts —__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!","of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|cost(q) )ü if q’s cost changed, make q point back to pØ Put q on the ACTIVE list (if not already there)3. Set r = node with minimum cost of the ACTIVE list4. Repeat Step 2 for p = rAlgorithm'1l I19LiveWire31 29 35 Woe 38 29 23 22*24<29 NH 47 2018 ew aisas 37 18 12 16*27 38 NY45 41 35 31 29 35 (@3}(34-36<40<50 3 @9) 43 38 29 23 28 18 (lé*21<28 49 47 40 (35) 1s (12) 16<27 38 53 59 53-=39 33 (1) 14 13 20 29 35 ” 54 35 (28) 32 1H +e 14{6) 6 12 14 22 28 35 27 (5) 31 18 #13 #7 6 14 17 #18 #%41l7*24 30-45ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Interactive Segmentation20Cooling The tool allows multiple seeds, but all seeds must be manually selected - Complex objects may require many seed pointsCan detect seed points automatically - As the user wraps the object, the early sections tend to (and need to) be fixed - Path segments that remain fixed over N cursor movements are considered stable - New seed points are created at the ends of stable segmentsACK: Prof. Tony Pridmore, UNUKCOMP 2032Interactive Segmentation21CoolingAutom atte ee Seed Poimts —__-/Basic version: several manual seed points are needed to capture the shapeWith cooling: two seeds are enough, the rest are created|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|1st and 2nd derivative methods. Please indicate one associated filter for each measure.ACK: Prof. Tony Pridmore, UNUK11iu aiu a1st vs. 2ndBased on your understanding of the 1st and 2nd derivative, complete the belowRAW DATA  1ST DERIVATIVE  2ND DERIVATIVE  9  -  -  9  7  5  4  3  0  8  0  1  0 ACK: Prof. Tony Pridmore, UNUK12CannyCannyBased on your understanding of Canny, what makes it better than Sobel and Roberts?ACK: Prof. Tony Pridmore, UNUK13University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!",
What is the chemical formula for water?,"I'm happy to help you, but I can only assist with image processing topics related to the Introduction to Image Processing module. The question about the chemical formula for water is outside the domain of image processing. If you have any questions or need help with image processing concepts, such as segmentation, histogram backprojection, or watershed algorithms, I'd be more than happy to assist you.","I'm sorry, I can only answer questions related to image processing.","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are","= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are",
Can you recommend a good restaurant in Nottingham?,"I'm happy to help with image processing questions for your university module. However, I'm not able to provide recommendations for restaurants in Nottingham or any other location as it is outside the domain of image processing. If you have any questions or need help with topics related to image processing, such as segmentation, CNN architectures, or digital images, I'd be more than happy to assist you.","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of…A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and —1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 — 2k + 2k?)(a(x) + 7) + 2(1 — 2k) B(x)2a(x)_+ki —kh)MMki —kh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 £82)(a(e) +7) +211 — 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L—b*)6(@) , 2ba(x) by bM bM bM 6M (1+ ’)(«?(D — W)a +17(D — W)1) nTD1 b (1 — B)17(D — W)z 7 b1/D1 2b27(D—W)z 2b17(D— W)1 “opi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1— z)'(D—W)(1+2) b17D1 T (+2) - 60 —2)] @- W)[(1 +2) — 41-2) ; b1°D1Let D be an N x N diagonal matrix with d","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions CNNQuestion 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74CNN is a subset of…A: Artificial Neural Network B: Reinforcement Learning C: Clustering D: Machine Learning2Answer 1Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi,|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|Aliased Signal Due to Undersampling8University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!*Thank you for trying!|can be found in Appendix A. However, we will show that, when we embed the normal- ized cut problem in the real value domain, an approximate discrete solution can be found efficiently.2.1 Computing the Optimal PartitionGiven a partition of nodes of a graph, V, into two sets A and : : a . B, let x be an N = |V| dimensional indicator vector, x; = 1 if node i is in A and —1, otherwise. Let d(i) = 7; w(?, j) be the total connection from node i to all other nodes. With the definitions x and d, we can rewrite Neut(A, B) as:. cut(A, B) cut(B, A) Neut(A, B) = 5, SE cut(A, B) assoc(A,V) assoc(B,V) > Wj 2x = (er 0.nj<0) Fi Lz>0 di Lee.<0.2,50) ~Wy@ik; Ya <0 d;(1 — 2k + 2k?)(a(x) + 7) + 2(1 — 2k) B(x)2a(x)_+ki —kh)MMki —kh)M M 5 (1-2k-+2k?) +) +2024) g() (1=k)? (a(x) +9) (1-k) & fy M 2a(a) M Letting b = he and since 7 = 0, it becomes (1 £82)(a(e) +7) +211 — 82) B(x) . 2ba(zx) = DM bM (L+P)(a(@) +7) , 2(0L—b*)6(@) , 2ba(x) by bM bM bM 6M (1+ ’)(«?(D — W)a +17(D — W)1) nTD1 b (1 — B)17(D — W)z 7 b1/D1 2b27(D—W)z 2b17(D— W)1 “opi D1 T - (1+#) @- Ww) +2) brDt R(-2)'(D-W)(1-2) 7 b17D1 _ 2b(1— z)'(D—W)(1+2) b17D1 T (+2) - 60 —2)] @- W)[(1 +2) — 41-2) ; b1°D1Let D be an N x N diagonal matrix with d",
What is the current weather forecast?,"I'm happy to help you, but I'm afraid I'm not capable of providing the current weather forecast as it's outside the domain of image processing. I'm here to assist with topics related to the Introduction to Image Processing module, such as image processing techniques, deep learning concepts, and related topics. If you have any questions or need help with a specific concept or assignment, feel free to ask!","I'm sorry, I can only answer questions related to image processing.","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","challenges, applications, future directions. Journal of big Data, 8, pp.1-74Which one is commonly used?A: Average B: Max C: Global Average15Question 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74 Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Reviewof deep learning: concepts, CNN architectures, challenges, applications, futuredirections. Journal of big Data, 8, pp.1-74Activation Function determines the neural network’s output. Which transfer functionis commonly used?A: ReLU B: Sigmoid C: Softmax D: Leaky ReLU16Answer 8Extracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-  Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review   of deep learning: concepts, CNN architectures, challenges, applications, future   directions. Journal of big Data, 8, pp.1-74|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=5Question 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=6Answer 3What operation is taking place in the figure on the right?A: Dot Product B: Scalar Product C: Inner Product D: All the aboveExtracted from: Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al- Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M. and Farhan, L. (2021). Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Journal of big Data, 8, pp.1-74: it = p|o=7Question 4Extracted|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
What is the meaning of life?,"I'm here to help with questions related to Image Processing for the Introduction to Image Processing module. The question about the meaning of life is outside the domain of Image Processing. I'm happy to help with any Image Processing-related topics or questions you may have regarding the module, such as histogram intersection, min-cut/max-flow algorithms, or superpixel segmentation. Please feel free to ask, and I'll do my best to assist you.","I'm sorry, I can only answer questions related to image processing.","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term ψ assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiﬁer trained using the SIFT descriptors. The pairwise term φ assigns to each pair of superpixels a potential to have similar or diﬀering labels based on the diﬀerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxﬂow algorithm to produce a ﬁnal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048×1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","term is infinity if aTABLE 6method input sequence Head, 5 views 384x288 | Garden, 8 views 352x240 | Dayton, 5 views 384x256 DINIC 2793.48 2894.85 2680.91 H_PRF 282.35 308.52 349.60 Q-PRF 292.93 296.48 266.08 Our 104.33 81.30 85.56Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.BOYKOV AND KOLMOGOROV: AN EXPERIMENTAL COMPARISION OF MIN-CUT/MAX-FLOW ALGORITHMS FOR ENERGY MINIMIZATION...Photo Editing Medical Data (a) (9)TABLE 7method 2D examples Beil photo (255x313) | Lung CT (409x314) | Liver MR (511x511) N4 NB N4 NB Na NB DINIC 2.73 3.99 2.91 3.45 6.33 22.86 H_PRF 1.27 1.86 1.00 1.22 1.94 2.59 Q-PRF 1.34 0.83 L17 0.77 1.72 3.45 Our 0.09 0.17 0.22 0.33 0.20 0.45 3D examples Heart MR (127x127x12) | Heart US (76x339x38) | Kidney MR (99x66x31) N6 N26 N6 N26 NG N26 20.16 39.13 172.41 443.88 3.39) 8.20 1.38 244 18.19 47.99 0.19 0.50 1.30 3.52 23.03 45.08 0.19 0.53 0.70 2.44 13.22 90.64 0.20 0.58configuration f violates the visibility constraint and zero otherwise. More details can be found in [24].The tests were done for three data sets: the Head sequence from the University of Tsukuba, the|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|weight w controls the relative importance of the so-called unary and pairwise terms. The unary term ψ assigns to each superpixel its potential to be mitochondria or background based on a probability P(ci|f(xi)) computed from the output of a support vector machine (SVM) classiﬁer trained using the SIFT descriptors. The pairwise term φ assigns to each pair of superpixels a potential to have similar or diﬀering labels based on the diﬀerence of intensities between the two superpixels,7 for (- Hed ear) ife; #¢; ; L Weal\ti)=aeaAey: Pc (esl) HP Cele) (es 0 otherwise.Eq. 4 is minimized using a mincut-maxﬂow algorithm to produce a ﬁnal segmen- tation, assigning a label to each superpixel.Segmentations obtained for each superpixel method were compared over a set of 23 annotated EM images of 2048×1536 resolution, containing 1023 mitochondria. We used k = 5 k-folds cross validation for testing and train- ing. Results are given in Table 3, and example images appear in Fig. 5. The VOC score = TP TP+FP+FN is used to evaluate segmentation performance, as it is more informative than pixelwise accuracy for sparse objects such as mitochon- dria 6.Table 3. Mitochondria segmentation results for|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How does a car engine work?,"I'm happy to help with questions related to Image Processing for your university module. However, the question ""How does a car engine work?"" is outside the domain of Image Processing. I'm here to assist with topics related to the Introduction to Image Processing module, such as image segmentation, morphology, and convolutional neural networks.

If you have any questions or need help with image processing-related topics, feel free to ask. You can also reach out to Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my) for guidance.","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|connecting to this group of node will be overstretched. Eventually, the group will “pop” off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).Assuming the solution takes the form of x(t) = vpcos(wet + 0), the steady state solutions of this spring-mass system satisfy:Kv, = w{Mv,, (21)analogous to (6) for normalized cut.Each solution pair (w;, v,) of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v; give the steady state displacement of the oscillation in each mode and the eigenvalues uz give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to “pop” off image regions with minimal effort.6 RELATIONSHIP TO OTHER GRAPH THEORETICAPPROACHES TO IMAGE SEGMENTATIONIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the|University of Nottingham ~ UK | CHINA | MALAYSIACOMP-2032Introduction toImage ProcessingPractical Exercises with Solutions MorphologyConnected Components, Morphology & MasksConnected Components converts a binary image into a label image, allowing properties of each component to be calculatedErosion, dilation, opening and closing are based on set theory, and require a structuring elementBinary images can be used as masks, to select which pixels an operation is applied to. Python provides some support for this.ACK: Prof. Tony Pridmore, UNUK2An Adaptive Thresholding Problemenough vaction process irameters co! t domain. is of images is a ated goals. A feature sj ad through the processiny a time. For ei subset, the feature of interest apped into a point in parameter. After thespendent high-level requires that, first, the enough re be controlled rameters. corresponding ji paradigmACK: Prof. Tony Pridmore, UNUKrameters correspondin ‘ domain. . ated goals. A feature space’ ‘d through the processing a time. For each subset, the feature of interest apped into a point in parameter. After the en3An Adaptive Thresholding Problemenough ce ‘action process be irameters corresp t domain. s of|2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 44An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 45An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 46An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B|movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input",
What are the rules of chess?,"I'm happy to help you with your question, but I'm afraid the rules of chess are outside the domain of Image Processing. As an expert assistant for the Introduction to Image Processing module, I can only assist with topics related to Image Processing.

If you're looking for information on Image Processing, I'd be more than happy to help. Please feel free to ask a question related to the module, and I'll do my best to provide a clear and concise answer based on the provided context. 

If you need information on the rules of chess, I suggest contacting your university's chess club or searching online for reliable sources that explain the rules of chess.","I'm sorry, I can only answer questions related to image processing.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
Can you tell me a joke?,"I'm happy to help with image processing questions or provide information related to the Introduction to Image Processing module. However, I'm not sure I have the right ""lens"" to tell a joke. If you're looking for a break from image processing, I can try to come up with a simple one, but I think it's more related to my general capabilities rather than image processing. Here's one: Why did the image go to therapy? Because it was feeling a little ""blurry""! If you have any image processing questions or need help with the module, feel free to ask.","I'm sorry, I can only answer questions related to image processing.","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!","2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A 47An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 48An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  B  B  A  A  A  B  Labels  A  A  A  A 49irComputing WatershedsWatershed based segmentations can be very efficient - It is possible to implement it in O(n) time, where n is the number of pixels - Since it takes O(n) time to read or write an image, this is as good as it can get in most situations To implement watersheds we need to sort the pixels -|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513|(continu: in figures 6 and 8). Each of the sixty-six models shown here represented by its color histogram.Fig. 5. The unknown objects. Each is identified with the model col- or histogram that best matches its own color histogram. Compared to the models the unknown objects are translated (Ajax), rotated about various axes (Frankenberry, Ajax) scaled (USA Flyer), occluded (Charmin), partly outside of the field of view (red, white striped shirt), and deformed (Mickey Mouse underwear).Fig. 6. Life cereal box image and reduced resolution copies. Left: 28x90 (1); Middle: 16x11 (2); Right 8x5 (30). The numbers in parenthesis indicate the rank of the match value for the Life cereal model. The middle image matches effectively, but the one on the right does not.Fig. 10. Images from figure 5, each with the bottom third removed These images and the images below are used in the occlusion ex periment (see table 4).Fig. 11. Images from figure 10, each with the right-hand third re- moved. The upper left-hand corner (four ninths of the original image) is left.z 7Fig. 16. Results of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped|A We =(1-A)Dz -0.04) “atta, Ana 0 10 20 30 40 od | a a 1768 u ry 1768, 0.5) a Average Cut: K aassansd (D —W)a = Ax 1768 ) pats ry i a a a a a a 0.4 0.174 0.2 /AManaaa, ~ a ~~ Average Association: Rn a 0 “as 0.176; 4a Wa=dAx “ Aah -0.2) “a as, ‘aa eae VOT 7&9 20~SCOOSC«O “44020 B0SCO (c) (d)Fig. 15. A weighting function with slow rate of fall-off: w(«) = 1 — d(a), shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b) shows the corresponding graph weight matrix W. The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.APPENDIXNP-COMPLETENESS PROOF FOR NORMALIZED CUTProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)for a graph on regular grids is NP-complete.Proof. We shall reduce NCUT on regular grids fromWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!",
What is the history of the internet?,"I'm here to help with questions about Image Processing for the Introduction to Image Processing module. Unfortunately, the history of the internet is outside the scope of this course. I'm happy to help with topics related to image processing, such as histogram intersection, thresholding, or binary images. If you have a question within these areas, I'd be more than happy to assist you. You might want to consider reaching out to the module convenor, Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), or the Teaching Assistant, Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my), for guidance on non-image processing topics.","I'm sorry, I can only answer questions related to image processing.","tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding € Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ° ‘e : ie ciewk pg toate om ‘ous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|entilEnough fachon process rameters corres I comair » «vf UV ages & af wed yows A features mi through the processing 4 time. For each subset, the feature of internet lapred into a point in Parameter After the . aeA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK9a irAn Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process te ont irameters corresponding t domain. . is of images is a paradigm ated goals. A feature space -d through the processing OF a time. For each subset @ the feature of interest is lapped into a point in the® parameter. After the entirepencent high TEQU res Unat, firet, TROUEN fepprenorta ochea pest OF oon rites. ffesronding € Diss s ~ LR! 23) ee eee et ow pure Ce ee oo ee vo [tres e ey o ° ‘e : ie ciewk pg toate om ‘ous the eres ee Chat hae areA: a 3 x 3 median filterACK: Prof. Tony Pridmore, UNUK10An Adaptive Thresholding ProblemD: Something else? Can we get rid of the text in the bottom left corner?‘action process b irameters corresp t domain. is of images is a parad ated goals. A feature spag ad through the processing ¢ a time. For each subset,” the feature of interest & iapped into a point in|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
How do I learn to play the guitar?,"I'm here to help with questions about Image Processing for the university module Introduction to Image Processing. Unfortunately, learning to play the guitar is outside the domain of image processing. I'm happy to assist with topics related to image processing, such as histogram equalization, CNNs, or interactive segmentation. If you have any questions or need help with a specific concept or assignment related to the course, feel free to ask. You can also reach out to Dr. Tissa Chandesa or Mr. Irfan Yaqub for guidance.","I'm sorry, I can only answer questions related to image processing.","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let’s create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let’s initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f° University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:● In the first interaction, the user can only draw a rectangle on the image.● The user is encouraged to press Key 1 to start drawing the rectangle.● The user can start segmentation after drawing rectangles or curves by pressing “Enter”.● After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.● After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.● The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5","movement.Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 6 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceSince there are no parameters to be learned in these two layers, we will only create one of each and they can be reused again if necessary.class ToyModel(nn.Module):def __init__(self):...self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)self.relu = nn.ReLU()After that, three consecutive fully connected layers are created to generate the confidence score of each category of the input image. This is done by using nn.Linear where in_features and out_features represent the number of dimensions of the input and output feature vectors respectively. Since there are 10 categories in MNIST (from 0 to 9), the out_features of the classification head is set to 10.class ToyModel(nn.Module):def __init__(self):...# create fully connected layersself.FC1 = nn.Linear(in_features=16*4*4, out_features=64)self.FC2 = nn.Linear(in_features=64, out_features=32)# create classification headself.cls = nn.Linear(in_features=32, out_features=10)Now let’s rewrite the self.forward() function. It receives one parameter x as the input|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|to conduct the training of the model. First, let’s create a Python file called train.py and import the three modules developed by us earlier:from DataLoader import MNISTfrom net import ToyModelfrom LossFunction import MyCrossEntropyLossWe also need to import other necessary dependencies:import torchimport torch.nn as nnimport torch.utils.data as DataThen we need to set some hyperparameters where BATCH_SIZE is the number of images in a mini-batch for each iteration, EPOCH is the number of epochs (1 epoch means the entire training dataset is passed through once) and LR is the learning rate.BATCH_SIZE = 32EPOCH = 10LR = 0.0001After that, let’s initialise each component one by one. Firstly, we shall initialise the neural network.model = ToyModel()Then, we are encouraged to set the training device, e.g., CPU or GPU. You can use GPU to accelerate the training process if you have installed a PyTorch supporting CUDA. But training our toy model on the CPU is sufficient.train_device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 10 of 16f° University of | wae Nottingham UK | CHINA ||in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|It should meet these requirements:● In the first interaction, the user can only draw a rectangle on the image.● The user is encouraged to press Key 1 to start drawing the rectangle.● The user can start segmentation after drawing rectangles or curves by pressing “Enter”.● After the first interaction, the user can choose to draw red curves indicating the foreground by pressing Key 2.● After the first interaction, the user can choose to draw blue curves indicating the background by pressing Key 3.● The user can press Key 4 to finish the segmentation and close all windows.Run your script or GrabCut.py and follow the prompts raised by the program. An example is shown below.Further interactions by drawing curves:Prepared by Ziling Wu, Dr Armaghan Moemeni, Li-Zenn Ang & Dr Tissa ChandesaPage 5 of 5",
What are the best programming languages to learn in 2025?,"I'm here to help with your questions about the Introduction to Image Processing module. While I can provide some general insights, your question about the best programming languages to learn in 2025 is quite broad and not directly related to image processing.

However, I can tell you that for image processing, Python is a popular and widely-used language due to its extensive libraries, such as OpenCV and scikit-image. In fact, the coursework for this module requires you to develop a software solution using Python.

If you're looking for information on programming languages specifically for image processing or related tasks, I'd be happy to help. Otherwise, for a more general query like the best programming languages to learn in 2025, I recommend checking resources outside of this module's scope. You might find it helpful to consult resources like the TIOBE Index or programming language popularity surveys for more general information.

For any image processing-related questions or topics within the scope of this module, feel free to ask!","I'm sorry, I can only answer questions related to image processing.","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes","practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|in this example, where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes",
Can you help me with my math homework?,"I'm happy to help with image processing questions for your Introduction to Image Processing module. However, I'm not here to assist with general math homework. If you have a specific question related to image processing, such as a concept or technique covered in the course, I'd be more than happy to help. 

If your math homework is related to image processing (e.g., mathematical concepts used in image processing techniques), please provide more context or clarify how it's related to image processing, and I'll do my best to assist you.","I'm sorry, I can only answer questions related to image processing.

QA about Course-Related Questions","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have","no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have neut; > “S2° = 247.neut;(m) = pee 4am 8M(n +1) + 4k + 12a n— 4am — 2x 2am 2am Goda c+d,’2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut valuewhere904IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 8, AUGUST 2000/. (a) (b)4a m cdame neut)(m) = >—5 ea} dame 2 —(B-2am)’ where B= 2M(n+1)— dac — 4a? m+ 4a B k+3anneut)(m) =One can check to see that ncuti(m) is a non- decreasing function and has its minimum at dac (2-P) 4a Ba when m = 1.In order to prove that ncut;(m) > oa.In order to prove that ncut;(m) > oa. we need to establish the inequality4a c4a n cdac (C — B’) + 4a B— 4a? ~ 2 1 (2 — B?) + 4a B- 4a? ((2 — B’) + 4a B—4a?)n < (4c k — 4k?)n + 4a n(e — 2k — a) 4 dane -1 orusing the fact that c= B+ 2k. To continue, note that, since an < 1, this will be true if(4c k — 4k?)n + 4(c — 2k) —4da+1<@ or if dc k? + 4c — (4k? + 8k + da — 1) <since n < k. Since 4k® + 8k + 4a —1> 0, we onlysince n < k. Since 4k® + 8k + 4a —1> 0, we only need to show that 4ck?+4c<c? or|Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can use) for image segmentation will be covered in lecture 7.You should seek a solution that is as automatic as possible (try to minimise the number of user-supplied parameters) and operates on all the provided images without user intervention (one parameter set for all).You are required to make your code easy to be executed by the marker. In other words, the marker can run your code and reproduce your given outputs. After the execution of your program, it is expected that each image in the input- image folder has a corresponding segmentation result represented by a .jpg file saved in the output folder, categorised within three sub-folders (e.g., easy, medium, and hard) with the same name as the image. As mentioned earlier, your output image must have a black background with only the segmented flower. Additionally, images generated as part of your image processing pipeline can be saved in the image-processing-pipeline folder whereby you can create sub- folders for each input image and label the sub-folders as the same name as the input image.NOTE: A readme file is|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|This dataset it to be used to fulfil the segmentation part (detailed below).Dataset 2: This consists of 847 images with its corresponding ground truths. You are to use this dataset when fulfilling the semantic part (detailed below).Your ACTION as a GroupSEGMENTATION PART: Design and implement a single Python program that inputs one of the images from dataset 1 at a time and outputs an output image which has a black background with ONLY the segmented flower as shown in Figure 1. You may not need to employ all the steps listed above, but you will probably find it worthwhile to at least consider them all. You are also encouraged to use other strategies covered in this module to address the flower segmentation problem.- = se Input Image Segmented ImageFigure 1: Segmentation Part Desired OutcomeThe focus of this coursework is on designing and developing your image processing pipeline to perform the above- mentioned tasks. You must make sure to evaluate the final outcome. The evaluation can be both qualitative and quantitative – for example mIoU (mean Intersection over Union) can be use as the metrics to evaluate your image processing pipeline’s performance. More metrics (that you can|n < 1 by construction,-4a n c4a n c4a ndane 4an < = : C-1 c-I/e dance 2 — (a n(2k, —1))? neut*81x (using k > 1, M > 2, c> 9).as was to be shown.To complete the proof we must show that all other dan ele Informally speaking, what will happen is that either the partitions result in a Neut greater than or equal to numerators of the terms in the Neut formula—the cut become too large, or the denominators become signifi- cantly imbalanced, again increasing the Ncut value. We need to consider three cases:3. A cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the xis can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of xis are grouped into one set, with total weight adding to x, where 1 < x < 2k. The corresponding ncut value,1. Accut that deviates from the cut in 1(b) slightly by reshuffling some of the x; edges so that the sums of the «; in each subset of the graph partition are no longer equal. For such cuts, the resulting Neut values are, at best, ncut; = an + 24h ue. But, since x > 1, we have",
Who is the module convenor for COMP2032?,The module convenor for COMP2032 (Introduction to Image Processing) is Dr. Tissa Chandesa. You can contact him at Tissa.Chandesa@nottingham.edu.my.,The module convenor for COMPT2032 is Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my).,"MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137","MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137",
Where can I find the lecture slides?,"To find the lecture slides for the Introduction to Image Processing module, I recommend checking the university's online learning platform or the module's specific webpage. You can also contact the module convenor, Dr. Tissa Chandesa, at Tissa.Chandesa@nottingham.edu.my or reach out to the Teaching Assistant, Mr Irfan Yaqub, at hcxiy1@nottingham.edu.my for assistance. They should be able to provide you with the necessary resources or direct you to where the lecture slides are available.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Request for Lecture Slides&body=Dear Dr. Tissa Chandesa, I am writing to inquire about the lecture slides for the Introduction to Image Processing module. Could you please let me know where I can find them? Thank you."">Send email</a>",You can find the lecture slides on Moodle’s course page.,"slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16● The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17● Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|where the output pixel values are 1,4,5,6 and 7:• No input values map to 0,2 or 3, so in the output image their normalised frequencies are all 0• Only the input value 0 maps to 1, so the output (normalised) frequency of 1 is the normalised frequency of 0 in the input image, i.e. 0.1• Only the input value 1 maps to 4, so the output (normalised) frequency of 4 is the normalised frequency of 1 in the input image, i.e. 0.5• Only the input value 2 maps to 5, so the output (normalised) frequency of 5 is the normalised frequency of 2 in the input image, i.e. 0.1• 3,4 and 5 in the input image all map to 6 in the output image, so the output normalised frequency of 6 is the sum of their frequencies: 0.1 + 0.1 + 0 = 0.2.• Input value 6 and 7 map to 7, so the output (normalised) frequency of 7 is the combined normalised frequency of 6 and 7 in the input image, i.e. 0.1ACK: Prof. Tony Pridmore, UNUK5Something IMPORTANT to REMEMBERHistogram equalisation is achieved by computing the cumulative density function (CDF). The CDF provides a lookup table which specifies the mapping from input to output pixel values. To complete the mapping, the selected CDF value must be scaled back into the range of|and click Install package (Figure 16).OpenCV on WheelsFigure 16● The library will begin installing, and the progress is shown at the bottom of the window (Figure 17).ntrib-python wads 271M OpenCV on WheelsFigure 17● Once it has been successfully installed, it will be shown under the list of packages installed (Figure 18).Figure 18Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 8 of 8|practical experience in writing programs to manipulate digital images Assessed by courseworkITo lay the foundation for studying advanced topics in related fields) LiteComputer Vision, Computer Graphics next year?ee ee ee ee ee ee ee ee ee eeeACK: Prof. Tony Pridmore, UNUKCOMP 2032Finale and Revision16Summary‘1. The Whiteboard Problem — revisited '2. What we have learnedSeeCOMP 2032Finale and Revision17University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!",
What are the office hours for the teaching assistants?,"I'm happy to help you with your question. However, I need to clarify that the office hours for the teaching assistants are not mentioned in the provided context. 

The contact details for the Teaching Assistant, Mr Irfan Yaqub, are available (Email: hcxiy1@nottingham.edu.my), but his office hours are not specified. I recommend reaching out to him directly via email to inquire about his office hours. 

If you're looking for information on image processing topics, I'd be more than happy to assist you.","I'm sorry, I don’t have access to the office hours for the teaching assistants.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100 150 LEFT-ALIGNED Section 513 extends the time in which to run away if the ap- plicant was outside Califor- nia when the kitten appeared or leaves the state after it ap- peared. It reads: “If, when the cute kitten appears beside a person, he is out of the State, he may run away at the earliest USTIFIED ection 513 extends the time n which te ron away if the ap- plicant was eutside Califor: ia when the kitten appeareel t leaves the state after it ap cared. It reads: “[f, when the ute kitren appears beside a erson, he is out of the State, he May run away at the ¢arliest 50 100 150 LEFT-ALIGNED Section $13 extends the time in which 10 run away if the ap- plicant was outside Cabfor- nia when the kitten appeared or leaves the stare after it ap poured. Ta reads: “[f, when the cute kitten appears beside a ferson, heis out of the State, he may cun away at the earliest 0 50 100 150 200 250 300 0 50 100 150 200 250 300 Adaptive Thresholding (Mean) Adaptive Thresholding (Gaussian) 0 TUstiFtee LEFT-ALIGNE® O Tostirtes LEFF-ALIGNEO ection 513 extcnds the ime = Section 513",
How do I submit my coursework?,"To submit your coursework for the Introduction to Image Processing module, you will need to follow these steps as outlined in the assessment sheet:

1. **Group Submission**: You are required to work in groups of 4. Your group will need to submit:
   - Python codes (.py files) as a ZIP folder.
   - A 6-page conference paper in PDF format, not exceeding 2000 words.

2. **Individual Submission**: In addition to the group submission, each member of the group is required to submit a peer assessment form. This is to be done through the ""Peer Assessment"" link on Moodle.

**Submission Deadline**: The deadline for the coursework submission is Friday, 25 April 2025, by 11:59 pm.

**Late Submission Policy**: Work submitted after the deadline will be subject to a penalty of 5 marks for each late working day out of the total 100 marks.

For more details, you can refer to the assessment sheet provided in the module materials: `./documents/Assessment Sheet COMP2032 Coursework2025.pdf` [1].

If you have any further questions or need clarification on the submission process, feel free to ask.",You submit your coursework through the Moodle assignment page.,"DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¶  ¶  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both – before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x – 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI’’(1) = (I’(1.5) - I’(0.5))/1  I’(0.5) = (I(1) - I(0))/1   and   I’(1.5) = (I(2) - I(1))/1  \  I’’(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the ﬁlter order.Equivalencing of spatial frequencies is performed by ﬁrst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares ﬁt can be used to create a normalized Butterworth ﬁlter B(u, v, d0', n') (whered0' and n' are the ﬁt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by ﬁtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¢ x y,( ) = F 1- [ ( , , B u v d 0 ¢ n¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are","DerivativeImage StripRaw data  1st Derivative  8  7  6  5  4  3  2  1  0  8  6  4  2  0  -2  -4  -6  -8  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  1st Derivative  0 -1 -1 -1 -1 -1 0 0 6 -6 0 0 0 1 2 -2 -1 0 0 0 7 0 0 0 ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges6002nd Derivative The formula for the 2nd derivative of a function is:  ¶  ¶  2  2  f  x  =  xf  (  ++  )1  xf  (  --  )(2)1  xf - Simply takes into account the values of both – before and after the current value - Derived by estimating the 1st derivative at x + 0.5 and x – 0.5 and computing the derivative of the resulting dataee eee IACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and Edges702nd DerivativeI0 1 2 xI’’(1) = (I’(1.5) - I’(0.5))/1  I’(0.5) = (I(1) - I(0))/1   and   I’(1.5) = (I(2) - I(1))/1  \  I’’(I) = 1.I(0) - 2.I(1) + 1.I(2)      1  -2  1 ACK: Prof. Tony Pridmore, UNUKCOMP 2032|Derivative and Edges82nd DerivativeImage StripRaw data  2nd Derivative  8  7  6  5  4  3  2  1  0  10  5  0  -5  -10  -15  5 5 4 3 2 1 0 0 0 6 0 0 0 0 1 3 1 0 0 0 0 7 7 7 7  -1 0 0 0 0 1 0 6 -12 6 0 0 1 1 -4 1 1 0 0 7 -7 0 0  2nd Derivative ACK: Prof. Tony Pridmore, UNUKCOMP 2032Derivative and|ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction of 5% for every additional page for the entire group).3. Result Images (submitted as a ZIP folder)− Output (folder name)Image Processing Pipeline (folder name)Individual submission:4. Each of you will need to make an additional submission, separately for your peer assessment form. Please click on the “Peer Assessment” link on Moodle to perform this submission. Note: those who fail to submit their peer assessment will be penalised 10% of their overall group mark.Aside from this assessment sheet, you will be provided with the following documentations:1. COMP2032-CW-GroupXXX.docx (Group 13, please use COMP2032-CW-Group013.docx instead): this is your conference paper template. Please change XXX to your group number (e.g., 001, 007, 011). Also, please do not change the formatting and headers. Marks will be deducted if submitted document does not follow the original formatting style. The provided table as well as text coloured red and blue, respectively are to be replaced by your own written words.2. Two datasets to be used (more on this later).3. A Transformer base|to low-pass ﬁlter the spectrum of the other image.The BLPF B(u, v, d0, n) is given by( , , , B u v d 0 n ) = 1 + 1 u2 v2+ d0 2nwhere d0 is the distance of the cutoff frequency from the origin and n is the ﬁlter order.Equivalencing of spatial frequencies is performed by ﬁrst com- puting the fourier transforms, S(u, v) and I(u, v) of the source image S(x, y) and the destination image I(x, y). We then compute the log power spectra s(u, v) and i(u, v):s u v,( ) = log [ S u v,( ) ] 2 i u v,( ) = log [ I u v,( ) ] 2By varying the two parameters d0 and n, a least squares ﬁt can be used to create a normalized Butterworth ﬁlter B(u, v, d0', n') (whered0' and n' are the ﬁt parameters) matched to the spatial frequency characteristics of i(u, v). If i(u, v) demonstrates lower spatial fre- quency content than s(u, v), the spatial frequencies between the two images can be equivalenced by ﬁtting B(u, v, d0', n') to s(u, v). The equivalenced result using the inverse Fourier transformS¢ x y,( ) = F 1- [ ( , , B u v d 0 ¢ n¢ , ) S u v,( ) ]is then pasted onto I(x, y). Prior to pasting, the colors in the source image are scaled to the range of I(x, y) to account for differences in contrast.5.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|key to the proof is the identity shown in equation (1). To derive this identity, we note thath=ming, M)+|h-Ml fh > MmI, = min(Z;, M,) if 1, <= M;26 Swain and BallardandM; = min, M) if > M, M,; = min, M) + |Z — Mil i if,< M,In either caseh+M,=2ming, M)+|h- Ml Cd)The proof follows easily. LetUM = Dhak isl i=lThen, using equation (1),k=->)G+™M) =1 i Nie Dl - Ml @) = I = >) ming, M) + = f=] 2 i=lBy definition,k — YL, min’, Mi) 1— HU, M) = kand so1 — HU, M) = k- Thi min, M;) k-Replacing the & in the numerator by the expression in equation (2) we have1 nt 1-HU,M)=—>) 14 - m| 2k iatand the theorem is proven.QE.D.If the model and image histograms do not contain the same number of pixels, that is, if1 >, # i=l i= n I 1then the symmetry relation (axiom number 2) does not hold and Histogram Intersection is not a metric.Appendix B: Representing a Large Database with Color HistogramsConsider the multidimensional space E defined by the bins of the histogram. That is, points in £ are n-tuples(C1, C2, ---, €,) Where n is the numbcr of bins in the histogram, and c, is the count in the ith bin. We ig- nore the discrete nature of histograms obtained from discrete images, and assume that the c; are",
What is the assessment criteria for the coursework?,"The assessment criteria for the coursework in the Introduction to Image Processing module (COMP 2032) are as follows:

1. Justification of chosen algorithms based on performed literature review: 15%
2. Critical analysis and discussion of your findings: 25%
3. Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%

Additionally, the overall coursework is worth 50% of the total module marks and is a group work that involves developing a software solution using Python and producing a conference paper.

The conference paper should be 6 pages (2000 words max) in PDF format, and the Python codes should be submitted as a ZIP folder. 

You can refer to the Assessment Sheet COMP2032 Coursework 2025 for more details. 

Source: ['./documents/Assessment Sheet COMP2032 Coursework 2025.pdf']

**Cited Materials:**

<a href=""http://localhost:5003/documents/Assessment Sheet COMP2032 Coursework 2025.pdf"" target=""_blank"">Assessment Sheet COMP2032 Coursework 2025</a>

","The assessment criteria is as follows:
+Python code(s): 30% [unable to run codes will result in 0% being awarded]
+Conference Paper:
− Justification of chosen algorithms based on performed literature review: 15%
− Critical analysis and discussion of your findings: 25%
− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%.
+In-Lab Test Individual (50%) - Focuses on theories covered in the lectures and practical exercises","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|MALAYSIAAssessmentVery IMPORTANThyCOMP2032 – Assessment (100% CW)III,II|• Programming + 2000-word   conference paper  - Python application  - Literature review, explanation &   evaluation of results  • In lab test  Group  - Answer ALL questions IndividualCOMP 2032IntroductionIntroduction31Summary1  • Image Processing  • Module Content COMP 2032IntroductionIntroduction2  Digital Images 32University of Nottingham UK | CHINA. | MALAYSIAQuestions- QuestionsUniversity of Nottingham UK | CHINA. | MALAYSIANEXT:NEXT: DigitalDigital Images andPoint Processes|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction",
Can I get an extension for the coursework?,"To inquire about an extension for the coursework, I recommend reaching out directly to our module convenor, Dr. Tissa Chandesa. You can contact him via email at Tissa.Chandesa@nottingham.edu.my. Additionally, you may also consider contacting the Teaching Assistant, Mr. Irfan Yaqub, at hcxiy1@nottingham.edu.my for any further assistance or guidance. They will be able to guide you through the process and provide more information on the coursework extension policy.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Coursework Extension Request&body=Dear Dr. Tissa Chandesa, I am writing to request an extension for the coursework submission deadline. I would like to request an extension of [number] days. I have [briefly mention the reason for the request]. Thank you for considering my request. Sincerely, [Your Name]"">Send email</a>","I’m not able to help you with an extension. You should contact the module convenor (Dr. Tissa Chandesa) and request a coursework extension, providing a reason for it.","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n » |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n — 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 — 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2” quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is—using equation (4)Proof. The points in E for whichn Sq=1 i=0form ann — 1 dimensional subset of E, which we will call P. We can find the n — 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)” (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‘V,,/Vp of the volume|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to","University of fy Nottingham UK | CHINA | MALAYSIASchool of Computer Science Module Assessment Sheet for 2024-2025Module Introduction to Image Processing (IIP) / COMP 2032 / Semester 2 Assessment Coursework (100%) Module Convenor(s) Tissa ChandesaProgramming and Conference PaperGroup50%Description and Deliverable(s)Release Date Friday, 7 March 2025 Submission Date Friday, 25 April 2025, by 11:59pm Late Policy Work submitted after the deadline will be subject to a penalty of 5 marks (the standard 5% (University of Nottingham default absolute) for each late working day out of the total 100 marks. will apply, if blank)Feedback Mechanism and DateMarks and written individual feedback will be returned via Moodle on the w/c 2 June 2025The coursework (details below) requires you to work in groups of 4 to develop a software solution, using Python, to a real image processing problem and collaboratively producing a conference paper describing and critically evaluating your solution. The deliverables required are:Per group submission:1. Python codes: .py files (submitted as a ZIP folder)2. A 6-page conference paper: 2000 words max, PDF format (note: additional pages will result in a deduction|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|nth axis is perpendicular to it. Using the triangle inequality we have for all points in the n-balln 12 Dlx? = i=l n n » |x; |? <> lal <1 i=l i=1and so it follows that for all i, |a;| = 1. Therefore, the intersection of the n-ball with the hyperplane could be contained within an n — 1 ball (in the coordinate system of the hyperplane), and so its is of smaller size. Q.E.D.We can now showTHEorem 2. The fraction of the volume of E occupied by a single model is at mostey! vnColor Indexing 27where 1 — 6 is the minimum Histogram Intersection match value allowed and n is the number of bins in a histogram.Under the city-block metric an m-ball has the shape of the region Z in each quadrant. Since there are 2” quadrants in n-dimensional Euclidean space, the volume of an n-ball of radius r is—using equation (4)Proof. The points in E for whichn Sq=1 i=0form ann — 1 dimensional subset of E, which we will call P. We can find the n — 1 dimensional volume of P by differentiating the n-dimensional volume of the set V, in whichn > gal i=0By induction, it can be shown that the volume of V is2 Vy (4) n!(2r)” (8) niUsing lemma 1 and equations (5) and (8), we have that the ratio ‘V,,/Vp of the volume|tion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1101-1113, Nov. 1993.Authorized licensed use limited to: UNIVERSITY OF NOTTINGHAM. Downloaded on February 10,2023 at 18:07:12 UTC from IEEE Xplore. Restrictions apply.1137|the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the model.There are a number of ways of determining the ap- proximate depth of an object, from laser or sonar range finders, disparity, focus, or touching the object with a sensor. The depth value combined with the known size of the object can be used to scale the model histogram. Alternatively, if it is possible to segment the object from the background and if it is not significantly occluded, the image histogram can be scaled to be the same size as the model histogram. Appendix A shows that when it is possible to segment the object from the background and thus scale the image histogram to be the same size as the model histogram, that Histogram Intersection is equivalent to the use of the sum of absolute differences or city-block metric. That is, ifn an UM = Dik f=1 i-1Color Indexing 15then if we let T equal this value, we haveL2 1- HUM) =~ >) |h-™, or , i=lTf images are scaled by depth then Histogram Intersec- tion does not define a metric, since there is an asym- metry between the model and the image. That is, the model and image historgrams are not constrained to",
What is the room number for the lectures?,"I'm happy to help you with your question about the Image Processing module. However, I need a bit more context or details to provide a precise answer regarding the room number for the lectures. 

The information about the room number for lectures isn't directly related to Image Processing concepts but rather to the course logistics. If you're looking for information on where the lectures are held, I recommend checking the university's course timetable or contacting the module convenor, Dr. Tissa Chandesa, or the Teaching Assistant, Mr. Irfan Yaqub, for the most accurate and up-to-date information.

Dr. Tissa Chandesa's contact details are:
- Email: Tissa.Chandesa@nottingham.edu.my
- Office: Room BB71, Block B, Malaysia Campus, Jalan Broga, 43500 Semenyih, Selangor Darul Ehsan, Malaysia

Mr. Irfan Yaqub's contact detail is:
- Email: hcxiy1@nottingham.edu.my

They should be able to provide you with the information you're looking for.

Contact Dr. Tissa Chandesa for lecture room number: <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Lecture Room Number Inquiry&body=Dear Dr. Tissa Chandesa, I am inquiring about the room number for the lectures."">Email Dr. Tissa Chandesa</a>","The lectures are in F3A04 (Block F3, Level A, Room 04) on Wednesdays (09:00 to 11:00). Labs are in BB80 (Block B, Level B, Room 80) on Tuesdays (14:00 to 16:00)","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A","slides:2. For each pixel:If its neighbours are all unlabelled, give it a new label(1,1) = 1(1,2) = 1(2,1) = 1-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-(4,3) = 1(1,3) = 2(2,2) = 2(4,2) = 2(1,3) = 2(3,4) = 2(4,4) = 2(1,4) = 3(3,2) = 3(2,3) = 3(3,3) = 3A  W  B B  A  W  W B  A  A  W W  AA  A W (4,1) = 3(2,4) = 4ACK: Prof. Tony Pridmore, UNUK4Watershed Exercise 2Based on your understanding of the concept covered in the lecture, consider the image fragment below (in which the numbers represent intensity values)5 4 4 3 3 3 5 3 4 3 2 3 5 2 4 21 3 5 1 11 2 3 5Using letters of the alphabet to label regions, show how this image fragment would be segmented by the Watershed algorithm.ACK: Prof. Tony Pridmore, UNUK5Watershed Solution 2(1,1) = 1(1,2) = 1(2,1) = 1(5,2) = 1(1,3) = 2(2,2) = 2(3,1) = 2(4,3) = 21. Sort the pixels (using x, y coordinates) as in the lecture slides:(1,4) = 3(2,3) = 3(3,2) = 3(3,4) = 3(4,1) = 3(4,5) = 3(5,4) = 3(5,5) = 3(2,5) = 4(3,5) = 4(4,4) = 4(5,3) = 4(1,5) = 5(2,4) = 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|(2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A 35An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  A  Labels  A  A 36An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  Labels  A  A 37An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|(3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 38An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  Labels  A  A 39An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A 40An Example 4  3  3  2  2  1  1  2  1  2  3  3  2  2  1  2  3  4  3  1  1  2  3  4 ACK: Prof. Tony Pridmore, UNUKCOMP 2032SegmentationSegmentationSorted list:(3,4) = 1  (4,4) = 1  (4,3) = 1  (1,2) = 1  (2,4) = 2  (1,3) = 2  (3,3) = 2  (4,2) = 2  (1,1) = 2  (2,1) = 2  (1,4) = 3  (2,3) = 3  (2,2) = 3  (3,2) = 3  (4,1) = 3  (3,1) = 4  B  A  A  A  Labels  A  A",
How many people are in the group for the coursework?,"The coursework for the Introduction to Image Processing module is an individual task. There is no indication that it is a group work. According to the provided context ['./documents/Assessment Sheet COMP2032 Coursework 2025.pdf'], it is mentioned that the In-Lab Test is individual, and although it doesn't directly state the coursework is individual, the absence of any mention of group work suggests it is individual. 

To confirm, you may want to contact Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my) or Mr. Irfan Yaqub (hcxiy1@nottingham.edu.my) for clarification.


*You can use the following button to send an email:*

 <a class=""email-btn"" href=""mailto:Tissa.Chandesa@nottingham.edu.my?subject=Question about coursework group size&body=Dear Dr. Tissa Chandesa, I am writing to inquire about the number of people in the group for the coursework."">Send email</a>",You need to form a group of four people for the coursework.,"30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.","30% [unable to run codes will result in 0% being awarded]Conference Paper:Justification of chosen algorithms based on performed literature review: 15%− Critical analysis and discussion of your findings: 25%− Discussion of the strengths and weaknesses of your proposed pipeline (based on your findings), prior to concluding on the classification result: 30%In-Lab Test Individual 50% Description Focuses on theories covered in the lectures and practical exercises Date, Time & Venue 29 April 2025 | 2:30pm till 3:30pm | TCR 1Page 3 of 3|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|= 5ACK: Prof. Tony Pridmore, UNUK(3,3) = 5os(4,2) = 5(5,1) = 5A sample solution (based on exactly how the pixels are ordered)2. For each pixel:If its neighbours are all unlabelled, give it a new label-If it has neighbours with a single label, it gets that labelIf it has neighbours with two or more labels, it is a watershed (W)-5  A  A  W C C  4  A  A  W W W  3  A  A  W B B  2  AA  W W B  1  AA  A W B 543216University of Nottingham UK | CHINA. | MALAYSIAThat’s ALL folks!Now, you know the answers!|Given a pair of histgrams, J and M, each containing n bins, the intersection of the histo- grams is defined to ben >) min (, Mj). j=lThe result of the intersection of a model histogram with an image histogram is the number of pixels from the model that have corresponding pixels of the same col- or in the image. To obtain a fractional match value between 0 and 1 the intersection is normalized by the numbcr of pixels in the model histogram. The match value is thenHG, M) = Lj-1 min, M;)The normalized histogram intersection match value is not reduced by distracting pixels in the background. This is the desired behavior since complete segmenta- tion of the object from the background cannot be guar- anteed. Segmentation is still a topic of active reserach, but the indications from the large amount of research done in the past are that complete, efficient, and reliable semgnetation cannot be done prior to recognition. The histogram intersection match value is only increased by a pixel in the background if ‘— the pixel has the same color as one of the colors in the model, and— the number of pixels of that color in the object is less than the number of pixels of that color in the|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.",
What programming language is used in this module?,"The programming language used in this module, Introduction to Image Processing, is **Python**. This is evident from the provided context, which mentions using libraries such as OpenCV and Matplotlib, and importing them using Python's import statements.",This module provides practical experience of image processing using Python.,"import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.● Aliasing using as. This modifies the name of the module during referencing. It is done when the module’s name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:● Read it in using imread.● Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).● Write it to a new file using imwrite.shape can provide basic information about the image.● Use shape on the image and print the results. How many values do you get, and what|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).● Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).● Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.● Using import. The entire module is imported, and the module must be referenced when used.● Using from… import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The","import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The entire module is imported, and the module does not need to be referenced. Note: this is not encouraged and is bad practice as it makes it unclear which module a function is from and pollutes the namespace.● Aliasing using as. This modifies the name of the module during referencing. It is done when the module’s name is too long or has already been used in the program.In this lab and all future labs, we will be using OpenCV. Please ensure you have installed the OpenCV library, which was discussed in the previous lab sheet.Import the OpenCV library using import cv2.2. Whole Image OperationsReading and writing images is achieved via the imread and imwrite functions, while imshow displays images in a window. Using the image cameraman.tif from the Moodle page, perform the following:● Read it in using imread.● Display it using imshow (Hint: Remember to use waitKey to keep the window displayed).● Write it to a new file using imwrite.shape can provide basic information about the image.● Use shape on the image and print the results. How many values do you get, and what|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 2: HistogramsWe will be using the Matplotlib library to create and display the histograms of the images. Matplotlib is a data visualisation and plotting library for Python which contains functions similar to the ones in MATLAB. Install the Matplotlib library and import it using from matplotlib import pyplot as plt as only the pyplot module is needed. This is the official documentation for the pyplot module https://matplotlib.org/stable/api/pyplot_summary.html.1. Creating HistogramsHistograms are often used in image processing to visualise and provide valuable information on the pixel distributions and function as a tool to assist in performing various image processing techniques. Read an RGB image (You may use the Tulip.jpeg image from the Moodle page).● Create and display a histogram for each colour channel in the image using hist (Hint: Use ravel from Matplotlib).● Convert the image to HSV and display the histograms of the hue and saturation channels. What should the number of bins be?● Convert the image to greyscale and view the effects of changing the number of bins in the|trouble searching for a conjunction of colors, as is done in the Histogram Backproduction algorithm. Until now, there has been no work in how color is used for identifica- tion or location in biological systems. This article pro- vides computational models whose presence could be explored in biological systems.AcknowledgmentsLambert Wixson, Chris Brown, Randal Nelson, and the Rochester vision group were a source of influential discussions and ideas, Thanks to Ketan Mulmuley for help with the theorem in Appendix B. Ray Rimey and the reviewers made careful comments on the manuscript. This work was supported by NSF research grant DCR- 8602958 and ONR research grant NO0014-91-J-1185.Color Indexing 25Appendix A: Relation to Pattern RecognitionEach of the different bins of the histogram can be considered a different feature, as is done in pattern recognition (Young & Fu 1986). This approach to recognition has been studied extensively, and so it is important to discuss Histogram Intersection in relation to the approaches used in this disciplinc.In pattern recognition, the set of features are designated to be axes in a feature space, in which the object is defined to be a point (fi, ...,|if’ University of |W Nottingham UK | CHINA | MALAYSIASchool of Computer ScienceCOMP2032 Laboratory Sheet 1A: Image Processing in PythonOpenCV is an open-source library of functions related to computer vision, image processing and machine learning. Some other notable libraries would be Pillow (PIL) and sci-kit image. In this module, we will be using OpenCV as the main image processing library. You may read the official documentation to better understand the functions available https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html.1. Using LibrariesTo use the libraries in Python, they must be imported using import. It is encouraged and good practice to import libraries at the beginning of the file. import is used to access the code in a Python module, which is a Python file containing a set of functions and classes (essentially what a library is). We can also create our own modules to organise code by grouping it, improving code readability and reusability.● Using import. The entire module is imported, and the module must be referenced when used.● Using from… import. The specified functions are imported, and the module does not need to be referenced.● Using from… import *. The|them perfectly flat or approximate the view in the database. Since this experiment tests the ‘‘what” or identification problem, the panning is done so that the image serves as a fovea, that is, each shirt, when centered, occupies the major- ity of the image. Nevertheless, the shirts are close enough that there is often another shirt in the background when a match is being done. The shirt oc- cupying the major portion of the image is invariably the top notch.As discussed at the beginning of this section, it is expected that a color-constancy algorithm be used before histogramming. Nonetheless, we tested Histo- gram Intersection in the presence of changing light in- tensity without color constancy. One aim is to see how necessary a color-constancy algorithm is. As we ex- pected, changes in lighting conditions affect the match value considerably. More importantly, this experiment can be also used to test how well a color-constancy algorithm must work. Changing light intensity was simulated by multiplying the image pixel values by a constant factor ranging from 0.4 to 1.6. The resulting pixel values were constrained to be no greater than 255, as would occur in a camera. The",
Where can I find the datasets for the coursework?,"The coursework dataset details are not explicitly mentioned in the provided context. However, I can guide you on how to find the dataset.

You can check the course materials, such as the laboratory sheets or the course Moodle page, for information on the datasets. For instance, the Tulip.jpeg image is mentioned as being available on the Moodle page in the context provided ['./documents/Lab 4.pdf'].

If you're still unsure, I recommend contacting the module convenor, Dr. Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my), or the Teaching Assistant, Mr Irfan Yaqub (hcxiy1@nottingham.edu.my), for clarification on the coursework dataset. They can provide you with the necessary information or direct you to the relevant resources.",You can download the datasets from Moodle’s coursework page.,"3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder’s name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image’s path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100","3 of 16f° University of | wae Nottingham UK | CHINA | MALAYSIASchool of Computer Scienceclass MNIST(Dataset):def __init__(self, partition=""train""):...if partition == ""train"":root_path = ""./MNIST/mnist_train/""else:root_path = ""./MNIST/mnist_test/""Since we aim to load all the image paths and labels and match them, we can initialise an empty list whose elements are tuples (image path, label).class MNIST(Dataset):def __init__(self, partition=""train""):...# create a list of image path - label pair: [(image_path, label), ...]self.image_path_label_pairs = []By observing the dataset structure, we can see that each subfolder’s name is the category of images under it. Hence, we can use os.listdir() to obtain the class list of all categories.class MNIST(Dataset):def __init__(self, partition=""train""):...categories = os.listdir(root_path)Since images of the same class are under the sub-folder of the class name, we can summarise the image path and label pair by getting each image’s path and the name of the corresponding sub-folder.class MNIST(Dataset):def __init__(self, partition=""train""):...# load images of each sub-folder (category)for category in categories:# load image names under the|of the ratio histogram backprojection step (2) of Histogram Backprojection, using figure 6 as the image and the striped blue and white shirt as the target. The blue hue is found in only a small area outside of the target, so it gives a strong response. White is found in many objects so it gives a weak response.Fig. 17. Results of the convolution step (3) of Histogram Back- projection, for the same image as above. The results for all the models are shown in the image, each in the rectangle corresponding to the location of that model in the composite photo. When the algorithm successfully finds the object, the darkest black dot in the small image is in the same location within that image as the image in the component.Fig. 18. Results of the ratio histogram backprojection step (2) Histogram Backprojection when the objects are occluded. Figure is the image and the striped blue and white shirt is the target.Fig. 19. Results of the convolution step (3) of Histogram Bac projection, for the same image as above. The results of all the mode are shown in the image, each in the rectangle corresponding to U location of that model in the composite photo. When the algorith successfully finds the|After observing the dataset structure, we can start to write the code for data preparation.First, let’s create a Python file called DataLoader.py and import the libraries used in this component:from torch.utils.data import Datasetimport os, random, cv2import torchvision.transforms.functional as TFtorch.utils.data.Dataset is required to customise our data preprocessing. We need to write a class that inherits from it to define how the data is loaded from the disk and how the images and labels are matched. torchvision.transforms.functional provides a series of APIs of image processing and data type conversion from NumPy/OpenCV to PyTorch tensor (similar to that of ndarray in NumPy), and vice versa. Remember, the input and output of the neural network should always be a PyTorch tensor, and they should be converted to another format (e.g., NumPy array) if further pre-processing and post-processing are required.After importing those libraries, we need to write a class inherited fromtorch.utils.data.Dataset. Specifically, three methods should be rewritten by us, which means we are not allowed to change the names of these three methods, as shown below.class MNIST(Dataset):def|by key.In the on-line phase:1. Sort the image histogram bins by size.2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously un- matched model bins are matched, match them to all larger image bins.100 99 Average Match 98-4 Percentile 97 96 + T 7 a -0 Tmage Histogram Bins MatchedFig. 15. Effectiveness of Incremental Intersection as function of the number of image histogram bins matched. For comparison, the average match percentile for Histogram Intersection is 99.86.The efficiency of Histogram Intersection and Incre- mental Intersection is compared in table 6. A complex- ity analysis shows that the time for Histogram Intersec- tion depends linearly on the product of the size of the histogram and the size of the database, ie., the algorithm is O(am), where nis the total number of bins in the histogram and m is the number of models in the database. The asymptotic complexity of Incremental In- tersection is O(n log n + cm), where c is the number of image bins used for indexing into the database. The complexity of Incremental Intersection is also linear in the size of the database.|threshold value. Has Otsu picked the best threshold value?For adaptive thresholding, there are two ways that the OpenCV function adaptiveThreshold uses to decide the threshold value. The first is by using the mean of the neighbourhood values, and the other is by using the Gaussian-weighted sum of the neighbourhood values.● Using the ADAPTIVE_THRESH_MEAN_C flag, tweak the blockSize and C parameters to achieve the best result.● Do the same using the ADAPTIVE_THRESH_GAUSSIAN_C flag. What is the difference between the two methods?● Compare the results of the four methods. Which method produced the best result?● Repeat the steps with the Tulip.jpeg from the Moodle page. Which method produced the best result?Prepared by Li-Zenn Ang & Dr Tissa ChandesaPage 1 of 2University of Nottingham UK | CHINA | MALAYSIASchool of Computer Science3. Expected ResultsOriginal Image Manual Thresholding Otsu Thresholding USTIFIED ection 513 extends the time n which to run away if the ap- slicant was outside Califor- ia when the kitten appeared r leaves the state after it ap- eared. It reads: “If, when the pute kitten appears beside a erson, he is out of the State, he may run away at the earliest 50 100",
